{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Day 9**: Unsupervised Learning üëª (***live in 1.51***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center><h1 style=\"color:maroon\">Unsupervised Learning</h1>\n",
    "    <img src=\"https://drive.google.com/uc?id=17-4C7cjFWbngU9UVi1bwuHlYs-Gctzfz\" style=\"width:1300px\">\n",
    "    <h3><span style=\"color: #045F5F\">Data Science & Machine Learning for Planet Earth Lecture Series</span></h3><h6><i> by C√©dric M. John <span style=\"size:6pts\">(2023)</span></i></h6></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plan for today's Lecture üóì "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Dimensionality reduction with matrix factorization (PCA)\n",
    "* Clustering algorithms (KMeans, DBSCAN, HDBSCAN)\n",
    "* Anomaly detection (IFOR, OCSVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Intended learning outcomes üë©‚Äçüéì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Build a feel for what unsupervised machine learning can do\n",
    "* Apply dimensionality reduction to data visualization\n",
    "* Explore dataset by clustering similar observations\n",
    "* Identify potential outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What is unsupervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "<img src=\"https://drive.google.com/uc?id=16sITqIKr3g0q2ys3ErFMtnMUGblHBmli\" style=\"width:1200px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p><strong>Unsupervised Algorithms</strong></p>\n",
    "<blockquote><p>find patterns in <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"mjx-chtml MathJax_CHTML\" data-mathml='&lt;math xmlns=\"http://www.w3.org/1998/Math/MathML\"&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/math&gt;' id=\"MathJax-Element-9-Frame\" role=\"presentation\" style=\"font-size: 116%; position: relative;\" tabindex=\"0\"><span aria-hidden=\"true\" class=\"mjx-math\" id=\"MJXc-Node-40\"><span class=\"mjx-mrow\" id=\"MJXc-Node-41\"><span class=\"mjx-mi\" id=\"MJXc-Node-42\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.434em; padding-bottom: 0.249em; padding-right: 0.024em;\">X</span></span></span></span><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script id=\"MathJax-Element-9\" type=\"math/tex\">X</script>, <strong>without supervision from a target  <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"mjx-chtml MathJax_CHTML\" data-mathml='&lt;math xmlns=\"http://www.w3.org/1998/Math/MathML\"&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/math&gt;' id=\"MathJax-Element-10-Frame\" role=\"presentation\" style=\"font-size: 116%; position: relative;\" tabindex=\"0\"><span aria-hidden=\"true\" class=\"mjx-math\" id=\"MJXc-Node-43\"><span class=\"mjx-mrow\" id=\"MJXc-Node-44\"><span class=\"mjx-mi\" id=\"MJXc-Node-45\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.249em; padding-bottom: 0.496em; padding-right: 0.006em;\">y</span></span></span></span><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script id=\"MathJax-Element-10\" type=\"math/tex\">y</script></strong></p>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=17JML9XsJ1_LW-ByvRzz6Sna3sWhIEqyJ\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a><br>\n",
    "    <br>Prompt: <i>The Eifel Tower contained within a small glass bottle, 3D rendering</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dataset\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=17FjtM7bbZDiq-S_PXkRX4HDgrJbebPI7\" style=\"width:500px\" align=\"left\"/>\n",
    "\n",
    "<span style=\"color:teal\">**Today we are using a convenient dataself for visualization:** </span><a href=\"https://github.com/allisonhorst/palmerpenguins/blob/main/README.md\"> the penguins dataset from Palmer Station, Antarctica LTER</a> published in <a href=\"doi:10.1371/journal.pone.0090081\"> Gorman et al, 2014.</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('Lecture_data/penguins.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = data.drop(columns=['species'])\n",
    "y = data.species\n",
    "                   \n",
    "X_train, X_test, y_train_raw, y_test_raw = train_test_split(X, y, train_size=0.8, random_state=12)\n",
    "\n",
    "label_encoder = LabelEncoder().fit(y_train_raw)\n",
    "\n",
    "y_train = pd.Series(data=label_encoder.transform(y_train_raw),name='species')\n",
    "y_test = pd.Series(data=label_encoder.transform(y_test_raw), name='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Visualizing the relationship between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_data = X_train.copy(); plot_data['species']=y_train_raw.values;\n",
    "sns.pairplot(data=plot_data, hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Colinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "corr = X_train.corr()\n",
    "fig, ax = plt.subplots(1,1,figsize=(10, 8))\n",
    "sns.heatmap(corr, cmap='coolwarm',ax=ax, annot=corr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "(Dimensionality reduction through matrix factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Remember Linear Regression Variants<br>\n",
    "**Polynomial**<br>\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2$ <br>\n",
    "**Log transformation**<br>\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 \\log(X_1)$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<p>üëâ PCA = finding the <strong>best linear combination</strong> of features:</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\color{red}{Z_1} = a_{11} X_1 + a_{12} X_2 + a_{13} X_3$<br>\n",
    "$\\color{red}{Z_2} = a_{21} X_1 + a_{22} X_2 + a_{23} X_3$<br>\n",
    "$\\color{red}{Z_3} = a_{31} X_1 + a_{32} X_2 + a_{33} X_3$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With the axis being orthogonal to each other, and oriented along the maximum variance of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Based on the eigenvector decomposition of the martix (see your math module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>üëâ <strong>PCA helps to reduce dimensions</strong></p>\n",
    "<p><img src=\"https://drive.google.com/uc?id=17FiCoTKfsYQq2Ep4GO-d67l-Ne9u24Ep\" width=\"1200\"/></p>\n",
    "<p>üìö <a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">G√©ron, 2017</a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(random_state=5)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Access our 11 PCs \n",
    "W = pca.components_\n",
    "features = X_train.columns\n",
    "# Print PCs as COLUMNS\n",
    "W = pd.DataFrame(W.T,\n",
    "                 index=features,\n",
    "                 columns=[f'PC{i}' for i in range(1, 5)])\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>¬†<br/>\n",
    "‚òùÔ∏è Each PC is a <strong>linear combination of the penguins features</strong></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Project our original data into the new PCA space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_proj_pca = pca.transform(X_train)\n",
    "X_proj_pca = pd.DataFrame(X_proj_pca, columns=[f'PC{i}' for i in range(1, 5)])\n",
    "X_proj_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### New colinearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10, 8))\n",
    "sns.heatmap(X_proj_pca.corr(), cmap='coolwarm',ax=ax, annot=X_proj_pca.corr().astype(int));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Plotting the data in the PCA space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Nice color map:\n",
    "cm = plt.colormaps['viridis']\n",
    "\n",
    "# 2D-slice\n",
    "\n",
    "plt.figure(figsize=(13,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Before PCA (initial space)'); plt.xlabel('flipper_length_mm'); plt.ylabel('body_mass_g')\n",
    "plt.scatter(X_train.iloc[:,2], X_train.iloc[:,3],c=y_train, cmap=cm)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('PC1 vs PC2 (new space)'); plt.xlabel('PC 1'); plt.ylabel('PC 2')\n",
    "plt.scatter(X_proj_pca.iloc[:,0], X_proj_pca.iloc[:,1],c=y_train, cmap=cm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### PC are ranked by order of importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's compute it\n",
    "X_proj_pca.std()**2 / ((X_proj_pca.std()**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Sklearn provides it automatically\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>‚òùÔ∏è Automatically ranked by scikit-learn <code>PCA</code></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<blockquote><p>68% of the dataset‚Äôs variance lies along the first axis</p>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PCA for dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Choosing 'k'**: <span style = \"color:teal\">***Trade-off***</span> between compression and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Elbow method**: Look for the <em>inflection point</em> in the explained variance chart. Here, <code>k=2</code> (> 85% of the variance explained) or <code>k=3</code>  (> 90% of the variance explained) are good potential candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "plt.plot(range(1,5),np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.ylim(ymin=0.65); plt.title('cumulated share of explained variance', size=16); plt.xlabel('# of principal component used', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h4 id=\"‚úèÔ∏è-Test-Model-Performance-(with-k=3-Dimensions)\">‚úèÔ∏è Test Model Performance (with <code>k=2</code> Dimensions)<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/camps/900/lectures/content/05-ML_06-Unsupervised-Learning.html#%E2%9C%8F%EF%B8%8F-Test-Model-Performance-(with-k=3-Dimensions)\">¬∂</a></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# PCA with only 2 components\n",
    "pca_compressed = PCA(n_components=2,random_state=5)\n",
    "\n",
    "# Project our data into 2 dimensions\n",
    "X_train_compressed = pca_compressed.fit_transform(X_train)\n",
    "X_test_compressed = pca_compressed.transform(X_test)\n",
    "\n",
    "full_model = KNeighborsClassifier(n_neighbors=8).fit(X_train, y_train)\n",
    "pca_model = KNeighborsClassifier(n_neighbors=8).fit(X_train_compressed, y_train)\n",
    "\n",
    "print(\"Accuracy with 2 PCs\")\n",
    "print(accuracy_score(y_test, pca_model.predict(X_test_compressed)))\n",
    "print(\"Accuracy all 4 initial features\")\n",
    "print(accuracy_score(y_test, full_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Kernel PCA\n",
    "Captures non-linear patterns (similar principle to SVM kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Fit a PCA with only 2 components\n",
    "kpca = KernelPCA(kernel='rbf',n_components=4,random_state=5)\n",
    "\n",
    "X_train_kpca = kpca.fit_transform(X_train)\n",
    "X_test_kpca = kpca.transform(X_test)\n",
    "\n",
    "kpca_model = KNeighborsClassifier(n_neighbors=8).fit(X_train_kpca, y_train)\n",
    "\n",
    "print(\"Accuracy with 2 PCs\")\n",
    "print(accuracy_score(y_test, kpca_model.predict(X_test_kpca)))\n",
    "\n",
    "print(\"\\nAccuracy all 4 initial features\")\n",
    "print(accuracy_score(y_test, full_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_kpca = pd.DataFrame(X_train_kpca, columns=[f'PC{i}' for i in range(1, 5)])\n",
    "X_train_kpca.std()**2 / ((X_train_kpca.std()**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Nice color map:\n",
    "cm = plt.colormaps['viridis']\n",
    "\n",
    "# 2D-slice\n",
    "\n",
    "plt.figure(figsize=(13,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Before PCA (initial space)'); plt.xlabel('flipper_length_mm'); plt.ylabel('body_mass_g')\n",
    "plt.scatter(X_train.iloc[:,2], X_train.iloc[:,3],c=y_train, cmap=cm)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('PC1 vs PC2 (new space)'); plt.xlabel('PC 1'); plt.ylabel('PC 2')\n",
    "plt.scatter(X_train_kpca.iloc[:,0], X_train_kpca.iloc[:,1],c=y_train, cmap=cm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Decompressing PCA\n",
    "* We can decompressed our compressed data back into 4 dimensions\n",
    "* Of course, some information will be lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructed PCA\n",
    "X_reconstructed = pca_compressed.inverse_transform(X_train_compressed)\n",
    "\n",
    "# Plotting it\n",
    "plt.figure(figsize=(15,4)); plt.subplot(1,2,1)\n",
    "sns.heatmap(X_train.iloc[0:30])\n",
    "plt.title(\"original data\"); plt.subplot(1,2,2); plt.title(\"reconstructed data\")\n",
    "sns.heatmap(X_reconstructed[0:30]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Limitations of PCA\n",
    "**PCA cannot separate data on a manifold** (an N-dimensional shape) if they are bent and twisted into a higher dimensional shape: this is a limitation of the projection approach.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=175A_L6sakMkEBzMOCNfLTEfrroDVPW3G\" style=\"width:1500px\"/>\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Hands-On Machine Learning</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Clustering\n",
    "\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=178rcdzEH_hhUdfNLhKp9C9nnWWfzggk2\" style=\"width:900px\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a><br>\n",
    "    <br>Prompt: a group of cows all clustered in the middle of a purple prairie, digital art.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Many clustering algorithms\n",
    "<a href=\"https://scikit-learn.org/stable/modules/clustering.html\">https://scikit-learn.org/stable/modules/clustering.html</a></p>\n",
    "<p><img src=\"https://drive.google.com/uc?id=17EP0HT6w7HGgGnSHy-AFQP4bOgM1bThJ\" style=\"width:1200px\"/></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>Find <strong>categories</strong> (classes, segments) of <strong>unlabelled</strong> data rather than just trying to reduce dimensionality</p>\n",
    "<p>üëâ Works better on data that is already clustered, geometrically speaking<br/>\n",
    "üëâ Use PCA for dimensionality reduction beforehand: Euclidean distances work better in lower dimensions)!</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-Means Explained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p><img src=\"https://drive.google.com/uc?id=17B4Ys6fpZUT1MYgkrgSaU6Hc0XuWCLEP\" width=\"1500px\"/></p>\n",
    "<a href=\"https://kindsonthegenius.com/blog/what-is-k-means-in-clustering-in-machine-learning/\">Kidson, 2017</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p><img src=\"https://drive.google.com/uc?id=17BWIyP3TJXMFKSL4DpD29YV6-IzS67Yj\" width=\"1500px\"/></p>\n",
    "<a href=\"https://kindsonthegenius.com/blog/what-is-k-means-in-clustering-in-machine-learning/\">Kidson, 2017</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p><img src=\"https://drive.google.com/uc?id=16xAIeZziY-pNpOgKbxj-__JgewRjlbb0\" width=\"1500px\"/></p>\n",
    "<a href=\"https://kindsonthegenius.com/blog/what-is-k-means-in-clustering-in-machine-learning/\">Kidson, 2017</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### KMeans Loss Function (Inertia)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p><code>km.fit(X)</code> finds parameters <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"mjx-chtml MathJax_CHTML\" data-mathml='&lt;math xmlns=\"http://www.w3.org/1998/Math/MathML\"&gt;&lt;mi&gt;&amp;#x03B2;&lt;/mi&gt;&lt;/math&gt;' id=\"MathJax-Element-22-Frame\" role=\"presentation\" style=\"font-size: 116%; position: relative;\" tabindex=\"0\"><span aria-hidden=\"true\" class=\"mjx-math\" id=\"MJXc-Node-296\"><span class=\"mjx-mrow\" id=\"MJXc-Node-297\"><span class=\"mjx-mi\" id=\"MJXc-Node-298\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.496em; padding-bottom: 0.496em; padding-right: 0.007em;\">Œ≤</span></span></span></span><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Œ≤</mi></math></span></span><script id=\"MathJax-Element-22\" type=\"math/tex\">\\beta</script> that minimize a loss</p>\n",
    "<ul>\n",
    "<li>Each <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"mjx-chtml MathJax_CHTML\" data-mathml='&lt;math xmlns=\"http://www.w3.org/1998/Math/MathML\"&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B2;&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;' id=\"MathJax-Element-23-Frame\" role=\"presentation\" style=\"font-size: 116%; position: relative;\" tabindex=\"0\"><span aria-hidden=\"true\" class=\"mjx-math\" id=\"MJXc-Node-299\"><span class=\"mjx-mrow\" id=\"MJXc-Node-300\"><span class=\"mjx-msubsup\" id=\"MJXc-Node-301\"><span class=\"mjx-base\" style=\"margin-right: -0.007em;\"><span class=\"mjx-mi\" id=\"MJXc-Node-302\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.496em; padding-bottom: 0.496em; padding-right: 0.007em;\">Œ≤</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" id=\"MJXc-Node-303\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.434em; padding-bottom: 0.496em;\">j</span></span></span></span></span></span><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>Œ≤</mi><mi>j</mi></msub></math></span></span><script id=\"MathJax-Element-23\" type=\"math/tex\">\\beta_j</script> parameter is the <strong>centroid</strong> <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"mjx-chtml MathJax_CHTML\" data-mathml='&lt;math xmlns=\"http://www.w3.org/1998/Math/MathML\"&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03BC;&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;' id=\"MathJax-Element-24-Frame\" role=\"presentation\" style=\"font-size: 116%; position: relative;\" tabindex=\"0\"><span aria-hidden=\"true\" class=\"mjx-math\" id=\"MJXc-Node-304\"><span class=\"mjx-mrow\" id=\"MJXc-Node-305\"><span class=\"mjx-msubsup\" id=\"MJXc-Node-306\"><span class=\"mjx-base\"><span class=\"mjx-mi\" id=\"MJXc-Node-307\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.249em; padding-bottom: 0.496em;\">Œº</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" id=\"MJXc-Node-308\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.434em; padding-bottom: 0.496em;\">j</span></span></span></span></span></span><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>Œº</mi><mi>j</mi></msub></math></span></span><script id=\"MathJax-Element-24\" type=\"math/tex\">\\mu_j</script> of its respective cluster <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"mjx-chtml MathJax_CHTML\" data-mathml='&lt;math xmlns=\"http://www.w3.org/1998/Math/MathML\"&gt;&lt;msub&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;' id=\"MathJax-Element-25-Frame\" role=\"presentation\" style=\"font-size: 116%; position: relative;\" tabindex=\"0\"><span aria-hidden=\"true\" class=\"mjx-math\" id=\"MJXc-Node-309\"><span class=\"mjx-mrow\" id=\"MJXc-Node-310\"><span class=\"mjx-msubsup\" id=\"MJXc-Node-311\"><span class=\"mjx-base\" style=\"margin-right: -0.045em;\"><span class=\"mjx-mi\" id=\"MJXc-Node-312\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.496em; padding-bottom: 0.311em; padding-right: 0.045em;\">C</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" id=\"MJXc-Node-313\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.434em; padding-bottom: 0.496em;\">j</span></span></span></span></span></span><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>C</mi><mi>j</mi></msub></math></span></span><script id=\"MathJax-Element-25\" type=\"math/tex\">C_j</script></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<ul>\n",
    "<li>The loss function is called <strong>inertia</strong> <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"mjx-chtml MathJax_CHTML\" data-mathml='&lt;math xmlns=\"http://www.w3.org/1998/Math/MathML\"&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo stretchy=\"false\"&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03BC;&lt;/mi&gt;&lt;mo stretchy=\"false\"&gt;)&lt;/mo&gt;&lt;/math&gt;' id=\"MathJax-Element-26-Frame\" role=\"presentation\" style=\"font-size: 116%; position: relative;\" tabindex=\"0\"><span aria-hidden=\"true\" class=\"mjx-math\" id=\"MJXc-Node-314\"><span class=\"mjx-mrow\" id=\"MJXc-Node-315\"><span class=\"mjx-mi\" id=\"MJXc-Node-316\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.434em; padding-bottom: 0.249em;\">L</span></span><span class=\"mjx-mo\" id=\"MJXc-Node-317\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.434em; padding-bottom: 0.619em;\">(</span></span><span class=\"mjx-mi\" id=\"MJXc-Node-318\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.249em; padding-bottom: 0.496em;\">Œº</span></span><span class=\"mjx-mo\" id=\"MJXc-Node-319\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.434em; padding-bottom: 0.619em;\">)</span></span></span></span><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mo stretchy=\"false\">(</mo><mi>Œº</mi><mo stretchy=\"false\">)</mo></math></span></span><script id=\"MathJax-Element-26\" type=\"math/tex\">L(\\mu)</script> </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "<li>=>  <strong>sum</strong> of <strong>squared distance</strong> between each observation and their <strong>closest centroid:</li> \n",
    "<li>$L(\\mu) = \\sum_{j=1}^{K}\\sum_{x_i \\in C_j}(||x_i - \\mu_j||^2)$</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kmeans in Practice\n",
    "\n",
    "Algorithm usually run a few times as results depends on initial random seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Two <code>sklearn</code> classes:\n",
    "<ul>\n",
    "<li><code>scikit.clustering.KMeans</code> </li>\n",
    "<li><code>scikit.clustering.MiniBatchKMeans</code> ‚Äî same but uses batch samples instead of the whole dataset, in order to go faster</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p>üí°Although not mandatory, applying PCA first helps to separate data more easily!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3 id=\"Choosing-Hyperparameter-K\">Choosing Hyperparameter K<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/camps/900/lectures/content/05-ML_06-Unsupervised-Learning.html#Choosing-Hyperparameter-K\">¬∂</a></h3><ul>\n",
    "<li>Choose <code>K</code> such that the inertia (<code>Kmeans().inertia_</code>) is minimized</li>\n",
    "<li>We can use the <strong>elbow method</strong> here as well</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertias = []\n",
    "ks = range(1,10)\n",
    "\n",
    "for k in ks:\n",
    "    km_test = KMeans(n_clusters=k, n_init=10).fit(X_proj_pca)\n",
    "    inertias.append(km_test.inertia_)\n",
    "\n",
    "plt.plot(ks, inertias)\n",
    "plt.xlabel('k cluster number');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We can choose either <code>k=2</code> or <code>k=3</code> given the elbow: but <code>k=3</code> seems more reasonable as it gives us a low inertia (plus in our case we do know we have 3 classes so we can cheat!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Application of KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_proj_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "km_pen_pca = KMeans(n_clusters=3, n_init=10, random_state=5)\n",
    "X_km_pen_pca = km_pen_pca.fit_transform(X_proj_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### KMeans projected in PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2)\n",
    "fig.set_figheight(6);fig.set_figwidth(15);\n",
    "axes[0].scatter(X_proj_pca.PC1, X_proj_pca.PC2, c=km_pen_pca.labels_);\n",
    "axes[0].set_xlabel('PC1'), axes[0].set_ylabel('PC2');\n",
    "axes[0].set_title('KMeans predictions')\n",
    "axes[1].scatter(X_proj_pca.PC1, X_proj_pca.PC2, c=y_train);\n",
    "axes[1].set_xlabel('PC1'), axes[1].set_ylabel('PC2');\n",
    "axes[1].set_title('Actual labels');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## DBSCAN (Density-based spatial clustering of applications with noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* based on the idea that a cluster in data space is a contiguous region of high point density, separated from other such clusters by contiguous regions of low point density.\n",
    "* Also uses a distance-based approach to clustering, i.e.  distance to the $k^{th}$ nearest neighbor: <code>k</code> is determined automatically\n",
    "* find the islands of higher density <code>data</code> amid a sea of sparser <code>noise</code>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=173nkWnbxMeAQFGr_j1AJgaXT4t0UBCXi\" style=\"width:1000\">\n",
    "<a href=\"https://github.com/NSHipster/DBSCAN\">Matt@GitHub</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Different types of points:\n",
    "* <code>Core</code> ‚Äî This is a point that has at least `min_samples` points within distance $\\epsilon$ from itself.\n",
    "* <code>Border</code> ‚Äî This is a point that has at least one Core point at a distance $\\epsilon$.\n",
    "* <code>Noise</code> ‚Äî This is a point that is neither a Core nor a Border. And it has less than `min_samples` points within distance $\\epsilon$ from itself.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=174ZeoyGgFwVoyYfermP3ZE1nVScsZxsF\" style=\"width:800\">\n",
    "<a href=\"https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html\">Chauhan, 2022<a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Algorithmic steps for DBSCAN clustering\n",
    "\n",
    "1. The algorithm proceeds by arbitrarily picking up a point in the dataset (until all points have been visited).\n",
    "2. If there are at least <code>min_samples</code> points within a radius of ‚ÄòŒµ‚Äô to the point then we consider all these points to be part of the same cluster.\n",
    "3. The clusters are then expanded by recursively repeating the neighborhood calculation for each neighboring point\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=16yz1lxuiBJBEpgG39P-bJbjbLaovk1B2\" style=\"width:1000px\">\n",
    "<a href=\"https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html\">Chauhan, 2022<a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* <code>min_samples</code>: The minimum number of samples for a cluster to be considered as such. Rule of thumb -> minimum <code>min_samples ‚â• D + 1</code>, where <code>D</code> is the dimensions of the dataset. Larger values (<code>min_samples = 2*D)</code> are usually better for data sets with noise and will yield more significant clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* <code>eps</code>: Œµ (<code>eps</code>) is the maximum distance between two samples for one to be considered as in the neighborhood of the other. In general, small values of Œµ are preferable, and as a rule of thumb, only a small fraction of points should be within this distance of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* <code>metric</code>: The choice of distance function (<code>metric</code>) is tightly linked to the choice of Œµ, and has a major impact on the outcomes. There is no estimation for this parameter, but the distance functions need to be chosen appropriately for the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit DBSCAN\n",
    "db_pca = DBSCAN(eps=0.6)\n",
    "db_pca.fit(X_proj_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### DBSCAN projected in PCA and UMAP Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2)\n",
    "fig.set_figheight(6);fig.set_figwidth(15);\n",
    "axes[0].scatter(X_proj_pca.PC1, X_proj_pca.PC2, c=db_pca.labels_);\n",
    "axes[0].set_xlabel('PC1'), axes[0].set_ylabel('PC2');\n",
    "axes[0].set_title('HDBSCAN predictions')\n",
    "axes[1].scatter(X_proj_pca.PC1, X_proj_pca.PC2, c=y_train);\n",
    "axes[1].set_xlabel('PC1'), axes[1].set_ylabel('PC2');\n",
    "axes[1].set_title('Actual labels');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### HDBSCAN (Hierarchical DBSCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "* More modern varian of <code>DBSCAN</code> \n",
    "* Incorporate some ideas from the algorithm <code>OPTICS</code>\n",
    "* Uses hierarchical clusters, unlike DBSCAN\n",
    "* While DBSCAN needs a minimum cluster size and a distance threshold epsilon as user-defined input parameters, HDBSCAN* is basically a DBSCAN implementation for varying epsilon values and therefore only needs the minimum cluster size as single input parameter.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=175xz3dJLkA6-MffU_vMlO1fs4R_lIoK_\" style=\"width:1000\">\n",
    "Read the paper for more details:<a href=\"https://link.springer.com/chapter/10.1007/978-3-642-37456-2_14\">Campelo et al, 2013</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit HDBSCAN\n",
    "from hdbscan import HDBSCAN\n",
    "hdb_pca = HDBSCAN(cluster_selection_method='leaf',cluster_selection_epsilon=0.62)\n",
    "hdb_pca.fit(X_proj_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### HDBSCAN projected in PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2)\n",
    "fig.set_figheight(6);fig.set_figwidth(15);\n",
    "axes[0].scatter(X_proj_pca.PC1, X_proj_pca.PC2, c=hdb_pca.labels_);\n",
    "axes[0].set_xlabel('PC1'), axes[0].set_ylabel('PC2');\n",
    "axes[0].set_title('HDBSCAN predictions')\n",
    "axes[1].scatter(X_proj_pca.PC1, X_proj_pca.PC2, c=y_train);\n",
    "axes[1].set_xlabel('PC1'), axes[1].set_ylabel('PC2');\n",
    "axes[1].set_title('Actual labels');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Outlier (Anomaly) Detection\n",
    "\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=175Y1o8Ruacf5DE99P7hLgHbQib841rKj\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a><br>\n",
    "    <br>Prompt: a single red fish isolated from a larger school of fish, digital art.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_outliers = X_train.copy()\n",
    "\n",
    "neg_out = pd.DataFrame(data=[X_train_outliers.min()*2], columns=X_train_outliers.columns)\n",
    "pos_out = pd.DataFrame(data=[X_train_outliers.max()*2], columns=X_train_outliers.columns)\n",
    "\n",
    "X_train_outliers = pd.concat([X_train_outliers, neg_out, pos_out], ignore_index=True)\n",
    "\n",
    "X_train_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Once-Class SVM (OCSVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "One-class SVM is a variation of the SVM that can be used in an unsupervised setting for anomaly detection. There are multiple implementations of this approach (see for instance <a href=\"https://proceedings.neurips.cc/paper/1999/hash/8725fb777f25776ffa9076e44fcfd776-Abstract.html\"> Sch√∂lkopt et al, 1999</a>).\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=17I1i08MrO77hEVViFIERFwkJXIFQw-07\" style=\"width:1000\">\n",
    "<a href=\"https://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html#sphx-glr-auto-examples-svm-plot-oneclass-py\">From Scikit-Learn Documentation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A regular SVM for classification finds a max-margin hyperplane that seperates the positive examples from the negative ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The one-class SVM simply finds the hyperplane at the center of mass of the dataset: if the data is standardized (zero mean and unit variance) this hyperplane will go through the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A new hyperparameter, <code>nu</code>, determines the fraction of samples furthest away from the boundary that should be considered as outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Some of Important Parameters:\n",
    "* <code>kernel</code>: as with SVM (linear, rbf, etc..)\n",
    "* <code>nu</code>: Controls the fraction of sample considered as outliers (distance from the margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "ocsvm = OneClassSVM().fit_predict(X_train_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12, 8))\n",
    "ax.scatter(X_train_outliers.culmen_depth_mm, X_train_outliers.flipper_length_mm, c=ocsvm, cmap=cm); \n",
    "ax.set_xlabel('culmen_depth_mm', size=16);ax.set_ylabel('flipper_length_mm', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Adjusted <code>nu</code> for 5% outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ocsvm = OneClassSVM(nu=0.95 * 0.01 +0.05).fit_predict(X_train_outliers)\n",
    "fig, ax = plt.subplots(1,1,figsize=(12, 8))\n",
    "ax.scatter(X_train_outliers.culmen_depth_mm, X_train_outliers.flipper_length_mm, c=ocsvm, cmap=cm); \n",
    "ax.set_xlabel('culmen_depth_mm', size=16);ax.set_ylabel('flipper_length_mm', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Isolation Forest (IFOR)\n",
    "\n",
    "Adaptation of the <code>RandomForest</code> classifier.\n",
    "<img src=\"https://drive.google.com/uc?id=16qpcOVTRKTvBZdOb726eBSM8jdUXhmjd\" style=\"width:1200px\">\n",
    "<a href=\"https://machinelearninggeek.com/outlier-detection-using-isolation-forests/\">Pandley, 2020</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Principle:** If data points end up in their own groups closer to the root of the tree, they are more different than the rest of the data: potential outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### Anomaly Score:\n",
    "<img src=\"https://drive.google.com/uc?id=17AgRWRSsp6gauVDlJX9UGkxUCWCMOCUW\"><br>\n",
    "From <a href=\"https://engineering.teknasyon.com/what-are-isolation-forests-151d8e98ef5f\">Kin, 2022</a><br>\n",
    "\n",
    "* h(x): path length of observation x\n",
    "* c(n): average path length of failed search of binary search tree and number of external nodes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Some of Important Parameters:\n",
    "* <code>Contamination</code>: It‚Äôs the most important parameter of IFOR. The amount of contamination of the data set, i.e. the proportion of outliers in the data set. It defines the threshold on the scores of the samples.\n",
    "* <code>n_estimators</code>: The number of base estimators in ensemble. Default value is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "ifor = IsolationForest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "outlier_prediction = ifor.fit_predict(X_train_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12, 8))\n",
    "ax.scatter(X_train_outliers.culmen_depth_mm, X_train_outliers.flipper_length_mm, c=outlier_prediction, cmap=cm); \n",
    "ax.set_xlabel('culmen_depth_mm', size=16);ax.set_ylabel('flipper_length_mm', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Adjusting <code>contamination</code> to reflect 1% of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ifor_tuned = IsolationForest(random_state=5, contamination=.01)\n",
    "outlier_prediction2 = ifor_tuned.fit_predict(X_train_outliers)\n",
    "fig, ax = plt.subplots(1,1,figsize=(12, 8))\n",
    "ax.scatter(X_train_outliers.culmen_depth_mm, X_train_outliers.flipper_length_mm, c=outlier_prediction2, cmap=cm); \n",
    "ax.set_xlabel('culmen_depth_mm', size=16);ax.set_ylabel('flipper_length_mm', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Using HDBSCAN <code>outlier_scores_</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<code>HDBSCAN</code> is based on the ***density*** of the datapoints in a hypervolume. The distance of each datapoint from a data high-density region can be turned into an <code>outlier score</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdbscan_od = HDBSCAN().fit(X_train_outliers)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(12, 8))\n",
    "cm = plt.colormaps['RdYlBu']\n",
    "im = ax.scatter(X_train_outliers.culmen_depth_mm, X_train_outliers.flipper_length_mm,c=hdbscan_od.outlier_scores_, cmap=cm); \n",
    "ax.set_xlabel('culmen_depth_mm', size=16);ax.set_ylabel('flipper_length_mm', size=16);\n",
    "plt.colorbar(im, label='Outlier Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Conclusions on anomaly detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*  üßø Anomaly is a judgement call: an outlier can still be a valid point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*  üí£ Whether a point is considered an outlier or not is highly dependent on the model selected and the choice of hyperparameter (**no silver bullet!**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*  üîÆ Always a good idea to combine several methods (**Ensemble**) to determine whether or not the datapoint is truly an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Suggested Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## üì∫ Videos \n",
    "* üìº <a href=\"https://youtu.be/12Xq9OLdQwQ\">Anomaly Detection: Algorithms, Explanations, Applications</a>, long lecture (> 1 hour) by Tom Dietterich, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## üìö Further Reading \n",
    "* üìñ <a href=\"https://serokell.io/blog/anomaly-detection-in-machine-learning\">What Is Anomaly Detection in Machine Learning?</a> by Yulia Gavrilova, 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## üíªüêç Time to Code ! "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "rise": {
   "scroll": true,
   "theme": "serif",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
