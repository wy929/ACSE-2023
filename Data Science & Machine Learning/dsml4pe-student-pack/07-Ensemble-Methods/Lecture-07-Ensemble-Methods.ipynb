{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Day 7**: Ensemble Learning ü´ê (***live in 1.47***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center><h1 style=\"color:maroon\">Ensemble Methods</h1>\n",
    "    <img src=\"https://drive.google.com/uc?id=15nHkPumClrtZSt2QMijeok-h06bkvF0L\" style=\"width:1300px\">\n",
    "    <h3><span style=\"color: #045F5F\">Data Science & Machine Learning for Planet Earth Lecture Series</span></h3><h6><i> by C√©dric M. John <span style=\"size:6pts\">(2023)</span></i></h6></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plan for today's Lecture üóì "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* <code>DecisionTree</code> for classification\n",
    "* <code>DecisionTree</code> for regression\n",
    "* Bagging algorithms: <code>RandomForest</code>\n",
    "* Boosting: <code>AdaBoost</code> and <code>xgboost</code>\n",
    "* Stacking algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Intended learning outcomes üë©‚Äçüéì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Understand variance in decision trees\n",
    "* Apply <code>RandomForest</code> and <code>xgboost</code> (some of the most powerful algorithms\n",
    "* Unleash the power of Ensemble methods on your problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Decision Trees\n",
    "<br>\n",
    "<center><img src=\"https://drive.google.com/uc?id=163OsC3pLMZwJvMrwpp_yrYwYnR8ETi-b\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a><br>\n",
    "    <br>Prompt: 3D rending of a glass Christmas bubble in purple colors with a tree visible in the reflection.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>Decision Trees are hierarchical supervised learning algorithms.</p>\n",
    "<ul>\n",
    "<li>Classification and Regression</li>\n",
    "<li>Non-linear modelling</li>\n",
    "<li>Break down the data through binary decisions</li>\n",
    "</ul>\n",
    "<p><img src=\"https://drive.google.com/uc?id=15k9rgeuN6lnBIKYHyVV6Rp_xtndtznAD\" style=\"margin:auto\" width=\"400\"/></p>\n",
    "<a href=\"https://medium.com/analytics-vidhya/ensemble-models-bagging-boosting-c33706db0b0b\">Silipo, 2020</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dataset\n",
    "\n",
    "<span style=\"color:teal\">**Let's start with a classic:** </span><a href=\"https://en.wikipedia.org/wiki/Iris_flower_data_set\">Iris Dataset (Fisher, 1936)</a><br>\n",
    "<img src=\"https://drive.google.com/uc?id=15f4Swq-LGEXPl128WepVMWNXF5bb0VCn\" style=\"width:1500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h2 id=\"1.1-üñ•-DecisionTreeClassifier\">1.1 üñ• <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\"><code>DecisionTreeClassifier</code></a><a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#1.1-%F0%9F%96%A5-DecisionTreeClassifier\">¬∂</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and prepare iris dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "X = data[['petal length (cm)', 'petal width (cm)']]\n",
    "y = data.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Instanciate and train model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=2)\n",
    "tree_clf.fit(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "# Export model graph\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree_clf, out_file=\"iris_tree.dot\", feature_names=X.columns,\n",
    "                class_names=['0','1','2'], rounded=True, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import model graph\n",
    "with open(\"iris_tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "    display(graphviz.Source(dot_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Jargon\n",
    "<p><img src=\"https://drive.google.com/uc?id=15dAvAf5YsBN4mH2E3twjzN5hfTxctyY7\" style=\"margin:auto\" width=\"1000\"/></p>\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/\">G√©ron, 2017</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Gini Index\n",
    "\n",
    "<p>The Gini index measures the ability of each feature to <strong>separate</strong> the data.</p>\n",
    "<p>It calculates the <strong>impurity</strong> of each node, between [0,1]. The lower the better</p><br>\n",
    "$$Gini(node)=1-\\sum{{p_i}^2}$$\n",
    "<br><p>Where $p_i$ is the ratio of observation of being of class $i$ at each node</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<p><img src=\"https://drive.google.com/uc?id=15dAvAf5YsBN4mH2E3twjzN5hfTxctyY7\" style=\"margin:auto\" width=\"1000\"/></p>\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/\">G√©ron, 2017</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate gini of root node\n",
    "1 - 3*(50/150)**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calcultate gini green leaf\n",
    "1 - 0**2 - (49/54)**2 - (5/54)**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 id='\"Growing\"-a-tree?'>\"Growing\" a tree?<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#%22Growing%22-a-tree?\">¬∂</a></h3><p>The tree structure is decided through the following steps:</p>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Start at root node containing all your dataset</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Try various combination of <strong>(feature, threshold)</strong> tuples. Each would split your dataset into 2 child nodes</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>For each combination, compute <strong>weighted average gini index</strong> of both child nodes (weighted by number of instances)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Select (feature, threshold) yielding the <strong>lowest</strong> index (i.e the \"purest child nodes\")</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Split dataset in two using this rule. Repeat step 2 for both subsets.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<li>Stop when no feature improves node impurity (...at what risk?)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3 id=\"Predicting\">Predicting</h3><ul>\n",
    "<li>A new point is passed through the tree from top down until it reaches a leaf. </li>\n",
    "<li>It is predicted to correspond to the most represented class in that leaf. </li>\n",
    "</ul>\n",
    "<p>E.g. New point: <code>X_new = [4(length), 1(width)]</code></p>\n",
    "<p><img src=\"https://drive.google.com/uc?id=15vJKlMg97r9QGVr1njq9wtCwpfL6209F\" style=\"margin:auto\" width=\"1000\"/></p>\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/\">G√©ron, 2017</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's predict a flower from the top right quardrant\n",
    "print(tree_clf.predict([[4,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Predict proba is just the ratio of flowers in this leaf/quadrant\n",
    "print(tree_clf.predict_proba([[4,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>‚ö†Ô∏è 91% is not really a \"probability\"</p>\n",
    "<p>Trees are not <em>calibrated</em> probability classifiers as opposed to logistic regression</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3 id='Think-about-decision-trees-as-\"orthogonal\"-classifiers'>Think about decision trees as \"orthogonal\" classifiers<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#Think-about-decision-trees-as-%22orthogonal%22-classifiers\">¬∂</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from utils import plot_decision_regions\n",
    "\n",
    "plot_decision_regions(X, y, classifier=tree_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<p><img src=\"https://drive.google.com/uc?id=15kUHhYRxXlap6skLxn8lTZW60EzgZEth\" style=\"width:1700px\"></p>\n",
    "<a href=\"https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\">scikit-learn doc</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h2 id=\"1.2-DecisionTreeRegressor\">1.2 <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\"><code>DecisionTreeRegressor</code></a><a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#1.2-DecisionTreeRegressor\">¬∂</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>Regression trees consist of predicting a continuous value. They are \"grown\" differently than classification trees.<br>\n",
    "<img src=\"https://drive.google.com/uc?id=15qT_ZdPYUSBVuyjiIgMdpx7xIjVbhW99\" style=\"margin:auto\" width=\"1300\"/></p>\n",
    "<p><a href=\"https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw\">John Starmer, 2019</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3 id=\"Growing-the-Regression-tree\">Growing the Regression tree<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#Growing-the-Regression-tree\">¬∂</a></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p><img src=\"https://drive.google.com/uc?id=15dBpVBe0lMXW9zyCxAafxhT0tqlLKmo7\" style=\"margin:auto\" width=\"900\"/></p>\n",
    "<p><a href=\"https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw\">John Starmer, 2019</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<ul>\n",
    "<li>Select a threshold </li>\n",
    "<li>Compute the <strong>SSR residuals</strong> between average and true values on both side</li>\n",
    "<li>Compute weighted-average sum of SSR on both sides, weighted by number of datapoints</li>\n",
    "</ul>\n",
    "<p><img src=\"https://drive.google.com/uc?id=161miEuFtVIvECmFu6JTi5pBbngUuo78R\" style=\"margin:auto\" width=\"1300\"/></p>\n",
    "<p><a href=\"https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw\">John Starmer, 2019</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<p>Compute residuals for next hypothetical threshold\n",
    "<img src=\"https://drive.google.com/uc?id=15kzAmCrcVlH-UUAb-_nG8Q6xnA-6X-7q\" style=\"margin:auto\" width=\"1300\"/></p>\n",
    "<p><a href=\"https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw\">John Starmer, 2019</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<p>The threshold that minimizes residuals becomes the Root node of the tree</p>\n",
    "<p><img src=\"https://drive.google.com/uc?id=15r-VyXdAKjoohkk95mfbxYahqKYmN18J\" style=\"margin:auto\" width=\"1300\"/></p>\n",
    "<p><a href=\"https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw\">John Starmer, 2019</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<p><img src=\"https://drive.google.com/uc?id=1602ORKB0EBnAubQSlEetp56Et6eI3viE\" style=\"margin:auto\" width=\"1300\"/></p>\n",
    "<p><a href=\"https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw\">John Starmer, 2019</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>We could further split the points below a dosage of 14.5, but we probably shouldnt. Why?</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<p>‚ö†Ô∏è We would be <strong>overfitting</strong>!</p>\n",
    "<p><img src=\"https://drive.google.com/uc?id=15qVShonMz3NSTbl3YRbsoJwYLIvYi4fq\" style=\"margin:auto\" width=\"900\"/></p>\n",
    "<p><a href=\"https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw\">John Starmer, 2019</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<p>Instead, stop splitting and use the average value of points within group to <strong>generalize</strong>.</p>\n",
    "<p><img src=\"https://drive.google.com/uc?id=16-_OA1JoaPupXfjhRg2Wvn0l9PZHVC_t\" style=\"margin:auto\" width=\"1300\"/></p>\n",
    "<p><a href=\"https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw\">John Starmer, 2019</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3 id=\"‚ö†Ô∏è-Controlling-overfitting\">‚ö†Ô∏è Controlling overfitting<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#%E2%9A%A0%EF%B8%8F-Controlling-overfitting\">¬∂</a></h3><ul>\n",
    "<li><strong>Decision trees must be tuned!!</strong></li>\n",
    "<li>Default parameters will almost certainly overfit</li>\n",
    "<li>Control split</li>\n",
    "<li>Control tree depth</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h4 id=\"min_samples_split\"><code>min_samples_split</code></h4><ul>\n",
    "<li>Specify the minimum number of samples required to split an internal node</li>\n",
    "<li>In the example, <code>min_samples_split</code> is set to 7</li>\n",
    "</ul>\n",
    "<p><img  src=\"https://drive.google.com/uc?id=15qT_ZdPYUSBVuyjiIgMdpx7xIjVbhW99\" style=\"margin:auto\" width=\"1300\"/></p>\n",
    "<p><a href=\"https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw\">John Starmer, 2019</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h4 id=\"max_depth\"><code>max_depth</code><a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#max_depth\">¬∂</a></h4><ul>\n",
    "<li>The maximum depth of the tree</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<h4 id=\"min_samples_leaf\"><code>min_samples_leaf</code><a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#min_samples_leaf\">¬∂</a></h4><ul>\n",
    "<li>The minimum number of samples required to be at a leaf node.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h2 id=\"üíª--Variance-illustrated\">üíª  Variance illustrated<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#%F0%9F%92%BB--Variance-illustrated\">¬∂</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Regression\n",
    "We will use a dataset of <span style=\"color:teal\">**Greenhouse Temperatures** </span>to illustrate regression with DecisionTrees.<br>\n",
    "<img src=\"https://drive.google.com/uc?id=15xoFfJBG6HBvuWyv0L0Qpc8KMhl34aSs\" style=\"width:1500px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reg_data = pd.read_csv('Lecture_data/greenhouse.csv')\n",
    "reg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reg_X = reg_data[['Overall Height', 'Glazing Area']]\n",
    "reg_y = reg_data['Average Temperature']\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(reg_X, reg_y, train_size=0.7, random_state=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "\n",
    "cv_results = cross_validate(tree, reg_X, reg_y, scoring = \"neg_mean_squared_error\", cv=60)\n",
    "\n",
    "print('std: ', cv_results['test_score'].std())\n",
    "print('mean: ',cv_results['test_score'].mean()*-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cv_results['test_score']*-1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Classification\n",
    "\n",
    "<span style=\"color:teal\">**We will use a dataset of pH vs water chemistry for classification** </span><br>\n",
    "<img src=\"https://drive.google.com/uc?id=15ghba_8Hus87XNDl5YoMuTBMpCItXy8Q\" style=\"width:1500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class_data = pd.read_csv('Lecture_data/geochem.csv')\n",
    "class_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = class_data.drop(columns='pH')\n",
    "y = class_data['pH']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train['Ca_ICP_PCT'],X_train['K_ICP_PCT'], c=y_train)\n",
    "plt.xlabel('Ca_ICP_PCT')\n",
    "plt.ylabel('K_ICP_PCT');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from ipywidgets import interact, IntSlider\n",
    "from utils import plot_decision_regions\n",
    "\n",
    "@interact(max_depth=IntSlider(min=1, max=15, step=1, value=3))\n",
    "def plot_classifier(max_depth):\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    clf.fit(X_train.values, y_train.values)\n",
    "    print(f'Test Accuracy: {clf.score(X_test.values, y_test.values)}')\n",
    "    plot_decision_regions(X_train, y_train, classifier=clf, y_pad=.0,x_pad=.2)\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h2 id=\"Pros-and-cons-of-Decision-Trees\">Pros and cons of Decision Trees<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#Pros-and-cons-of-Decision-Trees\">¬∂</a></h2><p>üëç Advantages</p>\n",
    "<ul>\n",
    "<li>No scaling necessary</li>\n",
    "<li>Resistant to outliers</li>\n",
    "<li>Intuitive and interpretable</li>\n",
    "<li>Allow feature selection (see gini-based <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_\"><code>feature_importance_</code></a>)</li>\n",
    "<li>Non-Linear modelisation</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>üëé Disadvantages</p>\n",
    "<ul>\n",
    "<li>High variance (i.e small change in data has a big change in the tree structure)</li>\n",
    "<li>Long training time if grown up to max depth <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"mjx-chtml MathJax_CHTML\" data-mathml='&lt;math xmlns=\"http://www.w3.org/1998/Math/MathML\"&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=\"false\"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mrow class=\"MJX-TeXAtom-ORD\"&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mrow class=\"MJX-TeXAtom-ORD\"&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=\"false\"&gt;)&lt;/mo&gt;&lt;/math&gt;' id=\"MathJax-Element-4-Frame\" role=\"presentation\" style=\"font-size: 116%; position: relative;\" tabindex=\"0\"><span aria-hidden=\"true\" class=\"mjx-math\" id=\"MJXc-Node-20\"><span class=\"mjx-mrow\" id=\"MJXc-Node-21\"><span class=\"mjx-mi\" id=\"MJXc-Node-22\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.496em; padding-bottom: 0.311em;\">O</span></span><span class=\"mjx-mo\" id=\"MJXc-Node-23\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.434em; padding-bottom: 0.619em;\">(</span></span><span class=\"mjx-msubsup\" id=\"MJXc-Node-24\"><span class=\"mjx-base\" style=\"margin-right: -0.085em;\"><span class=\"mjx-mi\" id=\"MJXc-Node-25\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.434em; padding-bottom: 0.249em; padding-right: 0.085em;\">N</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" id=\"MJXc-Node-26\" style=\"\"><span class=\"mjx-mrow\" id=\"MJXc-Node-27\"><span class=\"mjx-mi\" id=\"MJXc-Node-28\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.249em; padding-bottom: 0.311em;\">o</span></span><span class=\"mjx-mi\" id=\"MJXc-Node-29\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.496em; padding-bottom: 0.311em;\">b</span></span><span class=\"mjx-mi\" id=\"MJXc-Node-30\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.249em; padding-bottom: 0.311em;\">s</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\" id=\"MJXc-Node-31\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.188em; padding-bottom: 0.311em;\">‚àó</span></span><span class=\"mjx-msubsup MJXc-space2\" id=\"MJXc-Node-32\"><span class=\"mjx-base\"><span class=\"mjx-mi\" id=\"MJXc-Node-33\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.249em; padding-bottom: 0.311em;\">m</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" id=\"MJXc-Node-34\" style=\"\"><span class=\"mjx-mrow\" id=\"MJXc-Node-35\"><span class=\"mjx-mi\" id=\"MJXc-Node-36\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.496em; padding-bottom: 0.496em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mi\" id=\"MJXc-Node-37\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.249em; padding-bottom: 0.311em;\">e</span></span><span class=\"mjx-mi\" id=\"MJXc-Node-38\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.249em; padding-bottom: 0.311em;\">a</span></span><span class=\"mjx-mi\" id=\"MJXc-Node-39\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.434em; padding-bottom: 0.311em;\">t</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\" id=\"MJXc-Node-40\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.188em; padding-bottom: 0.311em;\">‚àó</span></span><span class=\"mjx-mi MJXc-space2\" id=\"MJXc-Node-41\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.496em; padding-bottom: 0.311em; padding-right: 0.003em;\">d</span></span><span class=\"mjx-mi\" id=\"MJXc-Node-42\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.249em; padding-bottom: 0.311em;\">e</span></span><span class=\"mjx-mi\" id=\"MJXc-Node-43\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.249em; padding-bottom: 0.496em;\">p</span></span><span class=\"mjx-mi\" id=\"MJXc-Node-44\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.434em; padding-bottom: 0.311em;\">t</span></span><span class=\"mjx-mi\" id=\"MJXc-Node-45\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.496em; padding-bottom: 0.311em;\">h</span></span><span class=\"mjx-mo\" id=\"MJXc-Node-46\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.434em; padding-bottom: 0.619em;\">)</span></span></span></span><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msub><mi>N</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msub><mo>‚àó</mo><msub><mi>m</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi></mrow></msub><mo>‚àó</mo><mi>d</mi><mi>e</mi><mi>p</mi><mi>t</mi><mi>h</mi><mo stretchy=\"false\">)</mo></math></span></span><script id=\"MathJax-Element-4\" type=\"math/tex\">O(N_{obs}*m_{feat}*depth)</script></li>\n",
    "<li>Split data \"orthogonally\" to feature directions (use PCA upfront to \"orient\" data)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Ensemble Methods\n",
    "<br>\n",
    "<center><img src=\"https://drive.google.com/uc?id=15vBdptV3FxSd9aT4B06yiXxWW-m5hBRq\" style=\"width:900px;\"><br>\n",
    "Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a><br>\n",
    "Prompt: A photo of a forest in autumn with colorful leaves and many animals in the branches. <br>¬© C√©dric John, 2022.</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Boostrap Aggregation\n",
    "\n",
    "<p>Bootstrap aggregating, also known as Bagging, is the aggregation of multiple versions of a model.</p>\n",
    "<ul>\n",
    "<li>It is a <strong>parallel</strong> ensemble method</li>\n",
    "<li>The aim of bagging is to <strong>reduce variance</strong></li>\n",
    "<li>Each version of the model is called a <strong>weak learner</strong></li>\n",
    "<li>Weak learners are trained on <strong>boostrapped</strong> samples of the dataset</li>\n",
    "</ul>\n",
    "<p><img align=\"center\" src=\"https://drive.google.com/uc?id=15karOVgKRq-AYmpfPcTW3oNQjesoJRPB\" style=\"margin:auto\" width=\"900\"/></p>\n",
    "<a href=\"https://medium.com/analytics-vidhya/ensemble-models-bagging-boosting-c33706db0b0b\">Silipo, 2020</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h2 id=\"Bootstrapping\">Bootstrapping</h2><ul>\n",
    "<li>Generating \"bootstrapped\" samples from the given dataset</li>\n",
    "<li>The samples are created by randomly drawing the data points with replacement.</li>\n",
    "<li>Features can also be randomly filtered to increase bagging diversity</li>\n",
    "</ul>\n",
    "<p><img align=\"center\" src=\"https://drive.google.com/uc?id=15e76Tpt-yb71fJ-1G0TdHQ7yP1iIdFzE\" style=\"margin:auto\" width=\"1000\"/></p>\n",
    "<a href=\"https://towardsdatascience.com/seeing-the-forest-for-the-trees-an-introduction-to-random-forest-41a24fc842ac\">Firmin, 2019</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h2 id=\"Random-Forests-=-Bagged-Trees\">Random Forests = Bagged Trees</h2><p>Random Forests are a Bagged ensemble of Decision trees.</p>\n",
    "<p><img align=\"center\" src=\"https://drive.google.com/uc?id=15gn_isa8pgkTkvRwowwR_QmrujKAvCfY\" style=\"margin:auto\" width=\"1000\"/></p>\n",
    "<a href=\"https://towardsdatascience.com/seeing-the-forest-for-the-trees-an-introduction-to-random-forest-41a24fc842ac\">Firmin, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>Prediction are averaged (for regression) or voted (classification)</p>\n",
    "<p><img align=\"center\" src=\"https://drive.google.com/uc?id=162UCDj9_FNbvR2dNLiRBNTMZcGBW2GVS\" style=\"margin:auto\" width=\"1000\"/></p>\n",
    "<a href=\"https://www.researchgate.net/publication/301638643_Electromyographic_Patterns_during_Golf_Swing_Activation_Sequence_Profiling_and_Prediction_of_Shot_Effectiveness\">Verikas et al, 2016</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3 id=\"üíª-Sklearn-RandomForestRegressor-and-Classifier\">üíª Sklearn <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">RandomForestRegressor</a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">Classifier</a><a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#%F0%9F%92%BB-Sklearn-RandomForestRegressor-and-Classifier\">¬∂</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "cv_results = cross_validate(forest, reg_X, reg_y, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "print(cv_results['test_score'])\n",
    "print('mean mse: ',-1*cv_results['test_score'].mean())\n",
    "print('std mse: ', cv_results['test_score'].std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "@interact(max_depth=IntSlider(min=1, max=30, step=1, value=3))\n",
    "def plot_classifier(max_depth):\n",
    "    clf = RandomForestClassifier(max_depth=max_depth)\n",
    "    clf.fit(X_train.values, y_train.values)\n",
    "    print(f'Test Accuracy: {clf.score(X_test.values, y_test.values)}')\n",
    "    plot_decision_regions(X_train, y_train, classifier=clf, y_pad=.0,x_pad=.2)\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h2 id=\"üíª-Bagging-any-algorithm-!\">üíª Bagging any algorithm !<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#%F0%9F%92%BB-Bagging-any-algorithm-!\">¬∂</a></h2><p>Bagging can be implemented on any algorithm using <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html\"><code>BaggingRegressor</code></a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\"><code>BaggingClassifier</code></a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "bagged_model = BaggingRegressor(linear_model, n_estimators=50)\n",
    "\n",
    "cv_results = cross_validate(bagged_model, reg_X, reg_y, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "print('mean mse: ',-1*cv_results['test_score'].mean())\n",
    "print('std dev: ', cv_results['test_score'].std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h4 id=\"Out-of-Bag-samples\">Out-of-Bag samples<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#Out-of-Bag-samples\">¬∂</a></h4><p>Sample not \"drawn\" by the bagging can be used to give a pseudo \"test\" score</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bagged_model = BaggingRegressor(\n",
    "    linear_model,\n",
    "    n_estimators=50,\n",
    "    oob_score=True\n",
    ")\n",
    "\n",
    "bagged_model.fit(reg_X,reg_y).oob_score_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3 id=\"Pros-and-cons-of-Bagging\">Pros and cons of Bagging<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#Pros-and-cons-of-Bagging\">¬∂</a></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>üëç Advantages:</p>\n",
    "<ul>\n",
    "<li>Reduces variance/overfitting</li>\n",
    "<li>Can be applied to any model</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>üëé Disadvantages</p>\n",
    "<ul>\n",
    "<li>Complex structure</li>\n",
    "<li>High training time</li>\n",
    "<li>Disregards the performance of individual sub-models</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 id=\"3.-Boosting\">3. Boosting<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#3.-Boosting\">¬∂</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>Boosting is a sequential ensemble method made up of weak learners that learn from their predecessor's mistakes.</p>\n",
    "<ul>\n",
    "<li>It is a <strong>sequential</strong> ensemble method</li>\n",
    "<li>The aim of boosting is to <strong>reduce bias</strong></li>\n",
    "<li>Focuses on the observations that are harder to predict</li>\n",
    "<li>The best weak learners are given more weight in the final vote</li>\n",
    "</ul>\n",
    "<p><img align=\"center\" src=\"https://drive.google.com/uc?id=15karOVgKRq-AYmpfPcTW3oNQjesoJRPB\" style=\"margin:auto\" width=\"1000\"/></p>\n",
    "<a href=\"https://medium.com/analytics-vidhya/ensemble-models-bagging-boosting-c33706db0b0b\">Silipo, 2020</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h2 >AdaBoost (Adaptative Boosting)</h2><p><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\"><code>AdaBoostRegressor</code></a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\"><code>AdaBoostClassifier</code></a></p>\n",
    "<p><strong>One implementation</strong> of boosting that works particularly well with trees.</p>\n",
    "<p><img align=\"center\" src=\"https://drive.google.com/uc?id=15xeV5WyvNmkNX0LpdW1zImAjJZ7TbFdC\" style=\"margin:auto\" width=\"1300\"/></p>\n",
    "<a href=\"https://chrisalbon.com/code/machine_learning/trees_and_forests/adaboost_classifier/\">Albon, 2017</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3 id=\"üíª-AdaBoosted-Trees-in-Sklearn\">üíª AdaBoosted Trees in Sklearn<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#%F0%9F%92%BB-AdaBoosted-Trees-in-Sklearn\">¬∂</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "adaboost = AdaBoostRegressor(DecisionTreeRegressor(max_depth=3))\n",
    "\n",
    "cv_results = cross_validate(adaboost, reg_X, reg_y, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "print('mean mse: ',-1*cv_results['test_score'].mean())\n",
    "print('std dev: ', cv_results['test_score'].std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "@interact(n_estimators=[10, 30, 50,100], max_depth=IntSlider(min=1, max=30, step=1, value=2))\n",
    "def plot_classifier(n_estimators, max_depth):\n",
    "    model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=max_depth),\n",
    "                               n_estimators=n_estimators)    \n",
    "    model.fit(X_train.values, y_train.values)\n",
    "    print(f'Test Accuracy: {model.score(X_test.values, y_test.values)}')\n",
    "    plot_decision_regions(X_train, y_train, classifier=model,  y_pad=.0,x_pad=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h2 id=\"3.2-Gradient-Boosting-üî•\">3.2 Gradient Boosting üî•<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#3.2-Gradient-Boosting-%F0%9F%94%A5\">¬∂</a></h2><ul>\n",
    "<li>Only implemented for trees</li>\n",
    "<li>Generally more performant than AdaBoost</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>Instead of updating the weights of observations misclassified...</p>\n",
    "<ol>\n",
    "<li>Recursively fit each weak-learner on the <strong>residuals</strong> of the previous one</li>\n",
    "<li>Then <strong>adds</strong> all the predictions of each weak learners (for regression)</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>For classification, ~ similar principle but in the logit space (if loss chosen is log-loss)</p>\n",
    "<p>üìö Read sklearn <a href=\"https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting\">user guide</a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_reg, y_train_reg)\n",
    "print(f'Test Accuracy: {model.score(X_test_reg, y_test_reg)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### XGBOOST\n",
    "\n",
    "* Extreme Gradient Tree Boosting\n",
    "* Dedicated library, optimized for this task\n",
    "* Nice features inspired from Deep Learning\n",
    "\n",
    "<a href=\"https://xgboost.readthedocs.io/en/latest/\">See the XGBOOST Documentation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "xgb_reg = XGBRegressor(early_stopping_rounds=15,\n",
    "    eval_metric=mean_squared_error)\n",
    "\n",
    "xgb_reg.fit(\n",
    "    X_train_reg, y_train_reg,\n",
    "    # evaluate loss at each iteration\n",
    "    eval_set=[(X_test_reg, y_test_reg)], \n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h2 id=\"Pros-and-cons-of-boosting\">Pros and cons of boosting<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#Pros-and-cons-of-boosting\">¬∂</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>üëç Advantages:</p>\n",
    "<ul>\n",
    "<li>Strong sub-models have more influence in final decision</li>\n",
    "<li>Reduce bias</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>üëé Disadvantages:</p>\n",
    "<ul>\n",
    "<li>Computationally expensive (sequential)</li>\n",
    "<li>Easily overfit</li>\n",
    "<li>Sensitive to outliers (too much time spent trying to correctly predict them)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h2 id=\"Ensemble-methods-recap\">Ensemble methods recap</h2><p>Ensemble learning combines several base algorithms (e.g. Decision trees) to form one optimized predictive algorithm. Ensemble methods can be broken down into two categories:</p>\n",
    "<ul>\n",
    "<li><strong>Parallel Learners</strong>:  different models are trained in parallel and their predictions are aggregated (e.g. Random Forest)</li>\n",
    "</ul>\n",
    "<ul>\n",
    "<li><strong>Sequential Learners</strong>: different models are trained sequentially and the mistakes of previous models are learned by their successors (e.g. Boosted trees)</li>\n",
    "</ul>\n",
    "<p><img align=\"center\" src=\"https://drive.google.com/uc?id=15karOVgKRq-AYmpfPcTW3oNQjesoJRPB\" style=\"margin:auto\" width=\"1000\"/></p>\n",
    "\n",
    "<a href=\"https://medium.com/analytics-vidhya/ensemble-models-bagging-boosting-c33706db0b0b\">Silipo, 2020</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Models Stacking\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=163V9vbFGzIyUZGwy3xCA7A0J_gLrVVXj\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a><br>\n",
    "<br>Prompt: A very high stack of delicious looking pancakes with glass-looking mapple sirup, creamy butter, and colorful rasberries, soft lighting.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>Stacking consists of training different algorithms and aggregating their predictions.</p>\n",
    "<ul>\n",
    "<li>Different algorithms capture different structures of data</li>\n",
    "<li>Combining sometimes enhances the predictive power</li>\n",
    "<li>The results are aggregated by voting (classification) or averaging (regression)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3 id=\"Simple-aggregation\">Simple aggregation<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#Simple-aggregation\">¬∂</a></h3><p>sklearn <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\">VotingClassifier</a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html\">VotingRegressor</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p><img align=\"center\" src=\"https://drive.google.com/uc?id=15iJYy1CHpzkPFJo0OO4K9u0K6RjzHIr3\" style=\"margin:auto\" width=\"1300\"/></p>\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/\">G√©ron, 2017-2022</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "forest = RandomForestClassifier(max_depth=2)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators = [(\"rf\", forest),(\"lr\", logreg)],\n",
    "    voting = 'soft', # to use predict_proba of each classifier before voting\n",
    "    weights = [1,1] # to equally weight forest and logreg in the vote\n",
    ")\n",
    "ensemble.fit(X_train.values, y_train.values)\n",
    "print(f'Test Accuracy: {ensemble.score(X_test.values, y_test.values)}')\n",
    "plot_decision_regions(X_train, y_train, classifier=ensemble,  y_pad=.0,x_pad=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3 id=\"Multi-layer-stacking!\">Multi-layer stacking!<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_07-Ensemble-Methods.html?title=Ensemble+Methods&amp;program_id=10#Multi-layer-stacking!\">¬∂</a></h3><p>sklearn <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html\">StackingClassifier</a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html\">StackingRegressor</a></p>\n",
    "<p>Train a <strong>final estimator</strong> on the predictions of the previous ones</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "ensemble = StackingClassifier(\n",
    "    estimators = [(\"rf\", RandomForestClassifier(max_depth=2)),\n",
    "                  (\"knn\", KNeighborsClassifier(n_neighbors=10))],\n",
    "    final_estimator = LogisticRegression())\n",
    "\n",
    "ensemble.fit(X_train.values, y_train.values)\n",
    "print(f'Test Accuracy: {ensemble.score(X_test.values, y_test.values)}')\n",
    "plot_decision_regions(X_train, y_train, classifier=ensemble, y_pad=.0,x_pad=.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Suggested Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## üì∫ Videos \n",
    "#### Short videos from my Undegraduate Machine Learning Classes:\n",
    "* üìº <a href=\"https://youtu.be/FHk46klXgZs?list=PLZzjCZ3QdgQCcRIwQdd-_cJNAUgiEBB_n\">Decision Trees</a>\n",
    "* üìº <a href=\"https://youtu.be/v15RrzYaXqY?list=PLZzjCZ3QdgQCcRIwQdd-_cJNAUgiEBB_n\">RandomForest</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## üìö Further Reading \n",
    "* üìñ <a href=\"https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f\">Ensemble Methods in Machine Learning: What are They and Why Use Them?</a> by Evan Lutins\n",
    "* üìñ <a href=\"https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/\">A Gentle Introduction to Ensemble Learning Algorithms</a> by Jason Brown, 2021\n",
    "* üìñ <a href=\"https://towardsdatascience.com/understanding-random-forest-58381e0602d2\">Understanding Random Forest - How the Algorithm Works and Why it Is So Effective</a> by Tony Yiu, 2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# üíªüêç Time to Code ! "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "rise": {
   "scroll": true,
   "theme": "serif",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
