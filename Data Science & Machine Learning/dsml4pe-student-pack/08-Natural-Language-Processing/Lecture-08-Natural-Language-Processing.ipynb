{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Day 8**: Natural Language Processing üì∞ (***live in 1.49/1.50***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center><h1 style=\"color:maroon\">Natural Language Processing</h1>\n",
    "    <img src=\"https://drive.google.com/uc?id=16bKE6mY9y66IJazCHGXkp3oLkTMC_-Wt\" style=\"width:1300px;\">\n",
    "    <h3><span style=\"color: #045F5F\">Data Science & Machine Learning for Planet Earth Lecture Series</span></h3><h6><i> by C√©dric M. John <span style=\"size:6pts\">(2023)</span></i></h6></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Plan for today's Lecture üóì "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Overview of text preprocessing\n",
    "* Text Vectorizing: different approaches\n",
    "* Text Embedding for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Intended learning outcomes üë©‚Äçüéì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Confidently preprocess text\n",
    "* Know what text vectorizing / embedding to choose\n",
    "* Transfer learning for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Text Preprocessing\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=16eNcTP_nqAT6mptg-Xj2u8nVq_D1Afc6\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a><br>\n",
    "<br>Prompt: Snowflakes looking like pure glass and in the shape of typefont characters falling, digital art, 4k photo, warm golden lighting.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h1 >Natural Language Processing</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h3>Language model </h3><p>A language model is a model which attempts to predict the next word or character given an input list of words or characters.</p>\n",
    "<p><img alt=\"Embedding\" src=\"https://drive.google.com/uc?id=16gaqOOrYNaY-xIkMhZi3dMSwJh8sfjVM\" width=\"800px\"/></p>\n",
    "(<a hlink=\"https://colab.research.google.com/github/dipanjanS/nlp_workshop_odsc19/blob/master/Module05%20-%20NLP%20Applications/Project08%20-%20Text%20Generation%20with%20Transformers.ipynb\">Image from Google</a>).\n",
    "<p>Such models are present on your phone when it is suggesting you next words.</p>\n",
    "<p>It is easy to implement, but it is not necessarily easy to obtain something meaningful !</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h3>Text classification, such as sentiment analysis</a></h3><p>Classification depending on a word, a sentence, a paragraph, ...</p>\n",
    "<p><img src=\"https://drive.google.com/uc?id=16JczPbBtae-f4kBW2F9A8AsaQdlYw0Xa\" width=\"1000px\"/></p>\n",
    "<p>The typical setting is <strong>sentiment analysis</strong>: Classify positive or negative sentence (but also happiness, sadness, joy, anger, ...).</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Reminder: Data Preprocessing\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=16CMa7VrgYDUjGlnrGR8t_D0yvJuGGkdd\" style=\"width:1600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=16Mwahg3hu2xW8ImaOax8ycGkwmy14283\" style=\"width:1600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h1>NLTK: the Natural Language Preprocessing Toolkit</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>Natural Language Toolkit (NLTK) is a library that provides preprocessing and modelling tools for text data.</p>\n",
    "<p><a href=\"https://www.nltk.org/\">NLTK</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<h2>Installing NLTK</h2>\n",
    "<p>For work in Colab, in your notebook, type the following:</p>\n",
    "<p><code>!pip install nltk</code></p>\n",
    "<p><a href=\"https://www.nltk.org/install.html\">Installation Documentation</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dataset\n",
    "\n",
    "<span style=\"color:teal\">**Today's data is:** </span><a href=\"www.twitter.com\">Twitter (now \"X\") Climate Change Sentiment Analysis</a> (modified)\n",
    "<img src=\"https://drive.google.com/uc?id=16mhEA5-Jdi3z5zvWYNky8lzs6ou8hPwo\" style=\"width:1500px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Lecture_data/twitter_sentiment_data_mod.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.message\n",
    "y = data.sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3>üñ• Lowercase</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>For two words to be considered the same they need to have the same casing.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = X_train.iloc[1018]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = text.lower() \n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3>üñ• Numbers</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "<p>Depending on the task, numbers may need to be removed as part of preprocessing.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_numbers(txt):\n",
    "    txt = ''.join(word for word in txt if not word.isdigit())\n",
    "    return txt\n",
    "\n",
    "text = remove_numbers(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>üëç Numbers are useful for date extraction</p>\n",
    "<p>üëé Not useful for topic modelling (or sentiment analysis)</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3>üñ• Punctuation</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Like numbers, punctuation may need to be removed as part of preprocessing.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(txt):\n",
    "    \n",
    "    for punctuation in string.punctuation:\n",
    "        txt = txt.replace(punctuation, '') \n",
    "    \n",
    "    return txt\n",
    "     \n",
    "text = remove_punctuation(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>üëç Punctuation is useful for authorship attribution (e.g. the style of writing) and text generation</p>\n",
    "<p>üëé Not useful for topic modelling (sentiment analysis)</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<p>‚ö†Ô∏è Punctuation is rarely respected in modern text forms (e.g. social media). Best to remove it if the style is not consistent.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3>Tokenizing</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Tokenizing means transforming a single string into a list of words, also called word tokens. For preprocessing tasks dealing with entire words, you will need to tokenize you text.</p>\n",
    "<p><a href=\"https://www.nltk.org/api/nltk.tokenize.html\">NLTK's <code>tokenize</code> module documentation</a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(txt):\n",
    "    word_tokens = word_tokenize(txt) \n",
    "    return word_tokens\n",
    "\n",
    "text = tokenize(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h3>üñ• Stopwords</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>\"Stopwords\" are words are so frequently used that for many tasks , they don't carry much information. NLTK has an inbuilt corpus of english stopwords that can be loaded and used.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(word_tokens):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = [w for w in word_tokens if not w in stop_words] \n",
    "    return word_tokens\n",
    "\n",
    "text = remove_stopwords(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<p>üëç Removing stopwords is useful for topic modelling, sentiment analysis</p>\n",
    "<p>üëé Counterproductive for authorship attribution or text generation</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h3>üñ• Stemming and Lemmatizing </h3><p>Stemming &amp; Lemmatizing are techniques used to find the root of words, in order to group them by meaning rather than exact form.</p>\n",
    "<p><img src=\"https://drive.google.com/uc?id=16Yrm2KajEJiTgXkCB50yjf0d2YYdsdv1\" style=\"margin:auto\" width=\"600\"/></p>\n",
    "<a href=\"https://www.madrasresearch.org/post/stemming-and-lemmatization\">Prajey Mehta, 2021</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed = [stemmer.stem(word) for word in text]\n",
    "\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<p>üëç Stemming/Lemmatizing is useful for topic modelling, sentiment analysis</p>\n",
    "<p>üëé Hinders authorship attribution and text generation</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(txt):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    txt = [lemmatizer.lemmatize(word) for word in txt]\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applying all of these transformations to our X_train\n",
    "\n",
    "We can use functions to apply the transformation to the entire `X_train` (and `X_test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = remove_numbers(txt)\n",
    "    txt = remove_punctuation(txt)\n",
    "    txt = tokenize(txt)\n",
    "    txt = remove_stopwords(txt)\n",
    "    return lemmatize(txt)\n",
    "\n",
    "transform_text(X_train.iloc[1018])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If tokens are needed\n",
    "\n",
    "def to_tokens(X_origin):\n",
    "    X = X_origin.copy()\n",
    "    for idx, txt in X.items():\n",
    "            X.loc[idx] = transform_text(txt)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If strings are needed\n",
    "\n",
    "def to_string_tokens(X_origin):\n",
    "    X = X_origin.copy()\n",
    "    for idx, txt in X.items():\n",
    "        X.loc[idx] = ' '.join(transform_text(txt))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_prep = to_string_tokens(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_prep = to_string_tokens(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Vectorizing text for NLP\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=16HcUdbRs9ShtRwJ1h0znHt668zbF8WdK\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: A self-portrait of DALL-E writing text for their NLP class, digital art.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h1>Vectorizing</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>In NLP word encoding is referred to <strong style=\"color:teal\">vectorizing</strong>: you have already encountered vectorizing in the form of <code>One Hot Encoding</code>, for instance. Three standard vectorizing techniques used with classical (non neural-network) machine learning approaches are:</p>\n",
    "<ul>\n",
    "<li>Bag of Words</li>\n",
    "<li>Tf-Idf</li>\n",
    "<li>N-grams</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's select 5 tweets to illustrate our lesson\n",
    "text = X_train_prep.iloc[123:128]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h2 >Bag of words representation</h2><p>The Bag-of-words representation consits of counting occurences of the each word in a text. The count for each word becomes a feature, and a sentence is a vector representing the number of each words.</p>\n",
    "<p><img src=\"https://drive.google.com/uc?id=16CnVU5VyiX8mOvy6RnbMv2u9N8Xu1uxt\" style=\"margin:auto\" width=\"1000\"/></p><br>\n",
    "<a href=\"https://www.ronaldjamesgroup.com/blog/grab-your-wine-its-time-to-demystify-ml-and-nlp\">RonaldJames consultants</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h3>üñ• Sklearn's <code>CountVectorizer</code> </h3><p>A tool to automatically generate a Bag-of-Word representation of text.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(text)\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>Limitations of Bag-of-words representation:</p>\n",
    "<ul>\n",
    "<li>Does not take into account document lenght</li>\n",
    "<li>Does not capture context</li>\n",
    "<li>Potentially large dimensionality of vector!</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h2 >Tf-Idf representation</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>Term Frequency - Inverse Document Frequency scores words according to their importance in a text, according to their presence in a collection of documents.</p>\n",
    "<p><img src=\"https://drive.google.com/uc?id=16C2TUNSLIEUpxJBPsH1pMaNAHCURC9ra\" style=\"margin:auto\" width=\"1000\"/></p><br>\n",
    "<a href=\"https://zcsheng95.github.io/2020/07/20/latent-models/\">Jeff's Blog</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h3 >üñ• Sklearn's <code>TfidfVectorizer</code></h3><p>A tool to automatically generate a Tf-idf representation of text.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = tf_idf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(),columns = tf_idf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>üëç Advantages of Tf-Idf representation:</p>\n",
    "<ul>\n",
    "<li>Using frequency rather than count is robust to document length</li>\n",
    "<li>Measure of importance</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>üëé Disadvantage of Tf-Idf representation:</p>\n",
    "<ul>\n",
    "<li>Does not capture context</li>\n",
    "<li>Dimensionality is also a potential problem</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h2 id=\"N-Gram-representation\">N-Gram representation<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_10-Natural-Language-Processing.html?title=Natural+Language+Processing&amp;program_id=10#N-Gram-representation\">¬∂</a></h2><p>Instead of considering individual words, N-grams consists of considering word sequences. This representation captures <strong>context</strong>. N is the number of words to be consiered as a one.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h3 id=\"üñ•-ngram_range-parameter\">üñ• <code>ngram_range</code> parameter<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_10-Natural-Language-Processing.html?title=Natural+Language+Processing&amp;program_id=10#%F0%9F%96%A5-ngram_range-parameter\">¬∂</a></h3><p>A parameter of the two vectorizers to specify the length of sequences to be considered.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range = (2,2))\n",
    "\n",
    "X = tf_idf_vectorizer.fit_transform(text)\n",
    "\n",
    "X.toarray()\n",
    "\n",
    "pd.DataFrame(X.toarray(),columns = tf_idf_vectorizer.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h2 >Key parameters of <code>CountVectorizer</code> and  <code>TfidfVectorizer</code> </h2><ul>\n",
    "<li><code>max_df</code></li>\n",
    "<li><code>min_df</code></li>\n",
    "<li><code>max_features</code></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h3 >üñ• <code>max_df</code></h3><p>Used to exclude \"corpus specific stopwords\", words that are very frequent in the dataset. The vectorizer will ignore the words that have a frequency higher than the specified threshold.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(max_df = 0.5)\n",
    "\n",
    "X = tf_idf_vectorizer.fit_transform(text)\n",
    "\n",
    "X.toarray()\n",
    "\n",
    "pd.DataFrame(X.toarray(),columns = tf_idf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>üëâ Particularly useful to remove words that are so frequent they have little predictive power.</p>\n",
    "<p>Example: When classifying texts about climate change, the words \"climate\" and \"change\" will appear often, but won't be useful to predict sentiment about climate change.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h3>üñ• <code>min_df</code></h3><p>Used to exclude words that are very infrequent in the dataset. The vectorizer will ignore the words that have a frequency lower than the specified threshold.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(min_df = 0.5)\n",
    "\n",
    "X = tf_idf_vectorizer.fit_transform(text)\n",
    "\n",
    "X.toarray()\n",
    "\n",
    "pd.DataFrame(X.toarray(),columns = tf_idf_vectorizer.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>üëâ Particularly useful to remove typos or text anomalies missed during preprocessing.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h3>üñ• <code>max_features</code> </h3><p>Used to specify the number of features to keep when vectorizing. It will retain the top features according to count or tf-idf score.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features = 2)\n",
    "\n",
    "X = tf_idf_vectorizer.fit_transform(text)\n",
    "\n",
    "X.toarray()\n",
    "\n",
    "pd.DataFrame(X.toarray(),columns = tf_idf_vectorizer.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>üëâ Particularly useful to reduce the dimension of the data.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h1 id=\"4.-(Multinomial)-Naive-Bayes-Algorithm\">4. (Multinomial) Naive Bayes Algorithm<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_10-Natural-Language-Processing.html?title=Natural+Language+Processing&amp;program_id=10#4.-(Multinomial)-Naive-Bayes-Algorithm\">¬∂</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>A classification algorithm based on Bayes' Theorem of probability.</p>\n",
    "<p><img src=\"https://drive.google.com/uc?id=167U6c9JP-zAMX9HyXACuKLWWqYdl0y8U\" style=\"margin:auto\" width=\"600\"/></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h2 id=\"Example\">Example<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_10-Natural-Language-Processing.html?title=Natural+Language+Processing&amp;program_id=10#Example\">¬∂</a></h2><p>We want to classify mails as normal or spam according to their content.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<ol>\n",
    "<li>Count occurences of all words in normal mail.</li>\n",
    "</ol>\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=16VpY64n7jVr9bfi0ceWI-5B2p4IC_i79\" style=\"margin:auto\" width=\"400\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<ol>\n",
    "<li>Calculate probability of each word being in the mail if its normal</li>\n",
    "</ol>\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=16nW3HNLnwyFSj1SHAXFraQaFF0TDyEAA\" style=\"margin:auto\" width=\"800\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=164oie98o07Rxk4pkmLOQwpvK_W9LTv77\" style=\"margin:auto\" width=\"800\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<ol>\n",
    "<li>Do the same with the spam mail!</li>\n",
    "</ol>\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=16ZaYMJkNvKUQAsD5v4IjiR2OQDorwmqf\" style=\"margin:auto\" width=\"500\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=16_Su7yMGZwZXm9vkZeryX_ymf2WZuMK_\" style=\"margin:auto\" width=\"800\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=16S8fpinkbFfY2e8TNYrwaqf8B22l9Cu_\" style=\"margin:auto\" width=\"350\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>Would a mail containing \"Dear friend\" be normal or spam?</p>\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=16PlcmtHebz1sI5Eny4NmWKv-1wBuvK4F\" style=\"margin:auto\" width=\"500\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<ol>\n",
    "<li>Calculate prior probability of mail being normal</li>\n",
    "</ol>\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=16BrKEB5hfPKeXGmMsQnPVtQbUFBkJThl\" style=\"margin:auto\" width=\"600\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<ol>\n",
    "<li>Calculate probability of the mail being normal</li>\n",
    "</ol>\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=16471ESfFKsvBFts7fp6OpT2bCKqpt9EK\" style=\"margin:auto\" width=\"700\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=16LCSVaAV_E5s1dgH-3x7WZjV-6XlIl8N\" style=\"margin:auto\" width=\"800\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<ol>\n",
    "<li>Calculate probability of the mail being spam</li>\n",
    "</ol>\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=16BWN6-VwKshedJ0fUSjYJ-30Uaft26qE\" style=\"margin:auto\" width=\"800\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>In the case of zero counts, we have a problem...</p>\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=16JcCFsvxudyTQxGHG3BHjwwgFAnVZDvN\" style=\"margin:auto\" width=\"800\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h2 id=\"Smoothing\">Smoothing</h2><p>Smoothing consists of adding a count +1 to each feature (word) to avoid zero counts. The smoothing parameter is most often called Alpha.</p>\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=16j0CWOvuJNXD9BESmpe78NEloKNWTDig\" style=\"margin:auto\" width=\"500\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p><img align=\"left\" src=\"https://drive.google.com/uc?id=16VfhF1ye3Kl4ekF95lh_PmUXkSpamrlb\" style=\"margin:auto\" width=\"800\"/></p><br>\n",
    "<a href=\"https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA\">From StatsQuest video</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>\"Naive\" refers to the fact that each feature (word) is treated individually and order isn't considered.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>üëç Advantages of Naive Bayes algorithm:</p>\n",
    "<ul>\n",
    "<li>Easy to implement</li>\n",
    "<li>Ouputs probabilities</li>\n",
    "<li>Not an iterative learning process. Fast!</li>\n",
    "<li>Works particularly well on text data because handles big dimensions</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>üëé Disadvantage:</p>\n",
    "<ul>\n",
    "<li>Assumes feature independence, rarely the case in real life datasets</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h1 id=\"5.-Modelling-Implementation\">5. Modelling Implementation<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_10-Natural-Language-Processing.html?title=Natural+Language+Processing&amp;program_id=10#5.-Modelling-Implementation\">¬∂</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(max_features = 15)\n",
    "\n",
    "X_train_prep = tf_idf_vectorizer.fit_transform(X_train)\n",
    "X_test_prep = tf_idf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "nb_model.fit(X_train_prep, y_train)\n",
    "\n",
    "nb_model.score(X_test_prep,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">MultinomialNB</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h3 id=\"Tuning-vectorizer-and-model-simultanously\">Tuning vectorizer and model simultanously<a class=\"anchor-link\" href=\"https://kitt.lewagon.com/karr/data-lectures.kitt/05-ML_10-Natural-Language-Processing.html?title=Natural+Language+Processing&amp;program_id=10#Tuning-vectorizer-and-model-simultanously\">¬∂</a></h3><p>Different vectorizing hyperparameters will affect model performance. As such, it is important to tune the hyperparameters of both the vectorizer and the model simultaneously. This can be done by using a <code>Pipeline</code>.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Set parameters to search\n",
    "parameters = {\n",
    "    'tfidf__ngram_range': ((1,1), (2,2)),\n",
    "    'nb__alpha': (0.1,1),}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "grid_search.best_score_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Suggested Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## üìö Further Reading \n",
    "* üìº <a href=\"https://www.nltk.org/book/\">A full online book on how to process text</a>, by Steven Bird, Ewan Klein, and Edward Loper (NLTK)\n",
    "* üìº <a href=\"https://web.stanford.edu/~jurafsky/slp3/\">Speech and language processing</a>, by Dan Jurafsky and James H. Martin\n",
    "* üìº <a href=\"https://www.youtube.com/watch?v=CMrHM8a3hqw\">A simple video on NLP</a>, by Simplilearn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## üíªüêç Time to Code ! "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "rise": {
   "scroll": true,
   "theme": "serif",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
