{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352e8549",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-cL5eOpEsbuIEkvwW2KnpXC12-PAbamr\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5691c",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66b189",
   "metadata": {},
   "source": [
    "Remember the **Golden Plains Roadside Biodiversity** dataset your worked on the first day? We ended up dropping many features in our **linear regression** whilst maintaining a good $R^2$ score. We will use this dataset again here.\n",
    "\n",
    "![kangaroo](https://s.yimg.com/uu/api/res/1.2/GANJCEs2SP0QamHePbZqUw--~B/aD0zNjE7dz03Njg7YXBwaWQ9eXRhY2h5b24-/http://media.zenfs.com/en_us/News/afp.com/b9a6c5065aab22b840d60a188e7767a7ce7c471c.jpg)\n",
    "\n",
    "However, there are a few differences:\n",
    "- We will use logistic classifiers here which are easy to interpret\n",
    "- We will model the `RCACScore` as out target variable changed to a binary class: `0` indicates a score <=12, `1` a score >12.\n",
    "- The dataset is already cleaned, scaled, and one-hot-encoded for you üòå\n",
    "- The goal is to use `regularization` to detect relevant/irrelevant features based on under/overfitting criteria\n",
    "- **Our goal is to compare `L1` and `L2` penalties**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052388a",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Load the data into a variable named `data`, and split it into an `X` feature matrix and a `y` target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada65fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1cIO50NnXZg6F1Y9-aRKorKhXSS5Idnjc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a43a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547eb150",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"raw_data/biodiversity-prepared.csv\")\n",
    "\n",
    "# the dataset is already one-hot-encoded\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build X and y\n",
    "\n",
    "y = data[\"RCACScore\"]\n",
    "X = data.drop(columns=\"RCACScore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8089f375",
   "metadata": {},
   "source": [
    "## Logistic Regression without regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57957d3",
   "metadata": {},
   "source": [
    "‚ùì Rank the feature by decreasing order of importance according to a simple **non-regularized** Logistic Regression\n",
    "\n",
    "- Careful, `LogisticRegression` is penalized by default\n",
    "- Increase `max_iter` to a larger number until the model converges\n",
    "- remember that you can access the coefficients of the regression by calling `.coef_` on your trained model. \n",
    "- *Hint*: it might help to put the coefficient of the model in a dataframe with column names from `X` to be able to interpret them. Also check the `transpose()` and `sort_values()` pandas functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cac334",
   "metadata": {},
   "source": [
    "‚ùìHow do you interpret, in plain english language, the value for the coefficient `RCACRareSp` ?\n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "> \"All other things being equal (i.e. if the other variables are the same),\n",
    "the abundance of rare species (`RCACRareSp`) increases the log-odds of the site being classified as important by 33.38 (your coef value)\"\n",
    "    \n",
    "> \"Controling for all other explaining factors available in this dataset,\n",
    "a high `RCACRareSp` increases the odds-ratio of a high score by exp(33.38) = 3.14E15\"\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c731687",
   "metadata": {},
   "source": [
    "‚ùì What are the 5 features that most impact the chances of classifying a site as a high scoring site? Save your answer as an array under a variable named `base_most_important`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaaa3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f8236e",
   "metadata": {},
   "source": [
    "‚ùì Now cross validate a model with the same parameters as the model above, and save the mean score under a variable named `base_model_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c1b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('unregularized', \n",
    "                         top_features = base_most_important,\n",
    "                            score=base_model_score)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10929034",
   "metadata": {},
   "source": [
    "## Logistic Regression with a L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b608474",
   "metadata": {},
   "source": [
    "Let's use a **Logistic model** whose log-loss has been penalized with a **L2** term to figure out the **most important features** without overfitting.  \n",
    "This is the \"classification\" equivalent to the \"Ridge\" regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2928db6",
   "metadata": {},
   "source": [
    "‚ùì Instantiate a **strongly regularized** `LogisticRegression` and rank its feature importance\n",
    "- By \"strongly regularized\" we mean \"more than sklearn's default applied regularization factor\". \n",
    "- Default sklearn's values are very useful orders of magnitudes to keep in mind for \"scaled features\"\n",
    "- We suggest trying a regularization factor of 10% of the default value in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c30355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76973b9b",
   "metadata": {},
   "source": [
    "‚ùì What are the top 5 features driving chances of survival according to your model ? Save them as an array under the variable name `l2_most_important`. Are these the same features as for `base_most_important`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4059add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959f081",
   "metadata": {},
   "source": [
    "‚ùì Now cross validate a model with the same parameters as the model above, and save the mean score under a variable named `l2_model_score`. What can you say about the new score compare to the `base_model_score`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9847a158",
   "metadata": {},
   "source": [
    "#### üß™ Test your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda2b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('ridge', \n",
    "                         top_features = l2_most_important,\n",
    "                        score=l2_model_score)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f34228",
   "metadata": {},
   "source": [
    "## Logistic Regression with a L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e8a5c",
   "metadata": {},
   "source": [
    "This time, we'll use a logistic model whose log-loss has been penalized with a **L1** term to **filter-out the less important features**.  \n",
    "This is the \"classification\" equivalent to the **Lasso** regressor\n",
    "\n",
    "‚ùì Instantiate a **strongly regularized** `LogisticRegression` and rank its feature importance. We suggest that you use the same regularization value as for **L2** to be able to compare your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a061a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e397bf",
   "metadata": {},
   "source": [
    "‚ùì What are the features that have absolutely no impact on chances of survival, according to your L1 model?\n",
    "- Save them as in a array variable named `zero_impact_features`\n",
    "- Do you notice how some of them were \"highly important\" according to the non-regularized model ? \n",
    "- From now on, we will always regularize our linear models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8381bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb457f",
   "metadata": {},
   "source": [
    "‚ùì Now cross validate a model with the same parameters as the model above, and save the mean score under a variable named `l1_model_score`. What can you say about the new score compare to the `base_model_score` and `l2_model_score`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac745580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75709d",
   "metadata": {},
   "source": [
    "üí° Have you noticed how the `l1_model_score` is slightly higher than the `l2_model_score` but using much less features, and that the `l2_model_score` itself higher than the `base_model_score` score? This is why regularization is so important: by filtering out the unecessary variables (i.e. setting their coefficient to zero) **L1** regularization has improved our classification score! Of course, this also comes down to the choice of the hyperparameter C, and it is possible to over-regularize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1923f",
   "metadata": {},
   "source": [
    "#### üß™ Test your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add00c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('lasso', \n",
    "                         zero_impact_features = zero_impact_features,\n",
    "                        score=l1_model_score)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d476e",
   "metadata": {},
   "source": [
    "# GridSearch the best hyperparameters\n",
    "\n",
    "So ***how*** do we determine the best hyperparameters for our algorithm? We can use `GridSearchCV` for that! \n",
    "\n",
    "For instance, which one of the L1 or L2 regularization is best for our performance? Or maybe we are looking at a mix of L1 and L2, known as `elastic net`? We can find out! Do a `GridSearchCV` for a logistic regression model initiated with the following arguments: `max_iter=5000`, `random_state=42`, `penalty='elasticnet'`, `solver='saga'`. Saga is the only solver that will work with elastic net. Then, find the best `LogisticRegression` model by testing the following hyperparameters in gridsearch:\n",
    "\n",
    "1. C = [1, 0.1, 0.01, 0.001]\n",
    "2. class_weight = [None, 'balanced']\n",
    "3. multi_class = ['multinomial','ovr']\n",
    "4. l1_ratio:[0, 1, 0.9, 0.7, 0.5, 0.2]\n",
    "\n",
    "Try to understand these parameters by reading the documentation, and then fit your GridSearchCV on `X` and `y`. Save the best estimator in a variable called `best_estimator`, the best parameters (as a dictionary) in a variable called `best_params`, and the accuracy score in a variable called `best_score` (hint: all of these values can be obtained from your fitted grid search model). Then test your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84b07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed0827f",
   "metadata": {},
   "source": [
    "#### üß™ Test your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('gridsearch', \n",
    "                        score = best_score,\n",
    "                        params=best_params)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a26ecb",
   "metadata": {},
   "source": [
    "# üèÅ Finished!\n",
    "\n",
    "Well done! <span style=\"color:teal\">**Push your exercise to GitHub**</span>, and move on to the next one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
