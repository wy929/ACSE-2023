{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4819a9b",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-d7H1l1lJ28_sLcd9Vvh_N-yro7CJZcZ\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31609254",
   "metadata": {},
   "source": [
    "# Golden Plains Roadside Biodiversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1968dcc",
   "metadata": {},
   "source": [
    "This is your first data problem! Remember, \"Data Problems\" are a little bit less directed than the skills problem. They are here to encourage you to use your critical thinking when dealing with data. It is also a better reflection of the type of problems you will encounter during your assessed coursework at the end of the course. Make sure you understand what you have done in the previous exercises, and apply it here. Also, ***get into the habit of maintaining a  clean, working notebook***. This will be a key assessment criteria for your marked coursework later next week, so take this opportunity (and further ones) to learn how to do this. This includes using `markdown` cells for comments and observations, making sure your code can run from top to bottom when using `run all cells` from the menu, and of course, keeping a **clean code** practice. It also also a good idea, once you are done with your work, to put all of your `import` statements at the top of the notebook: this way, it is clear what is imported in the entire notebook and allows you to focus on your more important code below.\n",
    "\n",
    "Here is a little bit of information on the data you are given. Golden Plains Shire (Australia) is responsible for managing 1834 kilometres of road reserves. Road reserves are not only used for transport but also act as service corridors, in fire prevention, recreation, and occasionally agricultural pursuits. Native vegetation on roadsides is important flora and fauna habitat and landscape character.\n",
    "\n",
    "In 2014, Golden Plains Shire acquired funding through the Victorian Adaptation and Sustainability Partnership (VASP) to undertake Councils ‚ÄòBuilding Adaptive Capacity on Roadsides‚Äô project. The Project was designed to identify significant environmental assets on roadsides, improve roadside management practices and reduce Council‚Äôs risk of potential breaches against Federal and State environmental legislation. \n",
    "\n",
    "The council made this <a href='https://data.gov.au/data/dataset/golden-plains-roadside-biodiversity'>dataset available here</a>.<br>\n",
    "![plain](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/Mount_Conner%2C_August_2003.jpg/375px-Mount_Conner%2C_August_2003.jpg)\n",
    "<br>\n",
    "\n",
    "üéØ Today, you will work with a simplified version of this real dataset. The dataset contains a number of biodiversity observations including one on tree size (`RCACTreesS`). This exercise consists of the data preparation and modelling techniques you have learnt: our goal is to predict via linear regression the `RCACTreesS` using the available features and obtain a good score.\n",
    "\n",
    "‚ö†Ô∏è This is a long exercises, which will require you to think about the data. Don't hesitate to plot things - if you need to use algorithm that use a `random_seed` such as `train_test_split` or others, remember to always use the value `42` so your results can be compared to the proposed solution. If you get stuck, ask a TA!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755da14",
   "metadata": {},
   "source": [
    "# Part I: Ensuring Generalization and EDA\n",
    "\n",
    "In this first part, do the following:\n",
    "1. üëá Load the data into this notebook as a pandas dataframe named `df`, and display its first 5 rows.\n",
    "2. Check for and drop duplicates\n",
    "3. We will use the `RCACTreesS` as our target variable (`y`) and all other columns as our features (`X`).\n",
    "4. Split the dataset into 80%/20% train/test splits (use a `random_state=42`) to create your `X_train`, `X_test`, `y_train`, `y_test` (see above regarding the `y`).\n",
    "5. **Using only the X_train**, spend some time exploring the dataset, for instance looking at the different columns it contains, it's data types, any missing values. Check for correlations between features, and draw some plots. At the end of this EDA stage, you should have a good idea of what the data is. Try to keep this notebook cleanly organised, using `Markdown` cells to put comments for yourself (and your TAs) about your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='19qi8xMUaamIAX8KcZproR33c2JQcOAul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e8749",
   "metadata": {},
   "source": [
    "# Part II: Missing values and scaling\n",
    "\n",
    "Now do the following:\n",
    "1. Drop features with >30% missing values\n",
    "2. Imput `RoadWidthM`, `PowerlineD` and `Trees` using the most appropriate strategy <details>\n",
    "    <summary> üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è Look at the datatype of <code>PowerlineD</code> and the distribution of the data using the <code>.unique()</code> method. Although <code>PowerlineD</code> is a numeric value, it clearly only has discrete distribution: what would be a logical value to impute? The same applies to <code>Trees</code> and <code>RoadWidthM</code> but for a different reason: they are a continuous variable but there is clearly one value that dominates the distribution: it makes sense to assume that the `nan` represent this most frequent value. So you can impute both of these variables at the same time.\n",
    "</details> \n",
    "3. Imput `Locality` and `EVNotes` <details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è Clearly <code>Locality</code> refers to the name of the county or region where the data comes from. We could impute the most frequent locality, but this would induce some errors. In this case, the best strategy is simply to replace the <code>nan</code> by something meaningful such as 'not known'. <code>EVCNotes</code> is somewhat similar: the <code>nan</code> values indicate that no notes exist, so we should replace them by 'no notes'.\n",
    "</details>\n",
    "4. Impute `SoilType` and `LandformLS` <details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è These two are tricky. They both are string values, and they both have two classes that are very common. On a real project, a good data scientist will study what those codes means <a href=\"http://vro.agriculture.vic.gov.au/dpi/vro/vrosite.nsf/pages/landform_land_systems_rees/$FILE/TECH_56%20ch6.pdf\"> by refering to the government publication</a>. In an ideal world we would explore different strategies for imputation (we will see this later in the course). However here we need to decide based on little evidence. Because we have no information, and because there is not a clear majority in either soil or landform classes, the best is to impute 'SoilTypeNA' and 'LandFormLSNA' as as a new class.\n",
    "</details>\n",
    "5. Imput `CanopyCont` <details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è If you do a <code>value_counts()</code> on <code>CanopyCont</code> you will see that this consists of 4 numerical variables, and 5 categorical variables. It is clear that this column has two different encoding for the same concept: how continuous is the canopy? The easiest is to transform this into a numerical column by doing the following replacements: 'none'=0, 'sparse'=1, 'patchy'=2, 'continous' or 'c' = 3. You probably want to use a python dictionary and an <code>apply()</code> function to do that, and remember to cast your values to an <code>int</code> or a <code>float</code>!\n",
    "</details>\n",
    "6. Scale all of your numerical features using an appropriate scaler. Check their distribution before deciding on your scaling strategy! <details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è <code>WidthVarie</code>, & <code>Powerline</code> are clearly binary variable ([0,1]). They should not be scaled, but rater can optionally be encoded using a <code>CategoricalEncoder</code>. Simply leave them as they are. All other numerical features are non-guassian so a `RobustScaler` is probably the most appropriate.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f035eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64cc59",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81238080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('missing_values',\n",
    "                         dataset = X_train)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baddaf4e",
   "metadata": {},
   "source": [
    "### Testing your scaling\n",
    "Test your code below for scaling before proceeding to ensure all worked well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c71e4d",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49763d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('scaling',\n",
    "                         dataset = X_train,\n",
    "                         features = numerical_columns\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44907bc7",
   "metadata": {},
   "source": [
    "# Part III: Encoding and Modelling\n",
    "\n",
    "All that is left to do now is deal with categorical data, and then use this to build a simple model.\n",
    "\n",
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd33ed1",
   "metadata": {},
   "source": [
    "üëá Investigate the non-numerical features that require encoding, and apply 'One hot encoding'. To ensure that we do not end up with an explosion of feature, we will retain only categorical features with <15 unique values for encoding. \n",
    "\n",
    "So your task is the following:\n",
    "\n",
    "1. Identify programmatically all of the categorical features that have <15 unique categories and require 'One Hot encoding'\n",
    "2. In the dataframe, replace the original features by their encoded version(s). Make sure to drop the original features, as well as the features with >15 unique categories from `X_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987a47b",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('encoding',\n",
    "                         dataset = X_train)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77333658",
   "metadata": {},
   "source": [
    "# Base Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02831468",
   "metadata": {},
   "source": [
    "All we need now is to cross validate  a Linear regression model with our `X_train` and `y_train` using `cv=5`. Save its score under variable name `base_model_score`. However, if you do this you will see that we obtain a very low `r2`. This is because not all of the features we have selected are useful - we will talk more about this in a couple of days. So instead, train your model using only the top features that have a correlation with your `y_train` > 0.05.  <details><summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è If you are unsure how to do this, check the documentation for the `corr()` function in pandas. Also, you will need to add group the `y_train` and the `X_train` in the same pandas object to do that.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c7c83b",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff181807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('base_model',\n",
    "                         score = base_model_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c087b3e",
   "metadata": {},
   "source": [
    "# üèÅ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
