{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51ec437",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-cL5eOpEsbuIEkvwW2KnpXC12-PAbamr\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e589e",
   "metadata": {},
   "source": [
    "# Data Preprocessing 101 \n",
    "\n",
    "In this first exercise, we will go step by step through the data preparation steps. This guide is not fully comprehensive but it gives you a good understanding of what to do. The main goal is to teach you the `sklearn` syntax for some of the operations.\n",
    "\n",
    "The skills you acquired with `NumPy` and with `Pandas` in the previous module will come handy here, as well as in the rest of the `machine learning` module!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb76ef",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will be using the <span style=\"color:teal\">**Todays's dataset:**</span><a href=\"https://www.kaggle.com/datasets/rohanrao/air-quality-data-in-india\"> India air quality data, Kaggle</a>, the very same dataset we have seen in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d483e",
   "metadata": {},
   "source": [
    "# Load the data \n",
    "\n",
    "Load the data `raw_data/India_air_quality.csv` into a new `Pandas.DataFrame` that you can call `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5802973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1HtOHNoKtbqxsROzC0grJsXLR1CvM58xa')\n",
    "rds = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba463b",
   "metadata": {},
   "source": [
    "# üíª <code>drop_duplicates</code>\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. Check how many duplicates are in the dataframe by chaining together the pandas functions `.duplicated()` and `.sum()` on the dataframe `data`. Save this value as a variable called `duplicates_count`\n",
    "2. Remove the duplicates from the dataframe by calling the function `.drop_duplicates()` on `data`. NOTE: you need to reassign data to the return value to replace it (`data = data.drop_duplicates()`\n",
    "3. Check that the duplicates have been removed by doing step 1 again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51dfc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959007b",
   "metadata": {},
   "source": [
    "# üíª <code>train_test_split</code>\n",
    "\n",
    "Do the following in this sections:\n",
    "1. Import the `train_test_split` function from `sklearn.model_selection`\n",
    "2. Create a `y` target series that contains `rspm`: this is what we will use as a label to our supervised training later, i.e. we will try to predict this property.\n",
    "3. Create an `X` feature matrix (dataframe) that contains all columns except `rspm` (use `data.drop(columns=\"rspm\")`) \n",
    "4. Create `X_train`, `y_train` (70% of the data) and a `X_test`, `y_test` (30% of the data) by using the `train_test_split` method\n",
    "5. Use the `shape` attribute of your `X_train` to verify the number of samples (should be 216741) and features (should be 11) that you have in the train set\n",
    "\n",
    "**hint 1:** When doing the `train_test_split`, you can set the `test_size` (or `train_size`) to the desired fraction\n",
    "**hint 2:** You can also split directly two dataframe / matrix together, for instance, `train_test_split(X, y, ...)`. Keep in mind that this returns `X_train, X_test, y_train, y_test` (in this order!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714649f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992b24c",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d07e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('duplicates',\n",
    "                         duplicates = duplicates_count,\n",
    "                         dataset = X_train\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02e8b3",
   "metadata": {},
   "source": [
    "# üíª Exploratory Data Analysis (EDA)\n",
    "\n",
    "Do the following in this sections:\n",
    "1. Check the **data type** of your train set by looking at the `X_train.dtypes`\n",
    "2. Use the `describe()` method on your `X_train` dataframe to find the various statistics on the numerical values\n",
    "3. Find the frequency of the different locations by calling the `value_counts()` method on the `X_train.location` series\n",
    "4. Plot a historgram of distribution of the `X_train.location` series\n",
    "5. Plot the histograms of all the numerical features in your `X_train` (explore the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html\">DataFrame.hist()</a> function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b544ca2",
   "metadata": {},
   "source": [
    "# ‚ùì Missing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff6999",
   "metadata": {},
   "source": [
    "The goal of this step is to identify what data is missing in your `X_train`, and to remove columns with too many missing datapoints (in our case, we will use `>30% missing data` as our threshold to remove columns).\n",
    "\n",
    "Do the following:\n",
    "1. On your train set, call the function `.isnull()` and explore its output. What `dtype` is it? Do you understand what it does?\n",
    "2. Now chain both `.isnull()` with `.sum()`: this will give you the number of missing data in each column! Why?\n",
    "3. Let's sort this output to have the columns with the largest number of missing data at the top. Simply chain a new function, `sort_values()` to the two previous ones. You also need to pass the argument `ascending=False` to the function to have it in descending order.\n",
    "4. This is great, but how do we know whether a column has more or less then 30% missing data? Simple! Devide the output of step 3 above by the lengths of the `X_train` dataframe (or the position 0 on it's `.shape` tuple)! Anything above 0.3 means >0.3% missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f49495",
   "metadata": {},
   "source": [
    "## Handling missing data\n",
    "\n",
    "Now you know what columns have more than 30% missing data. Let's handle them one by one:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1da6f",
   "metadata": {},
   "source": [
    "### <code>pm2_5</code>\n",
    "\n",
    "You should have learned from the exercise above that is >97% of data missing in this column. Simply drop the entire column by using the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\">DataFrame.drop()</a> function.\n",
    "\n",
    "‚ö†Ô∏è You either need to use the `inplace=True` argument or assign the return value of this function back to `X_train` for this change to be reflected in the original `X_train`.\n",
    "\n",
    "Do the same for the other features (columns) with >30% missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cae52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da3ac2",
   "metadata": {},
   "source": [
    "### <code>type</code>\n",
    "\n",
    "Type also has missing values. However, we will treat these missing values differently here. To understand why, do the following:\n",
    "\n",
    "1. Check the different categories and the number of instance of the `X_train.type` Series: you can see that there is a category named `Others`.\n",
    "2. Use the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\">DataFrame.replace()</a> function to replace all of the missing values (`np.nan`) by `Others`. Make sure to reassign those values back to the `X_train` dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f83d8",
   "metadata": {},
   "source": [
    "\n",
    "üö® Missing data does not necessarily mean no information!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227be019",
   "metadata": {},
   "source": [
    "# Imputing missing data with Sklearn's <code>SimpleImputer</code>\n",
    "\n",
    "We have now dropped all of the features (columns) with too many missing data. We are now going to use the Sklearn <code>SimpleImputer()</code> class: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\">make sure to look at the documentation first!</a> Looking at the documentation for classes and function is one of the best way to learn to code efficiently in machine learning.\n",
    "\n",
    "The second way it to use <a href=\"https://stackoverflow.com/\">Stack Overflow</a> to look for issues you are facing in your code: more often then not, someone has had the same problem and a good solution (with a green tick) is posted. Try to use these sources of information first before calling the teaching assistant for help: it is important to learn to troubleshoot your own problems first.\n",
    "\n",
    "Remember from the class that we will use the `fit()` function on our imputer class to learn the statistics from our `X_train`, and then use the `transform()` function to change our data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfcc862",
   "metadata": {},
   "source": [
    "## Numerical data\n",
    "\n",
    "Often, when you apply transformation to your data you will want to treat your numerical data differently than you categorical data. For instance, replacing a missing numerical value by the `mean()` of a column makes perfect sense. But this would not make sense for a categorical value (what is the `mean` of the `location` column?)\n",
    "\n",
    "So, we need to do the following:\n",
    "\n",
    "1. Identify the `numerical` columns: you can do this by calling the `select_dtypes()` function on the `X_train` dataframe, and passing the argument `include=np.number` to this function. This will return a dataframe containing only numerical columns! If you call `.columns` on it, you will obtain an array of column names. I suggest you store this in a variable named `num_cols` as it will be useful in later steps.\n",
    "2. Import the `SimpleImputer` class from `sklearn.impute`\n",
    "3. Create `num_imputer` instance with the \"mean\" as its strategy: `num_imputer = SimpleImputer(strategy=\"mean\")`\n",
    "4. Fit your new imputer on the `num_cols` of `train_test`\n",
    "5. You can check the statistics (in this case, the means) of each columns by calling the `.statistics_` property of the `num_imputer`\n",
    "6. Now replace the original numerical columns in `X_train` by the imputed ones, using the `transform` method of your `num_imputer`\n",
    "\n",
    "**hint** for this last step, you can write code that looks like `X_train[num_col] = ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeab135",
   "metadata": {},
   "source": [
    "### Categorical data\n",
    "\n",
    "For the categorical data, you can follow a very similar approach as above:\n",
    "1. Find all of the categorical columns (`cat_cols`), this time using `exclude=np.number` in step 1 from the numerical transformation\n",
    "2. Create a new instance of the `SimpleImputer` class, call it `cat_imputer`, with the `\"most_frequent\"` strategy.\n",
    "3. Apply this new imputer to all categorical columns in a similar way as you did above for numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af24452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30265ba",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb027f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('missing_values',\n",
    "                         dataset = X_train\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654d526",
   "metadata": {},
   "source": [
    "# Outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b633a792",
   "metadata": {},
   "source": [
    "We will take a simple approach to outliers here: we will consider that negative values are not possible given that we are looking at concentration of chemicals in the atmosphere. So, we will remove all negative values.\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. Find the minimum values of all `num_cols` columns (i.e. your numerical columns)\n",
    "2. Drop the rows where any of the `num_cols` are less than zero. Check the `DataFrame.drop()` function documentation to see how to do this \n",
    "\n",
    "**hint**: You may want to find the `index` of the rows where the features you want to remove are < zero, and then pay attention to the `index` parameter of the `drop()` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b91f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b5894a",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Here, we are going to do some feature engineering on `so2` and `no2`. But first, plot the histogram of these two features and see what you observe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a6406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebc5b6",
   "metadata": {},
   "source": [
    "## Transforming the two features to a log scale\n",
    "\n",
    "We are going to transform these two features into a logarithmic scale, because this will distribute our data better. Taking the log of a value is a very common data engineering trick that can sometime help improve the signal-to-noise ratio of your dataset.\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. For both `so2` and `no2` replace the original value by their log. Use `np.log`, and to avoid numerical error, add a small number (`0.001`) to the original values (because the log of 0 is not a number)\n",
    "2. Draw the histograms again: did they improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e778d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af737e",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "We are now ready to scale our numerical features! Import the `StandardScaler` class from `sklearn.preprocessing`. Remember, this is a `scikit-learn Transformer` so you already know how to use this type of object (if you forgot, go back to the `SimpleImputer` and study what you did with it.\n",
    "\n",
    "Do the following:\n",
    "1. Import the `StandardScaler`\n",
    "2. Create an instance of `StandardScaler` called `scaler`\n",
    "3. Fit the `scaler` to your numerical data, and replace the numerical columns in your `X_train` by their scaled versions!\n",
    "\n",
    "**hint**: If in doubt, read the doc! And again, even though this is a new class, you will use it in a very similar way as the `SimpleImputer` (but of course without the need to use the `strategy` argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e78c3",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7047ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('scaling',\n",
    "                         dataset = X_train\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72929a50",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc29ed37",
   "metadata": {},
   "source": [
    "Our numerical features are all ready now. We still want to use some of our categorical features, notably the `state` column. But to do this, we first need to encode it using the `OneHotEncoder` class.\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. From `sklearn.preprocessing` import the `OneHotEncoder` class\n",
    "2. Create a new instance of the `OneHotEncoder` class, call it `ohe`. Make sure to pass the argument `sparse_output=False` when you create the encoder!\n",
    "3. Fit the `ohe` to the `X_train[[\"state\"]]` feature\n",
    "4. We want to capture the column names in the `ohe`. Create a variable with this code: `encoded_columns = ohe.categories_[0]`. Read the doc to understand what this does!\n",
    "5. Create a new array called `state_encoded` by using `ohe.transform(X_train[[\"state\"]])`\n",
    "6. Only keep the numerical columns of `X_train`, for instance, by doing `X_train = X_train[num_cols]`\n",
    "7. Now add the new One-hot-encoded columns to `X_train` by doing `X_train[encoded_columns] = state_encoded`\n",
    "8. Check how your new `X_train` dataframe looks, and check its size: you should see (216735, 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53fef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b070027",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "max_values = [X_train[feature].max() for feature in ['Assam','Bihar', 'Chandigarh', 'Chhattisgarh']]\n",
    "nb_ohe = X_train.drop(columns=['so2','no2','rainfall']).columns.shape[0]\n",
    "\n",
    "result = ChallengeResult('encoding',\n",
    "                         nb_columns = len(X_train.columns),\n",
    "                         max_values = max_values,\n",
    "                         nb_ohe = nb_ohe\n",
    "                         \n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f0f94a",
   "metadata": {},
   "source": [
    "# Preparing our `y_train`\n",
    "\n",
    "## Droping samples\n",
    "When we prepared our `X_train` above, we ended up droping rows because some were duplicated, or outliers. If you test the `shape` of our `y_train` and compare it to the shape of the `X_train`, you will notice that we have different number of samples. This will be a problem soon, when we try to model.\n",
    "\n",
    "Luckily, we can use the index values of the `X_train` to only keep the `y_train` that are relevant to us!\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. create a new variable, `idx`, which is equal to the `index` of the `X_train` dataset\n",
    "2. Reassign `y_train` to only contain the values with the same index as `X_train`: `y_train = y_train[idx]`\n",
    "3. If you check the `shape` of your `y_train`, it should be now (216735,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbc2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf952311",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('y_cat',\n",
    "                         dataset = y_train\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e33bfb-8d83-42d5-b3bb-3861764f4f1d",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "Now comes the fun bit! We are going to try to predict the levell of pollution (`y_train_cat`) based on our features (`X_train`). We will also use cross validation to see which model performs better.\n",
    "\n",
    "Do the following:\n",
    "1. Import `LinearRegression` from `sklearn.linear_model` as well as `cross_validate` from `sklearn.model_selection`\n",
    "2. Create a new instance of `LinearRegression`, call it `lr_model`. \n",
    "3. Cross validate (`cv=5`, `scoring='neg_mean_squared_error'`) your logistic regression model: make sure to capture the output of the function in a variable (`lr_cv`)\n",
    "4. `lr_cv` should be a Python dictionary containing `fit_time`, `score_time`, and `test_score`. Calculate the mean test score as the `root mean squared error` for this linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df0666-a41d-4f53-9ea8-6089105193db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b8ad04-5762-476c-ac3f-53d47ffd1385",
   "metadata": {},
   "source": [
    "### How good is your score?\n",
    "Is your score decent? Why do you think that is?\n",
    "<details><summary>Answer</summary> This is a terrible score of course: the root mean square error is magnitudes larger than what we are trying to predict. But why? This has to do in part with the fact that we now have a lot of columns that determines where the measurement was taken. This does not help in the prediction, and add noise. We will see this week that having the right number of features (and features with high predictive power) is key. Another possible explanation is that the relationship between our `y` and our `X` is not linear, and thus a linear model such as ours will not work well.</details>\n",
    "\n",
    "## Modelling attempt 2\n",
    "\n",
    "Now do exactly the same process as above, but instead of modelling with the entire `X_train`, only use the features `so2`, `no2`, and `rainfall`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039ec3e-5fe4-4a2d-b747-f99ffbc85e9a",
   "metadata": {},
   "source": [
    "Is your new `lr_cv` score a good score? Save your answer as either `True` or `False` (boolean) in a variable called `is_good_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca522c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d291b4b",
   "metadata": {},
   "source": [
    "# Assessing your score\n",
    "\n",
    "Is your `lr_cv` score a good score? Save your answer as either `True` or `False` (boolean) in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f5eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_good_score = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3600b6-46e8-4d08-8680-81add610a8bf",
   "metadata": {},
   "source": [
    "<details><summary>Answer</summary> Yes, the score is now within the same order of magnitude as our `y`. So this is a good improvement. However, looking critically at it, we can see that the mean error is about 80% of the data itself. So our error is still large. This means that our target variables are not predictive enough for the task.</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b89c5",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb1f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('cross_validate',\n",
    "                         cv_lr = lr_cv,\n",
    "                         is_good = is_good_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc410a",
   "metadata": {},
   "source": [
    "# üèÅ Finished!\n",
    "\n",
    "Well done! Indeed, this was a terrible score. But the goal of this exercise was to make you familiar with the process, not necessarily to gain the best predictive model. We will be able to do this in subsequent exercises.<br> \n",
    "<span style=\"color:teal\">**Push your exercise to GitHub**</span>, and move on to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a24a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
