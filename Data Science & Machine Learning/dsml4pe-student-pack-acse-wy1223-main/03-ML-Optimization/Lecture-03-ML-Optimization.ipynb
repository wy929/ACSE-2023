{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188501cf-2295-4047-9e15-c72447a22c3f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Day 3**: Optimization üïµÔ∏è (***live in 1.51***)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8f5ad",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Helper function\n",
    "The code below needs to be run first for this class to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6c7bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_epoch(epoch):\n",
    "    gd.update_epoch(epoch)\n",
    "    gd.fig.canvas.draw_idle()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13083130-ba16-452b-aea1-b6222c902a65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center><h1 style=\"color:maroon\">Machine Learning Optimization</h1>\n",
    "<img src=\"https://drive.google.com/uc?id=12grmKE8ZV6LZCM0lkgLnDwtZb0ks8xM3\" style=\"width:1300px;\">\n",
    "    <h3><span style=\"color: #045F5F\">Data Science & Machine Learning for Planet Earth Lecture Series</span></h3><h6><i> by C√©dric M. John <span style=\"size:6pts\">(2023)</span></i></h6></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7886e5e7-548b-45f2-a6b7-66352af926a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Plan for today's Lecture üóì "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4b075-bf58-4be1-a504-15058765b4af",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Understanding what fitting a model means\n",
    "* Brief introduction to Gradient Descent and other solvers\n",
    "* Regression loss Functions\n",
    "* Logistic function and cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c0700c-5e5a-4354-a0a6-a95de8ce7b2b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Intended learning outcomes üë©‚Äçüéì"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb57aed-e77b-4ae3-a068-342530f2d5cf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Understand weights and biases\n",
    "* Build an intuition for ML solvers\n",
    "* Choose an appropriate loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83e2e0-f946-45a9-ac5d-734a17c271ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Fitting parametric models\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=12_Z8xtoi635OZRa9Uu2wHvxg8yMmldKf\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a><br>\n",
    "<br>Prompt: A 35 mm photo of the internal mechanism of a swiss mechanical clock, dramatic lighting.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe08ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Example of Ordinary Least Square (OLS)\n",
    "A Linear Regression (OLS) maps a linear relationship between the input $X$ and the output $y$. It optimizes slope $a$ and intercept $b$ by reducing the residuals between the actual $y$ and the prediction $\\hat{y}$.\n",
    "$$\\hat{y} = aX+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2bb9c4-4e90-4f64-9fdb-94e3f458d3d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\hat{y} = \\color{blue}{\\beta_0}+\\color{blue}{\\beta_1}X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3214b565",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In machine learning terminology, <span style=\"color:blue\">$\\beta_0$</span> (the intercept) is also known as a <span style=\"color:teal\">**bias**</span> (adds a constant) whereas <span style=\"color:blue\">$\\beta_{1}$</span> is referred to as a <span style=\"color:teal\">**weight**</span> (multiplies a feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0230e60",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Today's example: Kaggle Fish Market dataset\n",
    "Chosen because it is simple: <a href=\"https://www.kaggle.com/aungpyaeap/fish-market\">Kaggle fish market dataset</a>.<br>\n",
    "<img src=\"https://drive.google.com/uc?id=12gxXYL8N92aBm6p_aYX74_2ALzSrmglK\" style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf565d48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c565c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import IntSlider, interact, Button\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = pd.read_csv('Lecture_data/fish.csv')\n",
    "lc_train = data.copy()\n",
    "data.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a47ba3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e70569",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Width vs Height for 'Breams' and 'Parkkis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592eb024",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12, 6))\n",
    "data=data[(data.Species!='Bream') & (data.Species!='Parkki')]\n",
    "ax.scatter(data['Width'], data['Height'])\n",
    "ax.set_ylabel('Fish Height')\n",
    "ax.set_xlabel('Fish Width');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b5f60",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Fitting an Sklearn <code>LinearModel()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef2d4d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = MinMaxScaler().fit_transform(data[['Width']])\n",
    "y = data['Height']\n",
    "\n",
    "model = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38197c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30accadc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Plotting data and $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb54d7",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12, 6))\n",
    "ax.scatter(X, y)\n",
    "ax.plot(X, model.predict(X), c='r')\n",
    "ax.set_ylabel('Fish Height')\n",
    "ax.set_xlabel('Normalized Fish Width')\n",
    "ax.set_xlim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767043c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Formulation\n",
    "Given a model expressed as $\\hat{y} = h(X, \\color{blue}{\\beta}) + error$<br> where:\n",
    "* $h(X,\\color{blue}{\\beta}) = \\color{blue}{\\beta_0} + \\color{blue}{\\beta_1} X_1$ in our example\n",
    "* $h$ is the **hypothesis** function\n",
    "* $h(X,\\color{blue}{\\beta})$ is the **prediction** ($\\hat{y})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efbdbeb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sklearn <code>.fit()</code> finds the best parameters to reduce the $error(X,y,\\color{blue}{\\beta})$<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd347438",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model_intercept = model.intercept_ # Beta0\n",
    "model_intercept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb264d62",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model_slope = model.coef_[0] # Beta1\n",
    "model_slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c8468-1501-4bcc-86b5-3383a11af8f9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "**The Loss Function L**<br>\n",
    "In other words, <code>.fit()</code> minimizes the error on what is known as the loss function (<code>L(error)</code>).<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f207b4-ccea-4760-9d01-87587495dde8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "This can also be written as:<br> \n",
    "$$\\color{blue}{\\beta} = \\text{arg}\\min\\limits_{\\color{blue}{\\beta}}\\ L(\\color{blue}{\\beta}, X, y, h)$$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f60194",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "For ordinary least square regression $L$ is the squared error:<br>\n",
    "$$L_{OLS} = \\|error \\|^2 = \\|y -  \\color{blue}{\\color{blue}{\\beta}_0} - \\color{blue}{\\color{blue}{\\beta}_1} X_1\\|^2$$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba863cac-ca7b-43d7-a4c9-d3fe24444132",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loss/Cost Functions $L$ vs performance metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6220b1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A performance metric measures how well a model performs <span style=\"color:red\">***after fitting***</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a3abb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A <span style=\"color:teal\">**Loss function $L$**</span> is used to <span style=\"color:blue\">**fit** the model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9499c4e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Both measure an error response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2439378",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sometime, <span style=\"color:blue\">loss and performance metrics may be the same</span> (e.g. MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf008021",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* But loss needs to be <span style=\"color:teal\">**(sub)differentiable** (ie. smooth)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68304c28-8d6e-46b1-b453-08aa318a1fa3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Machine Learning Solvers\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=12K11w46YaT3FdIM3U5QaCWWmGuDCGK_z\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a><br>\n",
    "<br>Prompt: Photo of a dusty toolshed with metal tools hanging on a wall, washed out light.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf51fdf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Why we use solvers\n",
    "<img src=\"https://drive.google.com/uc?id=12fYUO35rKUDquPWhyf49hATeVKu7H4cD\" style=\"width:900px\"/>\n",
    "<a href=\"https://link.springer.com/chapter/10.1007/978-3-319-23036-8_25\">Menon & Namitha, 2015</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d80844",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Direct solution is <span style=\"color:red\">**not always possible:**</span> matrix inversion requires full rank matrices (same amount of columns than rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8acb63",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **For large matrices**, matrix inversion is **<span style=\"color:red\">very computationally inefficient</span>** (see above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a42a4c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* <span style=\"color:blue\">**SOLUTION:**</span> Approximate the solution through optimization (finding the minimum of a function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b516d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Intuition behind Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57b6e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Uses **the slope (gradient) of the Loss Function** as an indicator<br>\n",
    "* As the slope approaches zero, the Loss approaches its minimum<br>\n",
    "* Illustrated here for one of the two parameters of OLS (the intercept or the slope)<br>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=12tD5pHdrv-LmbK6SseeuZqW83jUwI8SV\" style=\"width:900px;\">\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/quick-guide-to-gradient-descent-and-its-variants-97a7afb33add\">Kansal, 2020</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96996a48",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 1-D gradient descent (finding the intercept $\\beta_{0}$)\n",
    "* Randomly initialize parameter value $\\beta_0^{(0)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6aef9e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Compute the derivative of the <span style=\"color:blue\">loss function</span> ($\\frac{\\partial L}{\\partial \\color{blue}{\\beta_0}}$) at point $\\beta_{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177978ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Update the $\\beta_{0}$ proportionally to $\\frac{\\partial L}{\\partial \\color{blue}{\\beta_0}}$ and to a given step size known as the <span style=\"color:teal\">***learning rate*** ($\\eta$)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab126327",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* $\\color{blue}{\\beta_0}^{\\color {red}{(k+1)}} = \\color{blue}{\\beta_0}^{\\color {red}{(k)}} - {\\eta} \\frac{\\partial L}{\\partial \\color{blue}{\\beta_0}}(\\color{blue}{\\beta_0}^{\\color{red}{(k)}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf407b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* One update cycle where all of the training data is seen is known as an <span style=\"color:teal\">**epoch**</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab3cb9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Repeat as many **epochs** as necessary for our algorithm to converge to our stopping criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5936e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss Function for our regression (Fish Market Dataset)\n",
    "$$MSELoss = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\beta_{0}-\\beta_{1}*X_{i})^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1803bdbf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\frac{\\partial\\ MSELoss}{\\partial\\ \\beta_{0}} =  \\frac{1}{n}\\sum_{i=1}^{n}-2*(y_{i}-\\beta_{0}-\\beta_{1}*X_{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb4e098",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\frac{\\partial\\ MSELoss}{\\partial\\ \\beta_{0}} =  -\\frac{2}{n}\\sum_{i=1}^{n}(y_{i}-\\beta_{1}*X_{i}-\\beta_{0})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dcd6ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Gradient Descent $\\eta$ =0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b6c8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from solvers import GradientDescent\n",
    "epochs = 1500;eta=.7;initial_beta0=-.01;\n",
    "gd = GradientDescent(initial_beta0, eta, epochs, model_slope, X, y, model_intercept, model_slope).regression_vs_gradient();\n",
    "middle_case = [gd.betazeros, gd.losses, gd.gradients]\n",
    "\n",
    "@interact(epoch=IntSlider(min=0,max=epochs-1,value=0))\n",
    "def plot(epoch):\n",
    "    update_epoch(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890683f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Stopping Criterions\n",
    "\n",
    "Gradient Descent can have different **stopping criterions** :<br>\n",
    "\n",
    "* **Minimum Step Size** (e.g. 0.001). When the step size is smaller, the Gradient Descent has converged, and the corresponding intercept is the optimal value.\n",
    "* **Maximum number of steps** (e.g. 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85dc2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gradient Descent $\\eta$ =0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717af3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 1500;eta=.05;initial_beta0=-.01;\n",
    "gd = GradientDescent(initial_beta0, eta, epochs, model_slope, X, y, model_intercept, model_slope).regression_vs_gradient();\n",
    "low_case = [gd.betazeros, gd.losses, gd.gradients]\n",
    "\n",
    "@interact(epoch=IntSlider(min=0,max=epochs-1,value=0))\n",
    "def plot(epoch):\n",
    "    update_epoch(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4e56c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Gradient Descent $\\eta$ =1.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b3e7e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 1500;eta=1.04;initial_beta0=-.01;\n",
    "gd = GradientDescent(initial_beta0, eta, epochs, model_slope, X, y, model_intercept, model_slope).regression_vs_gradient();\n",
    "high_case = [gd.betazeros, gd.losses, gd.gradients]\n",
    "\n",
    "@interact(epoch=IntSlider(min=0,max=epochs-1,value=0))\n",
    "def plot(epoch):\n",
    "    update_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a32f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3,figsize=(20,6))\n",
    "      \n",
    "for ax, index, title in zip(axes.flatten(),[0,1,2],['Intercept', 'Loss', 'Gradient']):\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    \n",
    "    for data, label in zip([low_case, middle_case], [r'$\\eta=0.05$',r'$\\eta=0.7$']):\n",
    "        ax.plot(range(len(data[0])),data[index], label=label)\n",
    "        ax.set_xlim(0,70) \n",
    "        ax.legend()\n",
    "        \n",
    "axes[0].set_ylim(0,3); axes[1].set_ylim(0.3,3); axes[2].set_ylim(2,-3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51023d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Effect of Learning Rate\n",
    "**Small learning rate**<br>\n",
    "\n",
    "* Smoother path to mimimum\n",
    "* Requires more epochs\n",
    "* May get stuck at local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2ba05",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Large learning rate**<br>\n",
    "\n",
    "* Requires less epochs\n",
    "* More random at first\n",
    "* May never converge!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f8eff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üí° **Descent always converges faster when features are scaled**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c19d6c-a1b1-452e-9e24-97a1423a8eda",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2D descent : How to co-optimize $\\color{blue}{\\beta_0}$ and $\\color{blue}{\\beta_1}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534fffa8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "The Loss function would be 3-dimensional and look something like this.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=12OAKwoVPcPRAznuwcGjqmz4n7TLOUvXE\" style=\"width:1200px\">\n",
    "<a href=\"http://primo.ai/index.php?title=Gradient_Descent_Optimization_%26_Challenges\">From Primo.ai</a><br>\n",
    "\n",
    "* This is called the **Energy Landscape** of the loss function<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447fdfb0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Use the **same principle** as above: calculate partial derivative for each term at all steps<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc2382-ab8f-4855-90b3-bdf4d288430b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Generalizes to **$n$ parameters** (dimensions)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a77e5-8016-4a9f-8dc6-405316d6f988",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Vector formulation of gradient descent for $n$ parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c040269-e82a-486e-b1b4-f0c064cdcb8d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Start with random values of $\\color{blue}{\\beta}$ (epoch 0)<br>\n",
    "\n",
    "At each **epoch** update all parameters $(\\color{blue}{\\beta_0}^{\\color {red}{(k+1)}}, \\color{blue}{\\beta_1}^{\\color {red}{(k+1)}},..., \\color{blue}{\\beta_n}^{\\color {red}{(k+1)}}$) in the direction of \"downard pointing gradient\" with a learning rate $\\eta$ (eta)<br>\n",
    "\n",
    "\n",
    "$\\color{blue}{\\beta}_0^{\\color {red}{(k+1)}} = \\color{blue}{\\beta_0}^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\color{blue}{\\beta_0}}(\\color{blue}{\\beta}^{\\color{red}{(k)}})$ <br>\n",
    "$\\color{blue}{\\beta_1}^{\\color {red}{(k+1)}} = \\color{blue}{\\beta_1}^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\color{blue}{\\beta_1}}(\\color{blue}{\\beta}^{\\color {red}{(k)}})$<br>\n",
    "$                      [...] $<br>\n",
    "$\\color{blue}{\\beta_n}^{\\color {red}{(k+1)}} = \\color{blue}{\\beta_n}^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\color{blue}{\\beta_n}}(\\color{blue}{\\beta}^{\\color {red}{(k)}})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ff6d3-616b-4b50-ab75-02775b35c4b9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "We can consider the vector of partial derivatives, called the **gradient** vector (**nabla**:$\\nabla$)$${\\displaystyle \\nabla L(\\color{blue}{\\beta})={\\begin{bmatrix}{\\frac {\\partial L}{\\partial \\color{blue}{\\beta_{0}}}}(\\color{blue}{\\beta})\\\\\\vdots \\\\{\\frac {\\partial L}{\\partial \\color{blue}{\\beta_{n}}}}(\\color{blue}{\\beta})\\end{bmatrix}}.}$$üí° hence the name <em>Gradient Descent</em><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1e2e2-1bb0-42ed-9740-b51a58af6b66",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Gradient descent - vector formula:\n",
    "$$\\color{blue}{\\beta}^{\\color {red}{(k+1)}} = \\color{blue}{\\beta}^{\\color {red}{(k)}} - \\eta \\ \\nabla L(\\color{blue}{\\beta}^{\\color{red}{(k)}})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82d44c2-eb73-4285-ae01-c3c504043e59",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Modified versions of Gradient Descent\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=12k9rbB1afto8PammQDk57E0YSfaxUPgs\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a><br>\n",
    "<br>Prompt: A futuristic looking formula 1 car is driving very fast down a busy market street.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ebca5-a8f5-48aa-ab40-641a262d2043",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "üëé Gradient Descent is **computationally expensive** on big datasets<br>\n",
    "At **each epoch** we evaluate $\\nabla L$  using all $n$ observations, for each $x$ features. Other approaches exist.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c233e1b-e9d9-40ea-93d0-964514f3f2db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Classical Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc8816-b99c-444c-9625-994c05d337f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Loop one by one over all $n$ observations\n",
    "* Select a **single, randomly selected data point** \n",
    "* Compute the Loss/gradient for this single point\n",
    "* update all the <span style=\"color:blue\">$\\beta$</span>\n",
    "* Once all $n$ observations have been viewed, repeat another epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa4c1d5-1983-41d3-affb-def9fe5d735b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Batch or mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663cc7c-d78c-49a3-b549-af7ac0bb7951",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "At each iteration, compute an **\"approximate\" Loss** based on a batch of data and take one step against its gradient<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a242cf-c200-4f1a-9d21-eef1ddbace92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Choose minibatch size (e.g. 16)<br>\n",
    "\n",
    "Loop over your $n$ observations, minibatch-per-minibatch<br>\n",
    "\n",
    "For each minibatch calculate $\\nabla L_{mini}$\n",
    "Use this gradient to update $\\beta^{\\color {red}{(k+1)}} = \\beta^{\\color {red}{(k)}} - \\eta \\ \\nabla L_{mini}(\\beta^{\\color{red}{(k)}})$\n",
    "Move to next $>X_{mini}$ (e.g. the 16-32 observations)\n",
    "\n",
    "\n",
    "Once all $n$ observations have been viewed, repeat another **epoch**<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626adf58-189c-451c-944e-199347b9f585",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By working on a single point rather than the dataset average, the SGD is less stable.<br>\n",
    "\n",
    "The **Loss fluctuates** from epoch to epoch more and does not necessarily decrease.<br>\n",
    "As a result, the **steps** taken are **less direct** towards the minimum\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=12n8lmQgcUNnWrdRfpaYx2cXHGLXWzlF7\" style=\"width:1300px\"/>\n",
    "<img src=\"https://drive.google.com/uc?id=12kt-SMa2dfAVkZEnC4KKZ7jqEaVH11_w\" style=\"width:1300px\"/>\n",
    "<a href=\"https://dominicm73.blogspot.com/2020/09/gradient-descent-rule-and-widrow-hoff.html\">Source of the plot</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de9e46-efa3-4557-909f-9a739e219656",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font color=\"green\">Pros</font><br>\n",
    "\n",
    "SGD is faster for very large datasets\n",
    "Jumps out of local minima!\n",
    "Allows to reduce RAM load (see deep learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339471d-1406-402d-abf6-d8cdbd8e45f6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<font color=\"red\">Cons</font><br>\n",
    "\n",
    "Need more epochs\n",
    "Never exactly converges (careful when to stop?)\n",
    "Maybe slower for small $p$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d3332",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# SGD Algorithms in Scikit-Learn\n",
    "\n",
    "* For large dataset, it is much faster to use **SGD** in Scikit-Learn\n",
    "* Two classes: <code>SGDRergessor</code> and <code>SGDClassifier</code>\n",
    "* The <span style=\"color:teal\">choice of loss function determines the exact algorithm</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2711b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd = SGDRegressor(loss='squared_error', eta0=0.5)\n",
    "\n",
    "sgd.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c334f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12, 6))\n",
    "ax.scatter(X, y)\n",
    "ax.plot(X, model.predict(X), c='r', label='LinearRegressor()')\n",
    "ax.plot(X, sgd.predict(X), c='purple', label='SGDRegressor(loss=\"squared_error\")')\n",
    "ax.set_ylabel('Fish Height')\n",
    "ax.set_xlabel('Normalized Fish Width')\n",
    "ax.set_xlim(0,1); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19767bbd-f8cd-4e89-a41d-8150fcbc2c40",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Second Order Derivative (Hessian) Solvers <span style=\"color:blue\">[in brief]</span>\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=12ahBFnMZ6GR2BElDMYhC5-CDpFWvVCA2\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a><br>\n",
    "<br>Prompt: Sunrise on the town of Hesse in Germany by a hazzy autumn day, golden colors, filtered light.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e2462",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### General Idea:\n",
    "By incorporating informations from the <span style=\"color:teal\"> **second derivative (Hessian)**</span> of the function, we should be able to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef91ab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. **Converge faster** to the global minima of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592e432",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Avoid **local minima**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619e0db",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Newton's Method\n",
    "* Newton is the most basic second order solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8b3ab9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Centered around a quadratic approximation of $f$ for points near $xn$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffdb6d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$f(x+\\Delta x)‚âàf(x)+\\Delta x^{T}\\nabla f(x)+\\frac{1}{2}\\Delta x^{T}(\\nabla^{2}f(x))\\Delta x$$\n",
    "Where $\\nabla$ are the gradient and $\\nabla^{2}$ the Hessian of the function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a5d8b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Newton's method bottom line:\n",
    "* will converge to a unique global minimizer for convex function. \n",
    "* For non-convex functions will still works but is only guranteed to converge to a local minimum. \n",
    "* Calculating the Hessian for large parameter space is impractical (too computationally expansive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa2d08",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Limited-memory Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno algorithm (L-BFGS)\n",
    "* L-BFGS is a ***Quasi-Newton*** method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca887fa6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The idea is to <span style=\"color:red\">**approximate**</span> the Hessian rather than exactly calculate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb21c1c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Much faster to compute than Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e3a01",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* L-BFGS has been called <span style=\"color:teal\">**\"the algorithm of choice\"**</span> for fitting log-linear (MaxEnt) models and conditional random fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3343b9-c940-4bc2-8ee2-40c08c35d6ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Loss Functions\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=12Xeqc_tQIJz9UcznfL-E5GFSJu4ovmU-\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a><br>\n",
    "<br>Prompt: A british tourist lost in the street of Beijing amongst a crowd of busy people, dramatic lighting.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d63dc8-cda8-43a1-badb-7ede715fdc1a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Regression Loss Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2286e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### L1 (MAE) vs L2 (MSE) loss\n",
    "\n",
    "$$ L_1 = MAE = \\frac{1}{n}\\sum_{i=0}^n | \\hat{y}_i - y_i|\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:L_2 = MSE = \\frac{1}{n}\\sum_{i=0}^n (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=12MelUt0kKQ50J3S95l5x0Sgg6QyAvjG6\" style=\"width:2000px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9c7b0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* MSE very sensitive to outliers vs. MAE less strict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173c7136-3c56-4ad2-b9dd-99284d36e7a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* MAE requires learning rate $\\eta$ which decreases at every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00263181-ec34-4daf-808b-8bcacd94a780",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Huber loss (mixed L1 and L2 losses, also called Smooth Absolute or Smooth L1)\n",
    "<img src=\"https://drive.google.com/uc?id=12MRmkH4u6YoUlAT4laxUo9yCbh4O7sL1\" style=\"width:900px\"/>\n",
    "<a href=\"https://link.springer.com/article/10.1007/s11263-019-01275-0\">Image from Feng et al 2020</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a129737",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Parametrized MAE which becomes MSE when error is small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2ded4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$L_{\\delta} = \\left\\{\n",
    "  \\begin{array}{lr}\n",
    "    \\frac{1}{2}(y - \\hat{y})^{2} & : |y - \\hat{y}| \\leq \\delta\\\\\n",
    "    \\delta*(|y-\\hat{y}|-\\frac{1}{2}*\\delta) & :  |y - \\hat{y}| > \\delta\n",
    "  \\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89fc36",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Adjustable for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66348ea5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Slope can be used as an indicator of reaching minima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d9ba22-56a9-4142-a2fe-62fb0c57f547",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Classification Losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fcb76-fd47-4a04-b6eb-4490e8bf5d48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### Sarting Point: Logistic classifiers\n",
    "<img src=\"https://drive.google.com/uc?id=12g2yRHu6xie9yRtf2_hbBKqVubMaI2qo\" style=\"width:1500px\"/><br>\n",
    "<a href=\"https://medium.datadriveninvestor.com/logistic-regression-18afd48779ce\">Maheshwari, V, 2018</a>\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab3a096-ef08-45e0-9bb4-7fb2417320c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### We can apply $\\sigma$ to our **hypothesis function**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba004178-dfc9-4c96-8973-203a45a2e036",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$h = \\color{blue}{\\beta}_0 + \\sum_{i=1}^{n}\\color{blue}{\\beta}_i x_i = \\color{blue}{\\beta}^{T}x$ (vectorized form of the hypothesis function where $x_0$ is always set to be 1)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d20f959-261e-469d-9956-3a9d35cf55aa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The logistic regression assumes that for a single point $x$ the probability of $y=1$ is the logistic function applied to $h$:<br>\n",
    "$P(Y=1 | X=x) = \\sigma (\\color{blue}{\\beta}^{T}x)$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f01c5dd-dcd5-4f55-a1b8-9cbc86d47994",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is also often written as:<br>\n",
    "$P(Y=1 | X=x) = \\sigma (\\color{blue}{\\beta}^{T}x)$ <br>\n",
    "$P(Y=0 | X=x) = 1 - \\sigma (\\color{blue}{\\beta}^{T}x)$ by law of probability<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b758f6-db28-404e-9017-7fad9652d7b6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Likelihood function\n",
    "In binary classification the outcomes follow a Bernoulli distribution and we can write our pobability function as follows:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70b1b25-2e31-4c0a-ba28-8e6a7eaf675a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$P(Y=y | X=x) = \\sigma (\\color{blue}{\\beta}^{T}x)^{y}[1-\\sigma (\\color{blue}{\\beta}^{T}x)]^{1-y}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97861a-357c-4148-819c-b52608c9f17f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$P(Y=y | X=x) = \\left\\{\n",
    "  \\begin{array}{lr}\n",
    "    \\sigma (\\color{blue}{\\beta}^{T}x)\\leftarrow y=0\\\\\n",
    "    [1-\\sigma (\\color{blue}{\\beta}^{T}x)]\\leftarrow y=1\n",
    "  \\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5fb66a-29f5-4228-9f9d-b0d14e1508bb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Log Likelihood\n",
    "It is easier to work with log of the likelihood function:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd39f0-703f-481f-aca0-a24cdc884a5f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\log{P(Y=y | X=x)} = \\log{[\\sigma (\\color{blue}{\\beta}^{T}x)^{y}[1-\\sigma (\\color{blue}{\\beta}^{T}x)]^{1-y}]}=\\log{[\\sigma(\\color{blue}{\\beta}^{T}x)]}+(1-y)\\log{[1-\\sigma(\\color{blue}{\\beta}^{T}x)]} $<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32519fa0-4860-4322-9a1c-a99aedc15046",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **log loss** or **binary cross-entropy loss** is simply the negative of the log likelihood so that we can **<i>maximize</i>** it with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47610a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$LL_{CE}(y,\\hat{y})= -\\log{[\\sigma(\\color{blue}{\\beta}^{T}x)]}+(1-y)\\log{[1-\\sigma(\\color{blue}{\\beta}^{T}x)]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83d9534-bf95-448d-bb1f-db2036908b69",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **<i>loss</i>** function is calculated for 1 sample, the **<i>cost</i>** function for all samples is the **average of all losses**:<br>\n",
    "$$LL_{CE}(\\color{blue}{\\beta}) = - \\frac{1}{n} \\sum_{i=1}^{n} y_i\\log\\sigma(\\color{blue}{\\beta}^{T}x_i)+(1-y_i)\\log[1-\\sigma(\\color{blue}{\\beta}^{T}x_i)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820fa69-5970-4732-84ee-02872eb39874",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cross-Entropy Loss\n",
    "<img src=\"https://drive.google.com/uc?id=12g11ACQ2XOaTtMRGkgR3VBjiBoHNhW1t\" style=\"width:2000px\"/>\n",
    "‚òùÔ∏è Infinitely penalize wrong prediction<br/>\n",
    "üí° Cross-Entropy name comes from <a href=\"https://www.youtube.com/watch?v=ErfnhcEV1O8\">Shanon Theory of information</a><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d303c-d2b8-45b0-b9a1-62103352d28d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "ü§î The gradient of the **log-loss** of the **sigmoid** function is simple in vectorial form<br>\n",
    "$\\nabla LogLoss_{sigmoid} = - \\frac{2}{n} \\beta^T(y - \\hat{y})$<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f9e0e0-966e-4e74-97e9-be9deb524d9b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Exact **same formula** than that of the mse-loss of a linear regression<br>\n",
    "$\\nabla MSE_{linear} = - \\frac{2}{n} \\beta^T(y - \\hat{y})$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7d2c2-c184-432d-ad89-980b46f00320",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<br/><br>\n",
    "‚ö†Ô∏è These gradients **do not have the same value** of course as:<br>\n",
    "* $\\hat{y}_{sigmoid} = \\frac{1}{1+e^{-\\color{blue}{\\beta}^T}}$\n",
    "* $\\hat{y}_{linear} = \\color{blue}{\\beta}^{T}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f48b52-cdd9-4ca7-9f8b-36ff36d282ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90feee1-78ee-4209-a187-237680082825",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Regularization means adding a **penalty term** to the Loss that **increases** with $\\beta$\n",
    "$$\\text{Regularized Loss} = Loss(X,y, \\beta) + Penalty(\\beta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2271ae-0325-4f69-a175-b636349fe37b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "üëâ Penalizes large values for $\\beta_i$<br>\n",
    "üëâ Forces model to shrink certain coefficients or even select less features<br>\n",
    "üëâ Prevents overfitting <br>\n",
    "$$\\hat{y} =  \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2 + \\beta_3 X_1^3 + ... $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ff7f5-6848-4689-8bd2-0544642e2a33",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Two famous Regularization penalties:<br><br>\n",
    "**Lasso** (L1)<br>\n",
    "$$L1 = Loss  + \\alpha \\sum_{i=1}^n |\\beta_i|$$ <br>\n",
    "**Ridge** (L2)<br>\n",
    "$$L2 = Loss + \\alpha \\sum_{i=1}^n \\beta_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18190e92-4900-420a-9e7e-97f807298328",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Introduces the new hyper-parameter $\\alpha$:\n",
    "* Dictates **how much** the model is **regularized**\n",
    "* Large $\\alpha$ force **model complexity and variance to decrease**, but **bias increases**\n",
    "* Notice $\\sum$ starts from $i=1$, i.e. intercept coefficient is not penalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973227d4-8d70-428f-ad96-66eacd3a8adb",
   "metadata": {
    "tags": []
   },
   "source": [
    "‚ö†Ô∏è   Always **scale** your feature before regularization to penalize each $\\beta_i$ fairly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e689b9-1ce7-4a28-b347-7d8f2fd2f733",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "def test_reg_coefs(data=lc_train):\n",
    "    X = data[['Length1','Length2','Length3', 'Height']]\n",
    "\n",
    "    poly_tr = PolynomialFeatures(degree=2).fit(X)\n",
    "    X = pd.DataFrame(poly_tr.transform(X))\n",
    "    X = pd.DataFrame(StandardScaler().fit_transform(X),columns=X.columns)\n",
    "\n",
    "    # The target is the weight of a fish\n",
    "    Y = data[['Weight']] \n",
    "\n",
    "    linreg = LinearRegression().fit(X, Y)\n",
    "    ridge = Ridge(alpha=1.5, max_iter=5000).fit(X, Y)\n",
    "    lasso = Lasso(alpha=1.5, max_iter=5000).fit(X, Y)\n",
    "\n",
    "    coefs = pd.DataFrame({\n",
    "        \"coef_linreg\": pd.Series(linreg.coef_[0], index = X.columns),\n",
    "        \"coef_ridge\": pd.Series(ridge.coef_[0], index = X.columns),\n",
    "        \"coef_lasso\": pd.Series(lasso.coef_, index= X.columns)})\\\n",
    "\n",
    "    return coefs\\\n",
    "        .map(lambda x: int(x))\\\n",
    "        .style.map(lambda x: 'color: red' if x == 0 else 'color: black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601be1f-5e33-41ff-93cd-5cf9a2e1ee53",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_reg_coefs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f5de9-ca28-4bd2-a439-371edcec1b89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Suggested Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e63425-18be-4ac6-94e5-6dda423aea97",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## üì∫ Videos \n",
    "#### Short videos from my Undegraduate Machine Learning Classes:\n",
    "* üìº <a href=\"https://youtu.be/l6mTzbPOWHE?list=PLZzjCZ3QdgQCcRIwQdd-_cJNAUgiEBB_n\">Gradient Descent</a>\n",
    "\n",
    "#### Others:\n",
    "\n",
    "* üìº <a href=\"https://www.youtube.com/watch?v=sDv4f4s2SB8\">StatsQuest - Gradient Descent</a> by Josh Starmer\n",
    "* üìº <a href=\"https://youtu.be/vMh0zPT0tLI\">StatsQuest - Stockastic Gradient Descent Clearly Explained</a> by Josh Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1fe0f6-cfdc-4359-a56a-aea5aabcf5fe",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## üìö Further Reading \n",
    "* üìñ <a href=\"https://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/\">Hessian Free Optimization</a> by Andrew Gibiansky\n",
    "* üìñ <a href=\"https://ruder.io/optimizing-gradient-descent/\">An overview of gradient descent optimization algorithms</a> by Sebastien Ruder, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12875bfd-781a-40f4-8871-67eaf007a396",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## üíªüêç Time to Code ! "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "rise": {
   "scroll": true,
   "theme": "serif",
   "transition": "none"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
