{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be87944c",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-d7H1l1lJ28_sLcd9Vvh_N-yro7CJZcZ\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd434f3",
   "metadata": {},
   "source": [
    "# Seismic event in California: Earthquake or not?\n",
    "\n",
    "In this data problem, you will try to train the best algorithm to detect man-made seismic event (a \"quake\") in California. We will use this as an opportunity to try out the two new `MLPClassifer` you encountered in the class, as well as `tuning` your algorithm. I am giving you the California Earthquakes dataset, with most columns still in. It is your decision what to do with the data (this is a data problem so close to what I would expect for your marked coursework). If you need more details about what the [columns names and what they mean you can be gather it from the USGS](https://earthquake.usgs.gov/data/comcat/data-eventterms.php).\n",
    "\n",
    "For test consistency, **always use `random_state=42`** when you need a random state!\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Open the `earthquakes.csv` dataset and do the following:\n",
    "\n",
    "1. Looking at the `type` column, you will see that it contains the type of seismic event. Some are `earhquake`, some are other things (such as `quarry blast`, `nuclear blast`, etc... Using this information, create a new column called `target` that contains the boolean `True` (is man-made, i.e not an earthquake) and `False` (is  an earthquake). \n",
    "2. Create a target vector `y` that contains the `target` column, and a DataFrame `X` that contains all other features\n",
    "3. Split your data (I suggest 80% train /20% test) and do an EDA on your `X_train` to understand the data\n",
    "4. Prepare your data as you see fit based on your EDA. Pause to think what you are doing!\n",
    "5. We are interested in the `precision` of our algorithm here: we want to make sure that when the algorithm tells us we are dealing with a man-made seismic event, we are over 91% confident that it is man-made. o\n",
    "6. Train an `MLPClassifier` on your `X_train`. Use a `GridSearchCV` approach to find the best hyperparameters, focusing mostly on the network architecture (i.e. number of neurons in each layer, and number of layers), and the learning rate.\n",
    "7. Because `MLPClassifier`s (and all deep-learning algorithms) are so slow to train, I will make a few suggestions here (take them or leave them). First, in your grid search, only train your algorithm with a `max_iter=20`. This will create ugly read warning that you have not converged, but it will be faster and will give you a first order idea of makes a difference. Then, I would suggest limiting your grid to any data preparation differences you might want to check (maybe 2 different pipeline?) and the following - hidden_layer_sizes:[(100,100,), (200,100,50,)], and alpha:[0.0001,0.01]. If you had more time, you could try more hyperparameters but you will see that training is indeed slow.  \n",
    "8. Check which algorithm performs best using your `best_score_`. Save the best algorithm (`best_estimator_`) in a variable called `model`. If you have followed my instruction above, I suggest creating the `model` from scratch but using a `max_iter` value between 500-600, and then refit this new algorithm. This gets around the problem of training to 20 epochs only.\n",
    "9. Using your best algorithm, estimate the final error using your `X_test` and save this as the variable `estimated_precision`.\n",
    "10. Test your code agains unseen data!\n",
    "\n",
    "### What will the `test` check?\n",
    "The test for this exercise will assert wether:\n",
    "1. Your overall `precision` is close to my results\n",
    "2. Your `estimated_precision` is within `5%` of the actual precision of your algorithm on an unseen data (your `predictions.csv` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aae0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1YfrnkagRGdyGEOfQRkUuhCr0V_vnEKej')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('raw_data/training_data.csv')\n",
    "new_events = pd.read_csv('raw_data/seismic_events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ae396",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077fad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('model_score',\n",
    "                         estimated_precision = estimated_precision\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58903c7",
   "metadata": {},
   "source": [
    "# üèÅ Finished!\n",
    "\n",
    "Well done! <span style=\"color:teal\">**Push your exercise to GitHub**</span>, and move on to the next one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
