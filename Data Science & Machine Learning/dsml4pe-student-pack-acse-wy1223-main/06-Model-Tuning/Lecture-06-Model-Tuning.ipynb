{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "868cd5c4-3c28-43eb-950e-b1b49ace461c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Day 6**: Model tuning üéµ (***live in 1.51***)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13083130-ba16-452b-aea1-b6222c902a65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center><h1 style=\"color:maroon\">Model Tuning</h1>\n",
    "    <img src=\"https://drive.google.com/uc?id=14b9bU6GQOrICefbcQXWTpkNOaklHaVpy\" style=\"width:1300px;\">\n",
    "    <h3><span style=\"color: #045F5F\">Data Science & Machine Learning for Planet Earth Lecture Series</span></h3><h6><i> by C√©dric M. John <span style=\"size:6pts\">(2023)</span></i></h6></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2ffcc-c27f-407f-b613-01670a0d1647",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Plan for today's Lecture üóì "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9b144-bbfd-4102-8799-a9a468e904d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Suport Vector Machine\n",
    "* Multi-layer perceptron (neural network)\n",
    "* Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e81a47-44c4-477c-b329-dba5c562b12b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intended learning outcomes üë©‚Äçüéì"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcb777-3c8c-43d6-bebf-7c0d0cf3c1ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Choose a fit for purpose model\n",
    "* Know how to optimize your parameters\n",
    "* Use SVR and SVC for respectively regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961eb62",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Data used today\n",
    "We are back to the <a href=\"https://www.kaggle.com/aungpyaeap/fish-market\">Kaggle fish market dataset</a>.\n",
    "<img src=\"https://drive.google.com/uc?id=15C8CpZBVfyZtK-Lg4_3RLJWkripvkMwW\" style=\"width:1300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798f958",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some packages we will use later to plot...\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "df = pd.read_csv('Lecture_data/fish_no_pikes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30225b4a-8a75-4277-a43e-69d7ba266b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up classification propblems\n",
    "Xc = df.drop(columns=['Species'])\n",
    "yc = df.Species.copy()\n",
    "\n",
    "Xc_train, Xc_val, yc_train, yc_val = train_test_split(Xc,yc, train_size=.7, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(Xc_train)\n",
    "Xc_train = scaler.transform(Xc_train)\n",
    "Xc_val = scaler.transform(Xc_val)\n",
    "\n",
    "encoder = LabelEncoder().fit(yc_train)\n",
    "yc_train = encoder.transform(yc_train)\n",
    "yc_val = encoder.transform(yc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2f2a6-823b-4553-af79-4b0c6788652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up regression propblems\n",
    "Xr = df.drop(columns=['Species', 'Weight'])\n",
    "yr = df.Weight.copy()\n",
    "\n",
    "Xr_train, Xr_val, yr_train, yr_val = train_test_split(Xr,yr, train_size=.7, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(Xr_train)\n",
    "Xr_train = scaler.transform(Xr_train)\n",
    "Xr_val = scaler.transform(Xr_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c5325-e788-4e6f-b80a-4d91ce6c1fe9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Non-Parametric Algorithm of the Day: Support Vector Machines\n",
    "<br>\n",
    "\n",
    "<center><img src=https://drive.google.com/uc?id=15A7GFl3Jk5z8frH-Q7XXWS1h8Zvapxtl\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: Two Doric columns supporting a temple in a soft green light, digital art.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a58d6-4665-455c-8c26-ff6f589302d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### What is the optimal decision boundary for this classification?\n",
    "<img src=\"https://drive.google.com/uc?id=15UW8HhfbxYEww0p7c7et-9jojdt1Y9bN\" style=\"width:1200px\">\n",
    "<a href=\"https://towardsdatascience.com/svm-feature-selection-and-kernels-840781cc1a6c\">Ippoloto, 2019</a>\n",
    "<p>Infinite number of potential decision boundaries that separate the classes (\"hyperplanes\")</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b684f94-0b44-45c7-b45c-00c203d85aa2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=15Ii1Zr9br6ow2cVd_B75TFXzxCXlpQQD\" style=\"width:1200px;\"><br>\n",
    "<a href=\"https://towardsdatascience.com/svm-feature-selection-and-kernels-840781cc1a6c\">Ippoloto, 2019</a>\n",
    "\n",
    "* The hyperplane that generalizes best to unseen data is the one that is furthest from all the points (maximizes the **margin**)\n",
    "* The points on the margin boundary are called **support vectors**\n",
    "* Finding them is a convex optimization problem (one single best solution)\n",
    "* **Maximum Margin Classifier** algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af044706-7883-486c-8f33-95593bead93c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "* Max Margin is super sensitive to outliers\n",
    "* It **overfits** to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad6884-2a9a-4933-b19e-44ff7e8f1382",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "For **generalization** purpose, we may want to allow some points to be **inside** the margin, or even **on the other side** of the decision boundary:<br>\n",
    "<img src=\"https://drive.google.com/uc?id=14ekY95fAY-sKnZP4wDEsyt8q1UQbGtDt\" style=\"width:900px;\">\n",
    "<a href=\"https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe\">Mishra, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c10501c-aa7f-4283-91a9-cc01ce12aee4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Soft margin classifier\n",
    "Allows a few points to be misclassified but with a **penalty ($\\xi$)** for how \"far\" they lie on the wrong side of the margin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fc537-11be-459b-bb47-33d6dd4f9a1d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The **Hinge Loss** is the penalty applied to each point on the wrong side<br>\n",
    "* The deeper a point lies within the margin, the higher the loss\n",
    "* The penalty is linear, like MAE <br>\n",
    "<img src=\"https://drive.google.com/uc?id=14ekY95fAY-sKnZP4wDEsyt8q1UQbGtDt\" style=\"width:800px;\">\n",
    "<a href=\"https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe\">Mishra, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b68ad7-6d22-4337-9694-7ed0f39461e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=15NsX8wGJIf9tar9i2p46SUN5RzCGTSs0\" style=\"width:1300px;\">\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7252cc-b339-4bc7-8a5c-9778f00bae8f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Regulariation hyperparameter <code>C</code>\n",
    "Stength of the penalty applied on points being on the wrong side of the margin\n",
    "* The higher <code>C</code>, the stricter the margin\n",
    "* A \"maximum margin classifier\" has <code>C</code> = $+ \\infty$\n",
    "* The smaller <code>C</code>, the softer the margin, the more it is ***regularized***\n",
    "* C similar to $1/ \\alpha$ in Ridge \n",
    "<img src=\"https://drive.google.com/uc?id=14nSHRMLiBlKmg4SJKGvDxNCMXA3OlhFU\" style=\"width:1200px;\">\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad6846-5d25-4cad-83a4-ec29b4b770d0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>üíª sklearn implementation</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced18a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear', C=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec927fba-152d-448c-846d-2927014f40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(Xc_train, yc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba68b9-05aa-4bb0-b79b-a3912770a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(yc_val, svc.predict(Xc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95528e4-c2b2-4e9f-99d9-4e34daeb98dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# equivalent but with SGD solver\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "svc_bis = SGDClassifier(loss='hinge', penalty='l2', alpha=1/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254cf42d-7302-45b8-a35a-f7c1e1b375cf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "‚ö†Ô∏è Warning: All support vector models requires **scaling**\n",
    "<img src=\"https://drive.google.com/uc?id=15UH1IariB6tQVcmyzLt43h8wPnumj2bZ\" style=\"width:1800px;\">\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2333cd3c-d507-44f6-9e8c-5cfc3d403fe7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# SVM 'kernels'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bc6f85-c643-4b62-84d1-cd10935c4627",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=15Y316SyzWWt9ms2aC0ILzVMXzNmv_CDK\" style=\"width:1200px\">\n",
    "\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a53a739-cc3f-4af4-971d-9650ee373dae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## We can create more features to separate this data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43490c-19ae-4893-8750-38e37a6f49d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=15DY_na_wFvO0COGDI-zN6blQYDyBcg7x\" style=\"width:1300px\">\n",
    "\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b5c4f-62d5-456d-a494-12cff27c9d2e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "‚òùÔ∏è Creates new features - curse of dimensionality!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341a4b4-8ffd-4280-9308-0dc523646f5c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "üåΩ Instead, we can use a mathematical <strong style=\"color:teal\">'kernel'</strong> to simulate new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3406c7-3abc-4b44-8fae-f3075caef4f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# The kernel-trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8e428-3916-47e8-b7dd-d3e454b1546f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "üìè Measure a distance pair-wise between each sample and use this to simulate creating new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe0815-5201-4632-bd20-e0c64e98a96e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* ***Linear kernel function*** for linear datasets (the best for high-dimensional datasets - speed!)<br>$F(x, xj) = sum( x.xj)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa1ef4-524d-4c87-b802-3fdd4d0b8091",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* ***Polynomial kernel functions*** (example of previous slide with $X^3$)<br> $F(x, xj) = (x.xj+1)^d$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95132e2d-4083-417c-8975-5befd6083ef9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* ***Gaussian Radial Basis function kernel (RBF)*** (one of the favourite kernels for non-linear datasets)<br> $F(x, xj) = \\exp(-\\gamma * ||x - xj||^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356732e-bea8-48ee-83fd-93b359e40925",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* ***Sigmoid kernel function*** <br> $F(x, xj) = tanh(Œ±xay + c)$<br><br><a href=\"https://dataaspirant.com/svm-kernels/\">Good reference on SVMs with kernels</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb674dc-f8ac-4139-a126-3911efe892c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## SVM-Regressors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643487a8-df83-4613-ad97-e523e877aef2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Reverse the objective:\n",
    "* **Classification**: fit the largest possible *street* **between** two classes\n",
    "* **Regression**: fit as many points as possible **within** the *street*\n",
    "* Width of the street controlled by an additional hyperparam $\\epsilon$\n",
    "<img src=\"https://drive.google.com/uc?id=14n0-iOcR47_1rChRr09kBi8zvjN6_Zgh\" style=\"width:1200px\">\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbac9b9-2aba-4b78-aa67-f2cadf9f80bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(epsilon=.1, kernel='rbf', C=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3acbc4c-4604-4b33-8691-814bdaf7c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr.fit(Xr_train, yr_train)\n",
    "np.sqrt(mean_squared_error(yr_val, svr.predict(Xr_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd0d8d-9f0c-4042-9cce-b0d5300d350b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Parametric Algorithm of the Day: Neural Networks\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=15PB5vlQbDpDZ0rOqeIQkCnCEMS3evUts\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: Algorithm of the day.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6cb613-50dd-42c7-b00e-3a411cf893a2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052cc66-62f0-4479-8634-8cc5d7093e3d",
   "metadata": {},
   "source": [
    "## Biological Neurons\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=14psocOpGRtt3epKPLpuPxw1MzMNwhlHY\" style=\"width:1000px\">\n",
    "<a href=\"https://www.youtube.com/watch?v=_HMLZHQpQDI\">Krigolson, 2019</a> (YouTube video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4093add-10e2-4961-b7f1-86c514060154",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* üî£ Biological neurons take inputs through their dendrite, transform the input, yield an output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd39ec0-f4a7-4d1c-90d3-aa879451a43e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* üî• The neuron <span style=\"color:brown\">**only fires if a threshold in signal is reached**</span> (non-linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313345d9-7d01-4c90-bb7c-7374d65bd192",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ü§ñ Artificial neurons are <span style=\"color:blue\">*loosely*</span> inspired from the biological neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a56a6e2-e02c-44bd-9c40-39d37951f333",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Artificial Neurons\n",
    "\n",
    "The combination of the linear regression followed by an activation function is effectively what is known as an **ARTIFICIAL NEURON**!\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=15QdD-8cXoqeIL18ChQeXGjilUkMP1vW9\" style=\"width:800px\">\n",
    "<a href=\"https://www.amazon.com/dp/0131471392\">Haykin, 2008</a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b563b-f75a-4490-af87-efe08d7903dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Notice the <span style=\"color:blue\">**Activation Function**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef32d7-ad58-4b26-bcf5-b5fda3bbebbe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's write one from scratch in Python!\n",
    "\n",
    "Neural networks are surprisingly easy to code. Let's imagine that we have feature vector (`X`, shape=4) that pertain to weather, and a label (`y`) that indicates whether it will rain (`1`) or not (`0`) in the next hour. Here is how a sample would look:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a721216-37b1-4206-b855-760675227a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example of a 'rainy day':\n",
    "\n",
    "y = 1 \n",
    "X = [1., -3.1, -7.2, 2.1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b40b71-f866-46ab-8ea1-65ecb7ff2d54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "‚òî We want to **predict whether or not it will rain**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51b0e8-a879-48c7-bad8-151cfe5658a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can write a function that returns a linear regression wiht some weights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ca6be-e24a-4ade-a1c6-d41260dc9484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linreg_1(X):\n",
    "    return -3 + 2.1*X[0] - 1.2*X[1] + 0.3*X[2] + 1.3*X[3]\n",
    "\n",
    "out_1 = linreg_1(X)\n",
    "out_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bdd1b4-faad-4583-99c0-6b0ed6c45af6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Activation function\n",
    "\n",
    "* üìè As writen above, our <span style=\"color:red\">algorithm is simply a linear regression</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf3fe6-cbf6-4ca0-aaf6-9b7c6aeb4e3f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    "* ‚ùÑÔ∏è The trick is to take the output of the linear function, and <span style=\"color:blue\">transform it via an activation function</code>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268380e-e8f3-44b7-9104-7189eb90842e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* üöß The activation function will <span style=\"color:teal\">only output the value if certain conditions are met</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6624c84-1975-4df9-b389-7270ce0d96c8",
   "metadata": {},
   "source": [
    "**Some well-known activation functions:**\n",
    "<img src=\"https://drive.google.com/uc?id=14YT7Hla4C4NchT7EVnGraEjRI_jnH9cO\" style=\"width:1000px\">\n",
    "                                                   \n",
    "<a href=\"https://medium.com/@shrutijadon/survey-on-activation-functions-for-deep-learning-9689331ba092\"> Jadon, 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b4cc7-dffa-41a3-9bc2-346b52c30a91",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<span style=\"color:brown\">(**ReLU** most commonly used these days)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e65083-6950-4470-998a-3fcf9eb57a3f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implementing the 'ReLU' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf72c0-2284-4d84-9156-f97eb5020e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    if x > 0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "out_1 = activation(out_1)\n",
    "out_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be456e-77a8-4077-9fad-cf498cb3ddd9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Adding more neurons with the same inputs but different weights\n",
    "\n",
    "We can\n",
    "* apply **other** linear regressions (neurons) to the same input X\n",
    "* followed by the **same** activation function\n",
    "* but with different **(trainable) weights and biases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9fe81e-96b3-4e4f-b31f-1bdce04402ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def linreg_2(X):\n",
    "    return -5 - 0.1*X[0] + 1.2*X[1] + 4.9*X[2] - 3.1*X[3]\n",
    "\n",
    "out_2 = activation(linreg_2(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d251c-e146-4d7e-8452-66057a8606ce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee12e061-2d23-4b60-8ae8-ac8fca3a4489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_3(X):\n",
    "    return -8 + 0.4*X[0] + 2.6*X[1] +- 2.5*X[2] + 3.8*X[3]\n",
    "\n",
    "out_3 = activation(linreg_3(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0283a7-0ffe-429d-8b30-5d0a4b0d7b40",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### We just wrote a layer of neurons!\n",
    "Each neuron receives the same input (`X`), has different weights, and uses the same activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368dc45-37cd-4ad6-adab-05eb0b7cc849",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In neural networks, the next step is to give the output of these neurons as input to the next layer of neurons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d24d0-a24b-42c6-bd6f-1d7c71fbe2a5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Building a Neural Network\n",
    "\n",
    "A neural network is a complex function $f_{\\theta}$:\n",
    "$$f_{\\theta}(X) = \\hat{y}$$\n",
    "\n",
    "Where <span style=\"color:blue\">$X$</span> is the feature vectors, <span style=\"color:purple\">$\\theta$</span> are the weights and biases of the linear regressions that take place within each neuron, and <span style=\"color:brown\">$\\hat{y}$</span> is the prediction output of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f0f9f-ecd7-479f-8696-bd5e00fe4a97",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=15M4QdS2e81iBmKsy3h5TvUwHmpup-Hzs\" style=\"width:1000px\">\n",
    "<a href=\"http://www.astroml.org/_images/fig_neural_network_1.png\">≈Ω. Iveziƒá et al., 2014</a>\n",
    "\n",
    "The way the neurons and the weight and biases are connected is known as the **architecture** of your neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d92f69-f339-403c-89c6-f0e32f3db33d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implementing the next layer (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264772c-800b-4765-9d25-6f2cc8c19bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linreg_next_layer(X):\n",
    "    return 5.1 + 1.1*X[0] - 4.1*X[1] - 0.7*X[2]\n",
    "\n",
    "def activation_next_layer(x):\n",
    "    # this is known as the sigmoid activation, used for clasification task!\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "def neural_net_predictor(X):\n",
    "    \n",
    "    out_1 = activation(linreg_1(X))\n",
    "    out_2 = activation(linreg_2(X))\n",
    "    out_3 = activation(linreg_3(X))\n",
    "    \n",
    "    outs = [out_1, out_2, out_3]\n",
    "    \n",
    "    y_pred = activation_next_layer(linreg_next_layer(outs))\n",
    "    \n",
    "    return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd2d41-a56e-45e7-b9a2-4325a7a7ef6a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Final prediction\n",
    "y_pred = neural_net_predictor(X)\n",
    "\n",
    "print(f' Probability of rain: {y_pred:.02f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2779ce77-0407-4b10-97dd-3c542cb256cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### üéâ Congrats! You just build your first (artificial) neural network\n",
    "\n",
    "* and because it is in pure `Python`, it is **super inefficent**!\n",
    "* Also, the weights and biases in our function are fixed (not trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bba5172-7b3e-4b35-be89-9125e1ef3b15",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deep-Learning with <code>scikit-learn</code>\n",
    "\n",
    "Although there is a module called <code>neural_network</code> in <code>scikit-learn</code>, it contains only three algorithms (<code>BernoulliRBM()</code>,<code>MLPClassifier()</code>, and <code>MLPRegressor()</code> and, in practice, it is very limited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38104461-1972-4772-873f-021bc3e3cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30, 10), activation='relu', max_iter=5000)\n",
    "\n",
    "x = [[1.2,0.1], [2.3,0.4], [1.3,0.2], [1.5,0.2], [4.6,0.12], [2.3,0.23]]\n",
    "y = [0,1,1,1,0,1]\n",
    "\n",
    "mlp.fit(Xc_train,yc_train)\n",
    "accuracy_score(yc_val, mlp.predict(Xc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f8370-5372-4a68-80f2-7379eea2f2c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üö® This works, but we are not able to devise our own architecture beyond deciding how many layers and number of neuron per layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb7cc49-f7a4-476d-9e76-cd1ed8679b33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üí° For this, we need a framework specific for Deep-Learning: the two most popular today are <code>TensorFlow.keras</code> written and supported by **Google**, and <code>PyTorch</code> written and supported by **Meta**. But this is the topic of the next module..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1181dd6-beaf-40b7-a4fb-3e4741bae326",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Model Tuning: Finding the best Hyperparameters\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1582C1iMgMXhGYkmIjzmdOb1JqxMXjhTL\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a><br>\n",
    "<br>Prompt: A jazz band playing a happy tune on their saxophone in the lively streets of New Orleans, vivid colors.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c692475c-885e-429b-97c8-7d759deb6111",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# How to select the right model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64f9193-61be-481b-aa33-395f751e1990",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Model complexity vs dataset size (rule of thumb üëç)\n",
    "\n",
    "* More than 100,000 datapoints: Parametric models (SGD, Neural Nets)\n",
    "* Less than 100,000 datapoints: Non-parametric models (KNN, SVM, Decision Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5811e6e-b14f-4ad3-a625-8e92ef2e545c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<img src=\"https://drive.google.com/uc?id=14lT6Ji1P3H8SjjNWu0rWyKnQ77XzusZL\" style=\"width:1500px;\">\n",
    "<a href=\"https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\"><code>sklearn</code> algorithm cheat sheet</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56eeb87-f3d5-45e6-a8ff-723a8f13f70d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### The grid search method\n",
    "Explores different hyperparam value combinations to find those optimizing performance\n",
    "<img src=\"https://drive.google.com/uc?id=15W2h4nY0n-PYVK66ub38GDPZ7wprbH8I\" style=\"width:600px;\">\n",
    "<a href=\"https://medium.com/@jackstalfort/hyperparameter-tuning-using-grid-search-and-random-search-f8750a464b35\">Stalfort, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc9f8e8-93c2-4b9b-89c3-4ba3f94e83e6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "* Also applied using a *validation set* (never use test set for model tuning!)\n",
    "* Select which grid of values of hyper-parameters to try out\n",
    "* For each combinations of values, measure your performance on the *validation set*\n",
    "* Select hyperparams that produce the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ab1c7-57df-4db4-baf2-56e989b8d019",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**üî• Grid Search CV**<br>\n",
    "<img src=\"https://drive.google.com/uc?id=15Z-e0IwNR10Dg2dF12Abpsjq0kIIbTBu\" style=\"width:1200px;\"><br>\n",
    "<a href=\"https://stats.stackexchange.com/questions/424477/how-to-make-train-test-split-with-given-class-weights\">StackExchange, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894bf3ee-4a25-458d-b253-0608e674d02a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Sklearn  <code>GridSearchCV</code> üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17448d1-ddd1-4554-aefc-877f34ff30f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X = df.drop(columns=['Species', 'Weight'])\n",
    "y = df[['Weight']]\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "# Instanciate model\n",
    "model = Ridge()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {'alpha': [0.01, 0.1, 1], \n",
    "        'solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg']}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "search = GridSearchCV(model, grid, \n",
    "                           scoring = 'r2',\n",
    "                           cv = 5,\n",
    "                           n_jobs=-1 # paralellize computation\n",
    "                          ) \n",
    "\n",
    "# Fit data to Grid Search\n",
    "search.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35041348",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best score\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385f831",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best Params\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47df4a-dbda-46ae-85b9-a97f6ba8d07b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best estimator\n",
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14426b6-cbdb-4cd0-b388-1cd0a9ef11eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p>üëé Limitations of Grid Search:</p>\n",
    "<ul>\n",
    "<li>Computationally costly</li>\n",
    "<li>The optimal hyperparameter value can be missed</li>\n",
    "<li>Can overfit hyperparameters to the training set if too many combinations are tried out for too small a dataset</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc81b0-ca10-437f-a14b-5cea27540414",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09551683-bd8a-40ae-8b2d-8585d9d04e8a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Randomly explore hyperparameter values from:\n",
    "* A hyperparameter space to randomly sample from\n",
    "* The specified number of samples to be tested<br>\n",
    "<img src=\"https://drive.google.com/uc?id=156Oe5p4yOCYEfYujZgqvTSlJqxo-5nvR\" style=\"width:1200px;\">\n",
    "<a href=\"https://medium.com/@jackstalfort/hyperparameter-tuning-using-grid-search-and-random-search-f8750a464b35\">Stalfort, 2019</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1967ee-0bb9-4f2c-8371-b46ed9b29a68",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "# Instanciate model\n",
    "model = Ridge()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {'solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg'], 'alpha': stats.loguniform(0.01,1)}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "search = RandomizedSearchCV(model, grid, \n",
    "                            scoring='r2',\n",
    "                            n_iter=100,  # number of draws\n",
    "                            cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit data to Grid Search\n",
    "search.fit(X_train, y_train)\n",
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dfc1b5-012e-4054-855c-ca73f0fa20c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Using a probability distribution in RandomSearch\n",
    "Can be generated with <a href=\"https://docs.scipy.org/doc/scipy/reference/stats.html\">scipy.stats.distributions</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a0a26-9c55-47d5-8652-3bd30e2a1ae6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dist = stats.norm(10, 2) # if you have a best guess (say: 10)\n",
    "\n",
    "dist = stats.randint(1,100) # if you have no idea\n",
    "dist = stats.uniform(1, 100) # same\n",
    "\n",
    "dist = stats.loguniform(0.01, 1) # Coarse grain search\n",
    "\n",
    "r = dist.rvs(size=10000) # Random draws\n",
    "plt.hist(r);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac79048",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Limitations of GridSearch and RandomizedSearch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a08ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Both algorithms are not tracking the history of optimization üìú"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a47d65",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Choice of next set of parameter is random üé≤ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc38ac9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Evaluation of the loss function is costly üí∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7927de6-aa54-4b7d-8e0c-c4a5495a2a05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Suggested Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71112f-3cc2-4c7d-9bad-91ae2eef7490",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## üì∫ Videos \n",
    "#### Short videos from my Undegraduate Machine Learning Classes:\n",
    "* üìº <a href=\"https://youtu.be/8mNPHGmXS5Q?list=PLZzjCZ3QdgQCcRIwQdd-_cJNAUgiEBB_n\">Support Vector Machines</a>\n",
    "* üìº <a href=\"https://youtu.be/-ohZINc7OCY?list=PLZzjCZ3QdgQCcRIwQdd-_cJNAUgiEBB_n\">Introduction to Neural Networks</a>\n",
    "* üìº <a href=\"https://youtu.be/A1-HocOPXms?list=PLZzjCZ3QdgQCcRIwQdd-_cJNAUgiEBB_n\">Convolutional Neural Networks</a>\n",
    "* üìº <a href=\"https://youtu.be/aircAruvnKk\">But what is a neural network? | Chapter 1, Deep learning</a>, by 3Blue1Brown\n",
    "* üìº <a href=\"https://youtu.be/04L4ZHiJbjs\">TensorFlow & Keras Tutorial 2022 | Deep Learning With TensorFlow & Keras</a>, by Simplilearn (> 7 hours of video, full course!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78477b-ecbb-4d68-86fb-8c10a5f56404",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## üìö Further Reading \n",
    "* üìñ <a href=\"https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\">Support Vector Machine ‚Äî Introduction to Machine Learning Algorithms</a> by Rohith Gandhi, 2018\n",
    "* üìñ <a href=\"https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf\">Taking the Human Out of the Loop: A Review of Bayesian Optimization</a> by Shariari et al\n",
    "* üìñ <a href=\"https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\">A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning</a> by Will Koehrsen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d5453-25c2-4a2f-9625-13eccae3d805",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## üíªüêç Time to Code ! "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "rise": {
   "scroll": true,
   "theme": "serif",
   "transition": "none"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
