{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Day 4**: Deep-Dive into Data Modelling ü§ø (***live in 1.47***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center><h1 style=\"color:maroon\">A Deep Dive into Data Modelling</h1>\n",
    "    <img src=\"https://drive.google.com/uc?id=13LVZytG5rHMu0t9swAkqlbwKzzczGWmm\" style=\"width:1300px\">\n",
    "    <h3><span style=\"color: #045F5F\">Data Science & Machine Learning for Planet Earth Lecture Series</span></h3><h6><i> by C√©dric M. John <span style=\"size:6pts\">(2023)</span></i></h6></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Plan for today's Lecture üóì "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "* No Free Lunch Theorem\n",
    "* Dealing wiht Spatial and temporal covariance\n",
    "* Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "## Intended learning outcomes üë©‚Äçüéì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "* Use a learning curve to assess the training of your algorithm\n",
    "* Be able to choose the correct train-test split method\n",
    "* Handling non-stationarity for machine learning modelling\n",
    "* Be able to select the best features for your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Classification Dataset\n",
    "**<span style=\"color:teal\">Dataset for classification today:</span>** <a href=\"https://archive.ics.uci.edu/ml/datasets/wine\">UCI Wine Dataset</a><br>\n",
    "<img src=\"https://drive.google.com/uc?id=12wkCgxubcJY2oFy-RvljDCnIBBvrX3Wu\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "def split_and_scale(X, y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.7, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def prep_classification():\n",
    "    data = pd.read_csv('Lecture_data/wines_binary.csv')\n",
    "    \n",
    "    X=data.drop(columns=['is_good_quality'])\n",
    "    y=data.is_good_quality\n",
    "    \n",
    "    return split_and_scale(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# \"No Free Lunch\" Theorem\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=134yepoqE1oeugxz81m0Q7ZEa_-ZycQvu\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: Photo of a beautifully colored greek lunch with olives, mousaka and various other elements set on a white and blue checkered table cloth, sunny bright lighting.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#  Bias vs Variance \n",
    "<img src=\"https://drive.google.com/uc?id=13aE9Z_4rNb9KCckNrU81x6CtXODLJmee\" style=\"width:900px;\"><br>\n",
    "<a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\">Scott-Fortmann, 2012</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# The Bias / Variance tradeoff\n",
    "For a model to generalize there will be a tradeoff between **bias** and **variance**.\n",
    "<img src=\"https://drive.google.com/uc?id=12x_SMJXidtd-0-3-jpaPrAxGRrclCbyg\" style=\"width:1300px\"><br>\n",
    "<a href=\"https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\">Singh, 2018</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* **Bias (Underfitting)**: The inability for an algorithm to learn the patterns within a dataset.\n",
    "* **Variance (Overfitting)**: The algorithm generates an overly complex relationship when modelling patterns within a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## No Free Lunch Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Some models **oversimplify**, while others **overcomplicate** a relationship between features and target.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "It's up to us data scientists to make **assumptions** about the data and evaluate reasonable models accordingly.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**There is no one size fits all model**, this is known as the **No Free Lunch Theorem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## The Learning Curves\n",
    "We can use learning curves to diagnose three aspects of model behaviour on the dataset:\n",
    "* Underfitting\n",
    "* Overfitting\n",
    "* Whether the model has sufficient data to learn the patterns of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Concept\n",
    "Increasing the size of the training set can affect the training and validation scores.\n",
    "<img src=\"https://drive.google.com/uc?id=13dPV7OpIxKD5wP_L9FRx6vSDLw4c6YMO\" style=\"width:900px;\"><br>\n",
    "<p><a href=\"https://www.dataquest.io/blog/learning-curves-machine-learning/\">Olteanu, 2018</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Reading  the curves\n",
    "As the training size increases:\n",
    "* The training score will decrease\n",
    "* The testing score will increase\n",
    "* The curves typically (but not always!) demonstrate convergence\n",
    "<img src=\"https://drive.google.com/uc?id=12ykMUpomQ6sVZ1T-aw8xoDLh529RxW5i\" style=\"width:900px;\"><br>\n",
    "<p><a href=\"https://www.dataquest.io/blog/learning-curves-machine-learning/\">Olteanu, 2018</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Bulding a learning Curve\n",
    "\n",
    "Let's build a learning curve for our wine dataset. First, we create a categorical data and split our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_cat, y_test_cat = prep_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_score = []\n",
    "train_score = []\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,8))\n",
    "for nb_samples in range(5,3000):\n",
    "    Xi = X_train[:nb_samples]\n",
    "    yi = y_train_cat[:nb_samples]\n",
    "    lr = LogisticRegression().fit(Xi, yi)\n",
    "    y_pred = lr.predict(Xi)\n",
    "    y_test_pred = lr.predict(X_test)\n",
    "\n",
    "    train_score.append(accuracy_score(yi, y_pred))\n",
    "    val_score.append(accuracy_score(y_test_cat, y_test_pred))\n",
    "                     \n",
    "ax.plot(train_score, label='Training Score')\n",
    "ax.plot(val_score, label='Testing Score')\n",
    "ax.set_xlabel('Number of training samples', size = 14)\n",
    "ax.set_ylabel('Accuracy', size = 14)\n",
    "ax.legend();\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<h3 id=\"The-Bias-Variance-Tradeoff\">The Bias-Variance Tradeoff</a></h3><p>One of the most important concepts in Data science!</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    " Measuring the error on an unseen **Test set**:<br>\n",
    "<img src=\"https://drive.google.com/uc?id=134TQ0Cb72pBha1I6kqxab8Z61ITvCRb_\" style=\"width:1300px;\">\n",
    "$$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "üìö <a href=\"https://hastie.su.domains/ElemStatLearn/\">Hastie et al, 2009 (Elements of Statistical Learning)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Best model complexity is the one reducing the **Total Error** on a unseen dataset\n",
    "<img src=\"https://drive.google.com/uc?id=12yh6UD5cxqT16pIPbO6amTzfhUp8RI5u\" style=\"width:1300px;\">\n",
    "üìö <a href=\"https://hastie.su.domains/ElemStatLearn/\">Hastie et al, 2009 (Elements of Statistical Learning)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# We have learned to save a test set\n",
    "\n",
    "‚úÇÔ∏è On Monday, we learned to split the dataset to insure our trained machine learning model **extrapolates** well to unknown samples (i.e. we can test the results on the 'test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "‚≠ê Today, we will refine this notion by adding two important considerations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "1. We will explain the use of a **validation** vs a **test set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "2. We will see when **NOT** to split randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Train-Validation-Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* When we model data, we need to make many decisions (what model to use, what hyperparameter to choose, what features to keep or remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* If we use the **test set** to make these decision, we effectively have to way to assess the error of our final model (we **leak** statistic from the test set during our training process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* The right practice is to extract a *Validation Set* from our train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<p><img src=\"https://drive.google.com/uc?id=135MTNG_LIJpBlZIIn0GogOJ8IiBw9wGz\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Temporal and Spatial colinearity\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=134scR_-A91h8PMramnv-rZhlcJfrj7yL\" style=\"width:900px;\"><br>\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: An oil painting of a pocket watch in dramatic green lighting, digital art.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dataset\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=13edDxWekC4jbRAkZq5-iODykRRRZSO3g\" style=\"padding:10px;width:500px;\" align=\"left\"/>\n",
    "\n",
    "<span style=\"color:teal\">**Porosity Dataset:** </span> To demonstrate the concept of spatial colinearity, we will be playing with a geographic distribution of porosity. I borrowed this dataset from my colleague <a href=\"https://github.com/GeostatsGuy/PythonNumericalDemos\"> Prof. Michael Pyrcz on GitHub.</a> This is an ideal dataset for this as it contains data spread geographically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"Lecture_data/poro-perm.csv\")  \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Limiting ourselves to sand data\n",
    "\n",
    "To simplify this example, we will only look at the sandy lithologies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sand = data[data.Facies==1]\n",
    "shale = data[data.Facies==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = sand.X.values\n",
    "y = sand.Y.values\n",
    "phi = sand.Porosity.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Geospatial Colinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Let's plot the spatial variability of porosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(15,12))\n",
    "sc = ax.scatter(x, y, c=phi,s=20);ax.set_title('Measured Porosity', size=16)\n",
    "ax.set_xlabel('X Coordinates', size=14);ax.set_ylabel('Y Coordinates', size=14)\n",
    "plt.colorbar(sc);ax.set_xlim(0,1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Exploring relationship to distance\n",
    "\n",
    "Let's extract all datapoints from the previous graph that are between 400 and 600 units on the <code>Y</code> axis, and average their values by their position on <code>X</code>. We will also normalize <code>X</code> (distance) and <code>Porosity</code> for easier plotting on the next slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,12))\n",
    "sc = ax.scatter(x, y, c=phi,s=20);ax.set_title('Measured Porosity', size=16)\n",
    "ax.set_xlabel('X Coordinates', size=14);ax.set_ylabel('Y Coordinates', size=14)\n",
    "rect = patches.Rectangle((0, 400), 1000, 200, linewidth=0, edgecolor='r', facecolor='blue', alpha=.2)\n",
    "ax.plot([0,1000],[500,500],c='blue', linewidth=4, alpha=.5)\n",
    "ax.add_patch(rect); plt.colorbar(sc);ax.set_xlim(0,1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "corr_df = sand[(sand.Y>400) & (sand.Y<600)][['X','Porosity', 'Perm']]\n",
    "corr_df = corr_df.groupby('X').mean().reset_index()\n",
    "\n",
    "corr_df = pd.DataFrame(MinMaxScaler().fit_transform(corr_df).T, corr_df.columns).T\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Let's plot the relationship between <code>Porosity</code> and <code>distance</code> on the X axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ool = np.linspace(0,1,10)\n",
    "ax.scatter(corr_df.X, corr_df.Porosity);ax.plot(ool,ool,c='red', linewidth=6.0, alpha=.3);\n",
    "ax.set_xlabel(\"Normalized Distance\", size=16);ax.set_ylabel(\"Normalized Porosity\", size=16);\n",
    "ax.set_xlim(0,1); ax.set_ylim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "‚ùó There is clearly a correlation between geographic position and values of porosity! This means that we have **spatial colinearity** (porosity is more closely related to points that are nearer to each other than far away)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* üé≤ For data with spatial or temporal co-linearity, if we do a **random split** we will end up with data in our test set that is very similar (has strong autocorrelation) to data in our train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* üëé This breaks our assumption of independance of training and testing set: our assessment of the performance of our machine learning algorithm will be unrealistically high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Geographic train-test split\n",
    "\n",
    "Often, for spatial data, the solution is to split the training and testing set based on location. This is also applicable to 1D (e.g. cores, logs) and 3D (e.g. seismic, atmospheric volume, oceans, ...) datasets. Hence, rather than take the approach shown on the right below we take the approach taken on the left:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=13dsf6Gbo3x-48iwQxILbBXdflXc3Ps3m\" style=\"width:1500px\">\n",
    "<a href=\"https://en.wikipedia.org/wiki/Machine_learning_in_earth_sciences\">Wikipedia</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "üëÜ This only works if the data distribution in the training set is similar to the data distribution in the test set. In other words, if the data is stationary (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Is cross-validation possible for spatial/temporal data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=13PaANdafySdobAwVuFnh4za7T_GBb3PZ\" style=\"width:800\">\n",
    "<a href=\"https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/\">Brownlee, 2016</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Train-test split for Time Series Forecasting\n",
    "\n",
    "For time series, we simply split along the time axis: we use the older part of the time series as the training set, and the newer part as the test set for our trained algorithm:\n",
    "\n",
    "<img src=\"figures/time_series_split.png\" style=\"width:1500\">\n",
    "<a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881\">Roberts et al, 2016</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "üëÜ Again, this assumes that the time series has no major trends. Let's talk about **stationarity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Stationarity\n",
    "\n",
    "Stationarity is a key concept to model spatial data and to forecast time series:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=12zRFM_Xfa1riPLsKnduyxysxR-FWe2A-\">\n",
    "<a href=\"https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322\">Palachy, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "üëÜ So, is out data **stationary**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(20,6));ax.plot(corr_df.X, corr_df.Porosity);ax.set_xlim(0,1); ax.set_ylim(0,1);\n",
    "ax.set_xlabel(\"Normalized Distance\", size=16);ax.set_ylabel(\"Normalized Porosity\", size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Let's try to predict permeability from porosity\n",
    "\n",
    "And let's ignore stationarity for now (at our peril!). We will do a geographic train-test-split, keeping <span style=\"color:blue\">**30% of the data as a test set**</span>, and <span style=\"color:red\">**70% as our train set**</span>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(20,6));ax.plot(corr_df.X, corr_df.Porosity);ax.set_xlim(0,1); ax.set_ylim(0,1);\n",
    "ax.set_xlabel(\"Normalized Distance\", size=16);ax.set_ylabel(\"Normalized Porosity\", size=16);\n",
    "rect = patches.Rectangle((0, 0), .3, 1, linewidth=0, edgecolor='r', facecolor='blue', alpha=.2);ax.add_patch(rect);\n",
    "rect = patches.Rectangle((.3, 0), 1, 1, linewidth=0, edgecolor='r', facecolor='red', alpha=.2);ax.add_patch(rect);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Let's use a <code>KNNRegressor</code> for our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "X_test_poro = corr_df.loc[:4,'Porosity']; y_test_poro = corr_df.loc[:4,'Perm'];\n",
    "X_train_poro = corr_df.loc[5:,'Porosity']; y_train_poro = corr_df.loc[5:,'Perm'];\n",
    "reg = KNeighborsRegressor().fit(X_train_poro.values.reshape(-1,1), y_train_poro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Now let's estimate the error on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_test_poro.values.reshape(-1,1))\n",
    "print(f'Relative error is {mean_squared_error(y_test_poro,y_pred)*100:.0f}% !!!!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Let's plot our results to explore where the errors are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(20,6));ax.plot(corr_df.X, corr_df.Porosity);ax.set_xlim(0,1); ax.set_ylim(-0.2,1);\n",
    "ax.scatter(corr_df.loc[:4,'X'], y_pred, c='r', marker='s', s=120);\n",
    "ax.set_xlabel(\"Normalized Distance\", size=16);ax.set_ylabel(\"Normalized Porosity\", size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Detrending data (introducing stationarity)\n",
    "\n",
    "We can first remove trends in the data, for instance, using a simple linear regression with distance for our variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "trend_poro = LinearRegression().fit(corr_df[['X']],corr_df.Porosity)\n",
    "trend_perm = LinearRegression().fit(corr_df[['X']],corr_df.Perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detrend:\n",
    "corr_df['Poro_det'] = corr_df.Porosity - trend_poro.predict(corr_df[['X']])\n",
    "corr_df['Perm_det'] = corr_df.Perm - trend_perm.predict(corr_df[['X']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Let's inspect the detrended porosity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(20,6))\n",
    "ax.plot(corr_df.X, corr_df.Poro_det);ax.set_xlim(0,1); ax.set_ylim(-.5,.5);\n",
    "ax.set_xlabel(\"Normalized Distance\", size=16);ax.set_ylabel(\"Porosity Residuals\", size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "üëâ We can now model residuals without worrying about non-stationarity of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Modeling detrended data (residuals):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train-test split:\n",
    "X_test_poro = corr_df.loc[:4,'Poro_det']; y_test_poro = corr_df.loc[:4,'Perm_det'];\n",
    "X_train_poro = corr_df.loc[4:,'Poro_det']; y_train_poro = corr_df.loc[4:,'Perm_det'];\n",
    "\n",
    "# Model the residuals:\n",
    "reg = KNeighborsRegressor().fit(X_train_poro.values.reshape(-1,1), y_train_poro)\n",
    "\n",
    "# Print prediction error:\n",
    "y_pred_det = reg.predict(X_test_poro.values.reshape(-1,1))\n",
    "print(f'Relative error is {mean_squared_error(y_test_poro,y_pred_det)*100:.0f}% !!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "##### We can add the trend to the residuals to obtain the true value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trend_perm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_best = y_pred_det + trend_perm.coef_[0] * corr_df.loc[:4,'X']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(20,6));ax.plot(corr_df.X, corr_df.Porosity);ax.set_xlim(0,1); ax.set_ylim(-.2,1);\n",
    "ax.scatter(corr_df.loc[:4,'X'], y_pred, c='r', marker='s', s=120);\n",
    "ax.scatter(corr_df.loc[:4,'X'], y_pred_best, c='purple', marker='o', s=160);\n",
    "ax.set_xlabel(\"Normalized Distance\", size=16);ax.set_ylabel(\"Normalized Porosity\", size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Modeling time series:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "üëâ **Same principle** as for spatial data: stationarity is a prerequisit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "üëâ Time Series need to be **detrended** (usually using a moving average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "üëâ **Seasonality** (Short-term trends) also need to be removed to **model residuals**! This can be done with Python packages such as <code>statsmodel</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Reducing Data Complexity through Feature Selection\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=13Geb28USIRLq2sFmsHhU3l8ms8wxnS4F\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>A cute furry squirel with soft brown eyes uses a ruler to measure the size of the African continent, digital art</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Part of our Data Preparation Process\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<img align=\"center\" src=\"https://drive.google.com/uc?id=13UQX6gUFuqwGlBrTK8rTFySN8Dzg_LjQ\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Feature selection is the process of eliminating non-informative features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### The curse of dimensionality\n",
    "We need to observe enough data to support a meaningful relationship.\n",
    "<img align=\"center\" src=\"https://drive.google.com/uc?id=138tCcGljN4oICErIthNASNhZ9RhhMKUX\" width=\"800px\"/>\n",
    "<a href=\"https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/\">Source</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<img align=\"center\" src=\"https://drive.google.com/uc?id=13HaYr5DmSGaFJDUAzPICpcLawpT3Q0SK\" width=\"1200px\"/>\n",
    "As the number of features or dimensions grows, the amount of data we need to generalise accurately grows **exponentially** e.g $5^1$, $5^2$, $5^3$, $5^n$\n",
    "<a href=\"https://www.freecodecamp.org/news/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335/\">Source</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Feature correlation\n",
    "One selection technique is to remove one (or more) of  features that are highly correlated to each other.\n",
    "* High correlation = redundant information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### üñ• Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T14:49:22.932139Z",
     "start_time": "2021-10-06T14:49:22.281166Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_data = X_train.copy()\n",
    "corr_data['target'] = y_train_cat\n",
    "corr = corr_data.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T14:49:22.932139Z",
     "start_time": "2021-10-06T14:49:22.281166Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1, figsize=(16,14))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns,\n",
    "        cmap= \"seismic\",ax=ax, annot=corr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Feature Permutation \n",
    "Feature permutation is a second feature selection algorithm that evaluates the importance of each feature in predicting the target.\n",
    "* Trains and records the test score of a base model containing all features \n",
    "* Randomly shuffles (permutation) <span style=\"color:blue\">**one**</span> feature within the test set \n",
    "* Records new score on shuffled test set \n",
    "* Compares the new score to the original score \n",
    "* Repeat for each feature \n",
    "üëâ <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html\">Sklearn's <code>permutation_importance</code> documentation</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "üëâ If the score drops when a feature is shuffled, it is considered important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### üíª Feature permutation in Sklearn\n",
    "\n",
    "This time, let's see if turning this problem into a classification problem works better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T14:49:24.600605Z",
     "start_time": "2021-10-06T14:49:23.588103Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "model = LogisticRegression(max_iter=3000)\n",
    "\n",
    "model.fit(X_train, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T14:49:24.600605Z",
     "start_time": "2021-10-06T14:49:23.588103Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "permutation_score = permutation_importance(model, X_train, y_train_cat, n_repeats=10) # Perform Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T14:49:24.600605Z",
     "start_time": "2021-10-06T14:49:23.588103Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "importance_df = pd.DataFrame(np.vstack((X_train.columns,\n",
    "                                        permutation_score.importances_mean)).T) # Unstack results\n",
    "importance_df.columns=['feature','score decrease']\n",
    "\n",
    "importance_df.sort_values(by=\"score decrease\", ascending = False) # Order by importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T14:49:24.602424Z",
     "start_time": "2021-10-06T14:49:23.589Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "base_model = LogisticRegression(max_iter=3000)\n",
    "\n",
    "scores = cross_val_score(base_model, X_train, y_train_cat, cv=3)\n",
    "\n",
    "scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "## Model with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T14:49:24.602424Z",
     "start_time": "2021-10-06T14:49:23.589Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_small = X_train[[\"citric acid\",\"fixed acidity\", \"alcohol\"]] # Keep strong features\n",
    "\n",
    "final_model = LogisticRegression(max_iter=3000)\n",
    "\n",
    "scores = cross_val_score(final_model, X_small, y_train_cat, cv=3)\n",
    "\n",
    "scores.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Training curve for model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "val_score = []\n",
    "train_score = []\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,8))\n",
    "\n",
    "for nb_samples in range(5,3000):\n",
    "    Xi = X_train[:nb_samples]\n",
    "    yi = y_train_cat[:nb_samples]\n",
    "    lr = LogisticRegression().fit(Xi, yi)\n",
    "    y_pred = lr.predict(Xi)\n",
    "    y_test_pred = lr.predict(X_test)\n",
    "\n",
    "    train_score.append(accuracy_score(yi, y_pred))\n",
    "    val_score.append(accuracy_score(y_test_cat, y_test_pred))\n",
    "                     \n",
    "ax.plot(train_score, label='Training Score')\n",
    "ax.plot(val_score, label='Testing Score')\n",
    "ax.set_xlabel('Number of training samples', size = 14)\n",
    "ax.set_ylabel('Accuracy', size = 14)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Training curve for model with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "val_score_small = []\n",
    "train_score_small = []\n",
    "X_test_small = X_test[[\"citric acid\",\"fixed acidity\", \"alcohol\"]] \n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,8))\n",
    "\n",
    "for nb_samples in range(5,3000):\n",
    "    Xi = X_small[:nb_samples]\n",
    "    yi = y_train_cat[:nb_samples]\n",
    "    lr = LogisticRegression().fit(Xi, yi)\n",
    "    y_pred = lr.predict(Xi)\n",
    "    y_test_pred = lr.predict(X_test_small)\n",
    "\n",
    "    train_score_small.append(accuracy_score(yi, y_pred))\n",
    "    val_score_small.append(accuracy_score(y_test_cat, y_test_pred))\n",
    "                     \n",
    "ax.plot(train_score_small, label='Training Score')\n",
    "ax.plot(val_score_small, label='Testing Score')\n",
    "ax.set_xlabel('Number of training samples', size = 14)\n",
    "ax.set_ylabel('Accuracy', size = 14)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Comparing validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,8))                 \n",
    "ax.plot(val_score_small, label='Validation Score Best Features model')\n",
    "ax.plot(val_score, label='Validation Score Full Features model')\n",
    "ax.set_xlabel('Number of training samples', size = 14)\n",
    "ax.set_ylabel('Accuracy', size = 14)\n",
    "ax.set_xlim(-10,1200)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Reducing Complexity\n",
    "The most simple solution is normally the best solutionüî™ <br>\n",
    "Reducing the number of features makes the model:\n",
    "* More interpretable \n",
    "* Faster to train \n",
    "* Requires less samples (**4-5 times less in our example**)\n",
    "* Easier to implement and maintain in production "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Suggested Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## üì∫ Videos \n",
    "* üìº <a href=\"https://youtu.be/QGjdS1igq78\">Machine-learning based modelling of spatial and spatio-temporal data (introduction)</a>, Lecture by Hanna Meyer, Prague Summer School, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## üìö Further Reading \n",
    "* üìñ <a href=\"https://kevinkotze.github.io/ts-6-unit-roots/\">Non-stationarity</a> by Kevin Kotz√© (a bit in depth but a lot of good references\n",
    "* üìñ <a href=\"https://www.analyticsvidhya.com/blog/2021/03/introducing-machine-learning-for-spatial-data-analysis/#:~:text=Machine%20Learning%20for%20spatial%20data%20analysis%20builds%20a%20model%20to,the%20spatial%20attribute%20into%20account.\">Introducing Machine Learning for Spatial Data Analysis</a> by Rendik, 2021\n",
    "* üìñ <a href=\"https://link.springer.com/book/10.1007/0-387-35429-8\">Statistical Analysis of Environmental Space-Time Processes (Book)</a> by Le and Zidek, 2006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## üíªüêç Time to Code ! "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "rise": {
   "scroll": true,
   "theme": "serif",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
