{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6945d790",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-hPP-XPm9_5M3orUgmompcVleQ5xvPST\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db555c",
   "metadata": {},
   "source": [
    "# Introducing the `XGBoost` Library\n",
    "\n",
    "We will continue working on our Earthquake damage predictions, but this time, we will use a different approach: `Boosted Trees` with `XGBoost`.\n",
    "\n",
    "`XGBoost` (this stands for `eXtreme Gradient BOOSTing`), and it is one of the most popular machine learning library. Many `Kaggle` competitions are won using `XGBoost`. This library is dedicated to `ensemble learning` with decision trees, and it is very complete and has many options. It can handle `RandomForest` classification, but also `BoostedTrees` by performing `AdaBoost` on a `RandomForest` classifier. \n",
    "\n",
    "I highly recommend you <a href=\"https://xgboost.readthedocs.io/en/stable/\">use the very complete the documentation</a> of `XGBoost` if you are serious about applying machine learning to tabular data: it is one of the most powerful family of algorithms at the moment.\n",
    "\n",
    "Part of the success of `XGBoost` is that it leverages principles from `Deep-learning` and applies the approach to `DecisionTrees`: for instance, you can control the `learning rate` of the `boosted trees` in `XGBoost`, something you are familiar with through your use of the `SGDClassifier` and `SGDRegressor` classes, and our lecture on Wednesday of week 1.\n",
    "\n",
    "Here, we will see that `XGBoost` will outperform all of the classifiers we developped in our previous notebook.\n",
    "\n",
    "\n",
    "# Opening the data\n",
    "\n",
    "As in the previous exercise, open the data in `earthquake_nepal.csv`,  separate the target (`damage_grade`) and the features, and do a train_test_split with 80% in the `train_set` (use a `random_state=42` to be consistent with my results). <br>\n",
    "Convert your `y_train` and `y_test` to categorical data using a `label_encoder`.\n",
    "\n",
    "**Note**: this time, I am giving you more data. In fact, one order of magnitude more data! But you will see that `XGBoost` can handle this relatively easily, and this will contribute to boosting our performance. We should achieve an **accuracy > 70%**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31206c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1q2id-UaRkjFRm_bCefaAPXONI5hB15mK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f84b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a828f8",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2427952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('data_preproc',\n",
    "                         X_shape = X_train.shape,\n",
    "                         y_values = pd.DataFrame(y_train).value_counts().shape[0],\n",
    "                         y_type = type(pd.DataFrame(y_train).value_counts().index[0][0])\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8de1c0",
   "metadata": {},
   "source": [
    "# Open the saved `preproc` pipeline\n",
    "\n",
    "Now, using `joblib`, load your saved `preproc` pipeline, save it in a variable named `preproc`, and transform your `X_train` and `X_test` with this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bde4f7",
   "metadata": {},
   "source": [
    "# Create a validation set\n",
    "\n",
    "First of all, as we would do when training neural networks, `XGBoost` can use a `validation_set` to track the performance of our algorithm over time. So let's create an `X_val` and `y_val` by further splitting our `X_train` and `y_train` into an 80%/20% `X_train` `y_train` / `X_val` `y_val` (don't forget to save the new `X_train` and `y_train` with the same names, or your `X_val` and `y_val` sets will be also present in your `X_train` `y_train` and thus you will have a data leak):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcde1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8cebe9",
   "metadata": {},
   "source": [
    "# XGBoost: A gentle Introduction\n",
    "\n",
    "Let's get acquainted with the basic use of the `XGBClassifier` class. Import it from the `xgboost` library, and run train a plain-vanilla version of the classifier. Save the accuracy score into a variable called `xgb_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d3101",
   "metadata": {},
   "source": [
    "<details><summary>üîç Observations</summary><br>\n",
    "    Right off the bat, the <code>XGBoost</code> classifier performs as strongly as our strongest previous classifier!</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d306188",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a02f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('base_accuracy',\n",
    "                         score = xgb_accuracy\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7c482",
   "metadata": {},
   "source": [
    "### `XGBoost` hyperparameters\n",
    "\n",
    "Now, let's explore some of the basic hyperparameters of `XGBoost`. We will focus here on the ones that are particular to the algorithm, and not the ones similar to `DecisionTrees`. The hyperparameters we will play with include:\n",
    "\n",
    "* `n_estimators`: the number of boosting round we want our algorithm to go through.\n",
    "* `learning_rate`: the learning rate for the gradient descent algorithm\n",
    "* `early_stopping_rounds`: determines after how many boosting rounds training will stop if no improvement is detected\n",
    "\n",
    "To be able to use the `early_stopping_rounds` we need to pass both our training set (the `X_train` and `y_train`) and our evaluation set (the `X_val` and `y_val`) as a `eval_set` argument of our `fit()` function.\n",
    "‚ö†Ô∏è This needs to be passed as a **list of (X, y) tupples**. \n",
    "\n",
    "We also need to set the `eval_metric` to reflect a valid cost function for classification (`\"accuracy\"` cannot be a cost function) when we create our `XGBClassifier`.\n",
    "\n",
    "To get a feel of how `XGBoost` works with a validation set, go ahead and  create a new `XGBClassifier` with `n_estimators=100`, a `eval_metric` of `\"mlogloss\"` (multinomial log-loss), a `learning_rate=1.8`,  and pass it your `train` and `validation` sets during fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39388f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef200f48",
   "metadata": {},
   "source": [
    "## ‚òùÔ∏è Training performance at each iteration: learning curve\n",
    "\n",
    "As you can see, `XGBoost` is outputing a log of the `losses` both for your training (`validation_0` if you passed `(X_train, y_train)` first in your `eval_set`) and your validation set (`validation_1` if you passed `(X_train, y_train)` first in your `eval_set`).\n",
    "\n",
    "If you remember last week's lecture on training curves (and I hope you do!) you will recognise the value of this data: we can plot a learning curve, and see what our algorthm is doing.\n",
    "\n",
    "**Do the following:**\n",
    "* See what the `evals_result()` function returns when applied to your trained `XGBClassifier` (*tip*: it returns the training loop losses for both of your validation sets). Save this into a variable called `results`\n",
    "* Explore `results`: it is a dictionary-like objects, so see what the keys are, and what the keys of the returned object are.\n",
    "* Once you understand your `results`, write a simple python function that will draw both training curves on a plot. I suggest using a `figsize` of 15, 10, or something similar to show the curve nicely.\n",
    "* Draw the learning curve for the `XGBClassifier` that you trained above\n",
    "\n",
    "Then, based on your observations, answer the following question:\n",
    "* Does the algorithm `'underfit'`, `'overfit'`, or is it `'balanced'`?\n",
    "\n",
    "Save your answer as a string in a variable named `performance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df779c",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4caf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('fitting',\n",
    "                         performance = performance\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834bdcf",
   "metadata": {},
   "source": [
    "## Changing Hyperparameters\n",
    "\n",
    "Now, let's change the hyperparameters of the `XGBClassifier` to:\n",
    "* `eval_metric='mlogloss'` \n",
    "* `n_estimators=150` \n",
    "* `learning_rate=0.3`\n",
    "\n",
    "Retrain your model, and draw the learning curve. What do you think your model is doing now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073aa71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4623b324",
   "metadata": {},
   "source": [
    "<details><summary><strong>üí° Observations and strategy</strong></summary><br>\n",
    "    You should see now that the training is much smoother, and that your validation set is showing some steady decrease.</details>\n",
    "    \n",
    "## Controlling overfitting\n",
    "\n",
    "Now, let's control overfitting by tweaking two hyperparameters:\n",
    "* Create a new `XGBClassifier` with the exact same hyperparameters as above\n",
    "* set the new `min_child_weight` parameter to `6`\n",
    "* set the new `max_depth` parameter to `7`\n",
    "* retrain the model, and check the learning curve.\n",
    "* Calculate an `accuracy_score` and save it under a variable named `final_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d01fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE -- You can create new markdown and code cells\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4831a740",
   "metadata": {},
   "source": [
    "<details><summary><strong>üí° Observations and strategy</strong></summary><br>\n",
    "    We are improving a little bit on our model, but not very much. Feel free to do a grid search or other hyperparameter search if you want to.</details>\n",
    "    \n",
    "# How did I come up with these hyperparameters?\n",
    "\n",
    "So, how did I come up with these sets of `hyperparameters` to improve on our model? Simply put, I used `hyperparameter search`, in particular the `BayesCV` from the <a href=\"https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html\">scikit-optimize</a> library.\n",
    "\n",
    "But this takes a lot of time, so rather than have you do it in this notebook, I did it for you. But how do you start optimizing a complex algorithm like `XGBoost`? This is not easy, because there are many hyperparameters to tweak.\n",
    "\n",
    "A good way to start is by <a href=\"https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\">following this article that explains how to do hyperparameter tuning</a> for `XGBoost`: there is a clear guidelines on which ones to optimize first, and which ones to do only later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9281391",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0cb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('final_score',\n",
    "                         score = final_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c13c73",
   "metadata": {},
   "source": [
    "# üèÅ Finished!\n",
    "\n",
    "Well done! <span style=\"color:teal\">**Push your exercise to GitHub**</span>, and move on to the next one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
