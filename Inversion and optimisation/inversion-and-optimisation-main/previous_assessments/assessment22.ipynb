{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49feb4f4",
   "metadata": {},
   "source": [
    "# This was the assessment for Inversion and Optimisation in 2022 <a class=\"tocSkip\"></a>\n",
    "It covers the material correspoinding to **this** (2023) year's lectures 1-5, and 9-11. Note that in 2022 this assessment also covered lecture 12 (data assimilation), which will be unassessed this year, in a separate part not included here, so this year the part corresponding to lecture 1-5 and 9-11 may be just slightly longer. Separately, there is also an assessment based on lectures 6-8 in week 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645580a",
   "metadata": {},
   "source": [
    "## Section A - Row Echelon Form, Nullspace and Minimum Norm Solution\n",
    "Consider the following matrix\n",
    "\n",
    "$$\n",
    "\\underline{\\mathbf A} =\n",
    "\\begin{pmatrix}\n",
    "  2  & -1 & 0 & 0 & 0 \\\\\n",
    "  -4 & 3 & 2 & 0 & 0 \\\\\n",
    "  7 & -4 & 1 & 2 & 4 \\\\\n",
    "  5 & -3 & -2 & -1 & -2 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "* Work out its Row Echelon Form\n",
    "* Using this result, determine the rank of the matrix and the dimension of its nullspace. Which of the following terms applies to this matrix (or a linear system of equations based on it)? Give all terms that apply and explain why.\n",
    "  - under-determined\n",
    "  - equi-determined\n",
    "  - over-determined\n",
    "  - mixed-determined\n",
    "  - full-rank\n",
    "  - singular\n",
    "  - rank-deficient\n",
    "* Give a (linearly-independent) basis for the nullspace of the matrix\n",
    "* You are given the exact solution $\\boldsymbol{x}=(0,0,0,0,31)$ for the linear systems $\\underline{\\mathbf A}\\boldsymbol{x} = \\boldsymbol{b}$ with right-hand side vector $\\boldsymbol{b}=(0,  0, 124, -62)$. Using the nullspace vectors, find the minimum norm (norm of $\\boldsymbol{x}$) solution to the same equation (with the same right-hand side). Use a different method to find the same minimum-norm solution for this case (you may use any scipy routine for this) and check that the answer is the same. Explain why this method also provides the same answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7aab6",
   "metadata": {},
   "source": [
    "## Section B - Krylov Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e830d6b",
   "metadata": {},
   "source": [
    "Consider the linear system $\\underline{\\mathbf A}{\\bf x}={\\bf b}$ based on the matrix\n",
    "\n",
    "$$\n",
    "\\underline{\\mathbf A} = \\begin{pmatrix}\n",
    "2 & 0 & 1 \\\\ 0 & 4 & 0 \\\\ 1 & 0 &2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "* Using an initial guess of ${\\bf x_0} = (0,0,0)$, give a (linearly independent) basis for the Krylov subspaces $\\mathcal{D}_0, \\mathcal{D}_1,$ and $\\mathcal{D}_2$ for the following cases:\n",
    "   - a right-hand side vector ${\\bf b} = (1, 0, 0)$\n",
    "   - a right-hand side vector ${\\bf b} = (1, 1, 0)$\n",
    "   - a right-hand side vector ${\\bf b} = (0, 1, 0)$\n",
    "* Based on the previous answer, predict how many iterations the Conjugate Gradient algorithm and the GMRES algorithm will take to solve the system $\\underline{\\mathbf A}{\\bf x}={\\bf b}$ with initial guess ${\\bf x_0} = (0,0,0)$ for each of the cases.\n",
    "* For the case ${\\bf b} = (1, 0, 0)$ compute what the iterative approximations ${\\bf x}^{i}$ are for $i=1, \\dots, n$, where $n$ is the number of iterations you have predicted in the previous question, using the Conjugate Gradient Algorithm. Work these out yourself (in code or by hand), do not use an existing CG implementation here.\n",
    "* For the case ${\\bf b} = (1, 0, 0)$ compute what the iterative approximations ${\\bf x}^{i}$ are for $i=1, \\dots, n$, where $n$ is the number of iterations you have predicted, using the GMRES Algorithm. Work these out yourself (in code or by hand), do not use an existing GMRES implementation here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afb607",
   "metadata": {},
   "source": [
    "## Section C - Nonlinear Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57621f",
   "metadata": {},
   "source": [
    "In the lectures we have seen the Steepest or Gradient Descent algorithm for minimisation of a quadratic function $f$:\n",
    "\n",
    "$$\n",
    "  {\\bf x}^{(i+1)} = {\\bf x}^{(i)} - \\alpha^{(i)} f'({\\bf x}^{(i)})\n",
    "$$\n",
    "\n",
    "where $\\alpha$ controls the step size, and we made the choice:\n",
    "\n",
    "$$\n",
    "  \\alpha^{(i)} = \\frac{{\\bf r}^{(i)}\\cdot {\\bf r}^{(i)}}\n",
    "  {{\\bf r}^{(i)} \\cdot\\underline{\\mathbf A} {\\bf r}^{(i)}}\n",
    "$$\n",
    "\n",
    "with $\\underline{\\mathbf A}$ being the (constant) Hessian matrix of $f$, and ${\\bf r}^{(i)}=-f'({\\bf x}^{(i)})$.\n",
    "\n",
    "For the minimisation of more general, nonlinear functions $f$ we need to consider a different formula for $\\alpha$. One option is the Barzilai-Borwein formula:\n",
    "\n",
    "$$\n",
    "  \\alpha^{(i)} = \\frac{| \\left({\\bf x}^{(i)} - {\\bf x}^{(i-1)}\\right)\\cdot \\left(f'({\\bf x}^{(i)}) - f'({\\bf x}^{(i-1)})\\right)|}\n",
    "  {\\|f'({\\bf x}^{(i)}) - f'({\\bf x}^{(i-1)})\\|^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce915a0",
   "metadata": {},
   "source": [
    "Implement the Steepest Descent method with this choice for the step size and test it on the so called Rosenbrock function\n",
    "\n",
    "$$\n",
    "  f(x,y) = 100 (y-x^2)^2 + (1-x)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c5362",
   "metadata": {},
   "source": [
    "Plot the convergence trajectory for a number of different initial guesses. Describe and try to explain what you observe. Compare the convergence with that of Newton's method (for this you may use any of the code in the lecture notes), no line search or trust region method is needed.\n",
    "\n",
    "Note that the Barzilai-Borwein formula depends on the last two iterations. In the very first iteration you can just use a fixed value of $\\alpha$ instead, say $\\alpha^{(0)}=0.01$.\n",
    "\n",
    "Hint: for the Rosenbrock function itself, and any of its derivatives you can use the implementation in scipy.optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as sop\n",
    "xy = [0,1]\n",
    "print(\"f(x, y) =\", sop.rosen(xy))\n",
    "print(\"f'(x, y) =\", sop.rosen_der(xy))\n",
    "print(\"f''(x, y) =\", sop.rosen_hess(xy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d2882",
   "metadata": {},
   "source": [
    "## Section D - Image Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3c5e3",
   "metadata": {},
   "source": [
    "In lecture 3 we saw how a discrete Laplace operator can be used to smoothen/blur an image. We solve the following linear system:\n",
    "\n",
    "$$\n",
    "  \\left[\\underline{\\mathbf I} + m \\underline{\\mathbf A}\\right]\n",
    "  \\boldsymbol{u}_{\\text{smooth}} = \\boldsymbol{u}_{\\text{orig}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efc19e",
   "metadata": {},
   "source": [
    "where $\\boldsymbol{u}_{\\text{orig}}$ is the original image and $\\boldsymbol{u}_{\\text{smooth}}$ the smoothed image we solve for. $\\underline{\\mathbf I}$ is the identity matrix, $\\underline{\\mathbf A}$ is the discrete Laplace operator, and $m$ is a positive constant. The images $\\boldsymbol{u}_{\\text{orig}}$ and $\\boldsymbol{u}_{\\text{smooth}}$ are stored as flattened vectors, where an image of $N_y\\times N_x$ is stored as single vector of length $n=N_yN_x$.\n",
    "\n",
    "To read in the original image, you may use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d94cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# reads in an image of Ny x Nx x 4\n",
    "# where the last dimension represents three RGB colour channels and an alpha channel\n",
    "img = plt.imread('london_road.png')\n",
    "\n",
    "# convert to black and white, by averaging over the RGB channels (dropping the alpha channel)\n",
    "img_bw = img[:,:,:3].sum(axis=-1)/3\n",
    "\n",
    "# flatten into vector of length Nx*Ny\n",
    "u_orig = img_bw.flatten()\n",
    "\n",
    "print(img_bw.shape, u_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24ae8b",
   "metadata": {},
   "source": [
    "which we can display using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2247212",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(17,10))\n",
    "ax.imshow(u_orig.reshape(img_bw.shape), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf06c5",
   "metadata": {},
   "source": [
    "The discrete Laplace operator matrix can be obtained from the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44703034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def Laplace(Ny, Nx):\n",
    "    \"\"\" Assembles a discrete Laplace operator with Neumann boundary conditions\n",
    "    into a sparse matrix.\n",
    "    \"\"\"\n",
    "    # construct matrix from 5 (off-)diagonals\n",
    "    # we provide the diagonals as one 5 x n array\n",
    "    # the actual off-diagonals should of course be shorter\n",
    "    # but dia_matrix cuts them off for us\n",
    "    n = Nx*Ny\n",
    "    offsets = [-Nx, -1, 0, 1, Nx]\n",
    "    diags = -np.ones((5,n))\n",
    "    diags[2] = -diags[2]*4  # main diagonal should be positive and 4 times the off-diagonals\n",
    "    A = sp.dia_matrix((diags, offsets), shape=(n,n)).tocsr()\n",
    "    \n",
    "    # grid point in the right-most column, should not be connected to\n",
    "    # the grid point in the first column on the next row\n",
    "    for i in range(1,Ny):\n",
    "        A[i*Nx-1, i*Nx] = 0\n",
    "        A[i*Nx, i*Nx-1] = 0\n",
    "        \n",
    "    # for homogenous Neumann boundary conditions all we have to do is\n",
    "    # make sure that the diagonal is set such that the row sum is zero\n",
    "    # This replaces some of the 4 values on the diagonal with the actual number\n",
    "    # of connected grid points on the boundary:\n",
    "    A.setdiag(A.diagonal() - np.array(A.sum(axis=1)).flatten())\n",
    "    \n",
    "    return A\n",
    "\n",
    "A = Laplace(img_bw.shape[0], img_bw.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ff5705",
   "metadata": {},
   "source": [
    "* What are the properties of the matrix $\\left[\\underline{\\mathbf I} + m \\underline{\\mathbf A}\\right]$? Based on your answer choose an appropriate iterative solver and produce a smoothened image with $m=10$ using a solver from https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527c547",
   "metadata": {},
   "source": [
    "We would now like to consider a spatially varying smoothing filter, where the amount of smoothing in each grid point is controlled by the entries in _vector_ $\\bf m$ of length $n=NyNx$. We can do this by solving\n",
    "$$\n",
    "  \\left[\\underline{\\mathbf I} + \\operatorname{diag}(\\bf m) \\underline{\\mathbf A}\\right]\n",
    "  \\boldsymbol{u}_{\\text{smooth}} = \\boldsymbol{u}_{\\text{orig}}\n",
    "$$\n",
    "where $\\operatorname{diag}(\\bf m)$ is the diagonal matrix with the entries of $\\bf m$ on its main diagonal. Its role is to scale each row of $\\underline{\\mathbf A}$ with the corresponding value of $\\bf m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cc3abf",
   "metadata": {},
   "source": [
    "An example for a spatially varying $\\bf m$ is set up in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ed4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up x and y coordinate vectors for the grid\n",
    "Ny, Nx = img_bw.shape\n",
    "x, y = np.meshgrid(np.linspace(0,1,Nx), np.linspace(0,1,Ny))\n",
    "x = x.flatten()\n",
    "y = y.flatten()\n",
    "\n",
    "# blur everywhere except for an area around (0.25,0.6)\n",
    "m = (1-np.exp(-((y-0.6)**4 + (x-0.25)**4)*500))*10\n",
    "\n",
    "# plot the values of the m vector\n",
    "plt.imshow(m.reshape(img_bw.shape))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb1786",
   "metadata": {},
   "source": [
    "* Assemble the matrix $\\left[\\underline{\\mathbf I} + \\operatorname{diag}(\\bf m) \\underline{\\mathbf A}\\right]$ (hint you may find [scipy.sparse.diags](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.diags.html) useful) and investigate its properties. Solve the linear system using an iterative solver from scipy.sparse.linalg and show the resulting image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897b9f84",
   "metadata": {},
   "source": [
    "In the file 'london_road_tilt.png' you'll find the result of the same spatially varying smoothing process but based on an unknown vector $\\bf m$. We would like to find out what vector $\\bf m$ has been used to produce that image. We can do this by formulating the following PDE-constrained optimisation problem:\n",
    "\n",
    "$$\n",
    "  \\text{minimize} f({\\bf u}, {\\bf m})\\;\\;\n",
    "  \\text{subject to }g({\\bf u}, {\\bf m}) = 0\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "  f(\\bf u, \\bf m) = \\| \\bf u - \\bf{u_{\\text target}} \\|^2\n",
    "$$\n",
    "\n",
    "is the functional that measures the difference between the image 'london_road_tilt.png' stored as the vector $\\bf{u_{\\text target}}$ and an image $\\bf u$ that satisfies the PDE constraint\n",
    "\n",
    "$$\n",
    "  g(\\bf u, \\bf m) \\equiv \\left[\\underline{\\mathbf I} + \\operatorname{diag}(\\bf m) \\underline{\\mathbf A}\\right] {\\bf u} - \\bf{u_{\\text orig}} = \\bf 0\n",
    "$$\n",
    "\n",
    "where $\\bf u_{\\text orig}$ represents the original image 'london_road.png'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd98ebc",
   "metadata": {},
   "source": [
    "* Implement the reduced functional $\\hat f({\\bf m})$ associated with this PDE-constrained optimisation problem and implement its derivative\n",
    "$$\n",
    "  \\frac{\\partial \\hat f({\\bf m})}{\\partial {\\bf m}}\n",
    "$$\n",
    "Make sure you appropriately test this derivative!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a003a9",
   "metadata": {},
   "source": [
    "**Hint:** one of the derivatives that you might need is given by\n",
    "    \n",
    "$$\n",
    "  \\frac{\\partial g({\\bf u}, {\\bf m})}{\\partial{\\bf m}}\n",
    "  =\\operatorname{diag}(\\underline{\\mathbf A}{\\bf u})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ae01c",
   "metadata": {},
   "source": [
    "* Using only the reduced functional $\\hat f({\\bf m})$ and its derivative, describe an optimisation algorithm that we might use to solve the PDE constrained optimisation problem and motivate your choice. You do not need to perform this optimisation here! In trying out such an algorithm we find poor convergence and very noisy solutions; Describe why this might be the case and what we could do to improve the situation. We also find a number of other images that have had the same smoothing applied, based on the same unknown vector $\\bf m$. Describe how we might use these to improve the accuracy of the inversion problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
