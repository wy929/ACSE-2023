{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ea8187d",
   "metadata": {},
   "source": [
    "# Question 2 <a class=\"tocSkip\"></a>\n",
    "Consider the linear system of equations\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "  2 & 1 & 1 & 1 \\\\\n",
    "  1 & 2 & 1 & 1 \\\\\n",
    "  1 & 1 & 3 & 1 \\\\\n",
    "  1 & 1 & 1 & 3 \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "  x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "  1 \\\\ 1 \\\\ 0 \\\\ 0\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4f25f",
   "metadata": {},
   "source": [
    "**2.1** Using an initial guess of ${\\bf x}^0 = (0,0,0,0)$, compute the solution using the Conjugate Gradient algorithm. You may use any code from the lecture notes, but don't use any external implementation like `scipy`'s. You do not need to do any calculations by hand.You do need to show (or print) **every** iteration vector ${\\bf x}^i$ in your answer. How many iterations were required to reach the exact solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a68e69",
   "metadata": {},
   "source": [
    "## Solution <a class=\"tocSkip\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdabe8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99afa5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2,1,1,1],[1,2,1,1],[1,1,3,1],[1,1,1,3]])\n",
    "b = np.array([1,1,0,0])\n",
    "x0 = np.array([0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e166f3",
   "metadata": {},
   "source": [
    "In the first iteration of the Conjugate Gradient method, we compute the initial residual, and alpha to arrive at the minimum along the line spanned by that initial residual vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691e7a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0.33333333, 0.33333333, 0.        , 0.        ])\n"
     ]
    }
   ],
   "source": [
    "r0 = b - A @ x0\n",
    "alpha0 = np.dot(r0, r0) / np.dot(r0, A @ r0)\n",
    "x1 = x0 + alpha0 * r0\n",
    "pprint(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109cf4e4",
   "metadata": {},
   "source": [
    "We compute the residual again, but in the second iteration we need a direction ${\\bf p}^{(1)}$ that's $A$-orthogonal to ${\\bf r}^{(0)}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a5e9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = b - A @ x1\n",
    "beta = np.dot(r1, r1)/np.dot(r0, r0)\n",
    "# or alternatively beta = -np.dot(r0, A @ r1)/np.dot(r0, A @ r0)\n",
    "p1 = r1 + beta * r0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3495146",
   "metadata": {},
   "source": [
    "And again we aim to arrive in the minimum along the line spanned by ${\\bf p}^{(1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "328eac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 0.5 ,  0.5 , -0.25, -0.25])\n"
     ]
    }
   ],
   "source": [
    "alpha1 = np.dot(p1, r1) / np.dot(p1, A @ p1)\n",
    "# or alternatively: alpha1 = np.dot(r1, r1) / np.dot(p1, A @ p1)\n",
    "x2 = x1 + alpha1 * p1\n",
    "pprint(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649ce74",
   "metadata": {},
   "source": [
    "Now if we compute residual again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc1b53b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "r2 = b - A @ x2\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728508e",
   "metadata": {},
   "source": [
    "we have obtained a zero residual, which means that we have exactly solved $\\underline{\\bf A}{\\bf x} = {\\bf b}$ with \n",
    "$${\\bf x}={\\bf x}^{(2)} = \\begin{pmatrix} \\tfrac 12 \\\\ \\tfrac 12 \\\\ -\\tfrac 14 \\\\ -\\tfrac 14 \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70edc7b",
   "metadata": {},
   "source": [
    "in **two** iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb517d",
   "metadata": {},
   "source": [
    "### General feedback <a class=\"tocSkip\"></a>\n",
    "A number of students have simply taken the full conjugate gradient algorithm and put in a number of print statements. This is absolutely fine, but you do then have to explain what you are observing. So if you run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b530f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(A, b, x0):\n",
    "    r0 = b - A @ x0\n",
    "    p = r0\n",
    "    x = x0\n",
    "    for i in range(A.shape[0]):\n",
    "        print(f\"x_{i}: {x}\")\n",
    "        print(f\"r_{i}: {r0}\")\n",
    "        Ap = A @ p\n",
    "        alpha = np.dot(r0, r0) / np.dot(p, Ap)\n",
    "        x = x + alpha * p\n",
    "        r1 = r0 - alpha * Ap\n",
    "        beta = np.dot(r1, r1)/np.dot(r0, r0)\n",
    "        p = r1 + beta * p\n",
    "        r0 = r1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0de12ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_0: [0 0 0 0]\n",
      "r_0: [1 1 0 0]\n",
      "x_1: [0.33333333 0.33333333 0.         0.        ]\n",
      "r_1: [ 0.          0.         -0.66666667 -0.66666667]\n",
      "x_2: [ 0.5   0.5  -0.25 -0.25]\n",
      "r_2: [0. 0. 0. 0.]\n",
      "x_3: [nan nan nan nan]\n",
      "r_3: [nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147537/1102709967.py:9: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  alpha = np.dot(r0, r0) / np.dot(p, Ap)\n"
     ]
    }
   ],
   "source": [
    "conjugate_gradient(A, b, x0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2872c9",
   "metadata": {},
   "source": [
    "you need to explain where the `nan`s are coming from: they arise in the calculation of $\\alpha$ as $r$ and thus $p$ become zero, also the denominator becomes zero. Also you need to explain how you know that the exact solution is arrived at in the second iteration, namely by observing that the residual vector is exactly zero at that point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3aa63",
   "metadata": {},
   "source": [
    "If you use an implementation of the conjugate gradient algorithm with a stopping criterion, since we are asked for an exact solution and not an approximate answer within some tolerance, you need to at least check that the answer exactly satisfies the equation, i.e. that the residual is exactly zero. Note that even if you use something like `np.allclose` there is a default absolute tolerance `atol` (with a value of `1e-8`) that you are tolerating, so again you are not guaranteed to actually reach the exact solution. Of course in practice, in particular for larger and ill-conditioned systems, we may never be able to reach the solution exactly due to round off errors, but that's not an issue for the simple system here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045678c7",
   "metadata": {},
   "source": [
    "**2.2** What conclusion can you draw about the Krylov subspaces from the number of iterations that were required to reach the exact solution in the previous question? Work out the Krylov subspaces for this problem with the given initial guess ${\\bf x}^0=(0,0,0,0)$ by giving a _linearly independent_ basis for each subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33af7ee7",
   "metadata": {},
   "source": [
    "## Solution <a class=\"tocSkip\"></a>\n",
    "As a general note, make sure you always answer each part/sub-question of every question. The first sub-question here gives you an immediate hint about the rest of the question: the conclusion you can draw from the fact that only two iterations are required in the Conjugate Gradient algorithm to reach the exact solution, is that\n",
    "the solution ${\\bf x}^{\\ast}= {\\bf x}^{(2)}$ must be in the second Krylov subspace $\\mathcal{D}_1$:\n",
    "\n",
    "$$\n",
    "  {\\bf x}^{\\ast} - {\\bf x}^{(0)} \\in \\mathcal{D}_1\n",
    "$$\n",
    "\n",
    "and that the Krylov subspace stops growing after iteration two, i.e. there only two Krylov subspaces, $\\mathcal{D}_0$ and $\\mathcal{D}_1$, and the last one has the maximum dimension of two, with all further subspaces being the same $\\mathcal{D}_1 = \\mathcal{D}_2 = \\mathcal{D}_3$.\n",
    "\n",
    "The first two subspaces can be calculated by computing the following vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37355f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'r0 = [1 1 0 0]'\n",
      "'A @ r0 = [3 3 2 2]'\n",
      "'A @ A @ r0 = [13 13 14 14]'\n",
      "'A @ A @ A @ r0 = [67 67 82 82]'\n"
     ]
    }
   ],
   "source": [
    "r0 = b - A @ x0\n",
    "pprint(f\"r0 = {r0}\")\n",
    "pprint(f\"A @ r0 = {A @ r0}\")\n",
    "\n",
    "# just to confirm that these are indeed linearly dependent on the first two\n",
    "pprint(f\"A @ A @ r0 = {A @ A @r0}\")\n",
    "pprint(f\"A @ A @ A @ r0 = {A @ A @ A @r0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37f6ce",
   "metadata": {},
   "source": [
    "Which means:\n",
    "    \n",
    "\\begin{align}\n",
    "  \\mathcal{D}_0 = \\operatorname{span}(\\{{\\bf r}^{(0)}\\}) \n",
    "  &= \\operatorname{span}(\\{\n",
    "  \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}  \n",
    "  \\}) \\\\\n",
    "  \\mathcal{D}_1 = \\operatorname{span}(\\{{\\bf r}^{(0)}, \\underline{\\bf A} {\\bf r}^{(0)}\\}) \n",
    "  &= \\operatorname{span}(\\{\n",
    "  \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix},\n",
    "  \\begin{pmatrix} 3 \\\\ 3 \\\\ 2 \\\\ 2 \\end{pmatrix},  \n",
    "  \\}) \\\\\n",
    "  &= \\operatorname{span}(\\{\n",
    "  \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix},\n",
    "  \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix},  \n",
    "  \\})\n",
    "\\end{align}\n",
    "\n",
    "and indeed the for third subspace $\\mathcal{D}_2 = \\operatorname{span}(\\{{\\bf r}^{(0)}, \\underline{\\bf A} {\\bf r}^{(0)}, \\underline{\\bf A}^2 {\\bf r}^{(0)}\\})$ but as we saw $\\underline{\\bf A}^2 {\\bf r}^{(0)}$ is a linear combination of ${\\bf r}^{(0)}$ and $\\underline{\\bf A} {\\bf r}^{(0)}$, and thus as expected $\\mathcal{D}_2=\\mathcal{D}_1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3e89a",
   "metadata": {},
   "source": [
    "**2.3** Use the Krylov subspaces to calculate the iteration vectors ${\\bf x}^i$ that are generated by the GMRES algorithm. You do **not** need any GMRES implementation to answer this question; you can answer it purely from its definition as a Krylov subspace method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2cc049",
   "metadata": {},
   "source": [
    "## Solution <a class=\"tocSkip\"></a>\n",
    "Here you can use the fact that GMRES is a Krylov subspace method, and, when you realize that the definition of Krylov subspaces is independent of which particular Krylov subspace method you use, the Krylov subspaces derived in the previous question. Again, because we know that the exact solution is reachable in the second Krylov subspace, the GMRES algorithm will find the exact solution in the second iteration. So in fact you only need to work out what the solution vector is in the first iteration!\n",
    "\n",
    "The GMRES method is defined as the Krylov subspace method that minimizes the residual, i.e. it chooses each new iteration to be in the point in the Krylov subspace where the residual $\\|{\\bf b} - \\underline{\\bf A}{\\bf x}\\|$ is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a83bd",
   "metadata": {},
   "source": [
    "For the first iteration we have ${\\bf x}^{(1)}-{\\bf x}^{(0)}\\in\\mathcal{D}_0$ which means\n",
    "\n",
    "$${\\bf x}^{(1)} = \\begin{pmatrix} \\alpha \\\\ \\alpha \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n",
    "\n",
    "for some $\\alpha$, which gives a residual of:\n",
    "\n",
    "$$\n",
    "{\\bf r}^{(1)} = {\\bf b} - \\underline{\\bf A}{\\bf x}^{(1)}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "  1 \\\\ 1 \\\\ 0 \\\\ 0\n",
    "\\end{pmatrix}\n",
    "-\n",
    "\\begin{pmatrix}\n",
    "  2 & 1 & 1 & 1 \\\\\n",
    "  1 & 2 & 1 & 1 \\\\\n",
    "  1 & 1 & 3 & 1 \\\\\n",
    "  1 & 1 & 1 & 3 \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "  \\alpha \\\\ \\alpha \\\\ 0 \\\\ 0\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "  1-3\\alpha \\\\ 1-3\\alpha \\\\ -2\\alpha \\\\ -2\\alpha\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "the norm of which is\n",
    "$$\n",
    "\\| {\\bf r}^{(1)} \\|^2 = 2 (1-3\\alpha)^2 + 2(-2\\alpha)^2 = 26\\alpha^2 - 12\\alpha + 2\n",
    "$$\n",
    "We find its minimum by solving\n",
    "$$\\frac{\\partial \\| {\\bf r}^{(1)} \\|^2}{\\partial\\alpha} = 52\\alpha - 12 = 0\n",
    "$$\n",
    "which gives $\\alpha=3/13$, and so\n",
    "$$\n",
    "{\\bf x}^{(1)} = \\begin{pmatrix} \\frac 3{13} \\\\ \\frac 3{13} \\\\ 0 \\\\ 0 \\end{pmatrix}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f5e5a",
   "metadata": {},
   "source": [
    "If you like to check your answer with scipy's gmres (this was not asked for of course!), you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6114bd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23076923076923078, 0.23076923076923078, 0, 0]\n",
      "(array([0.23076923, 0.23076923, 0.        , 0.        ]), 1)\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse.linalg as spl\n",
    "pprint([3/13,3/13,0,0])\n",
    "# need to use restart=1, otherwise the actual iterations that are performed\n",
    "# is maxiter*restart\n",
    "pprint(spl.gmres(A, b, maxiter=1, restart=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015756c",
   "metadata": {},
   "source": [
    "In the second iteration you can immediately use the fact that exact solution ${\\bf x}^\\ast$ is in the second Krylov subspace: $\\bf{x}^{(*)}-\\bf{x}^{(0)}\\in\\mathcal{D}_1$ so that the minimum residual in that subspace is in fact achieved in ${\\bf x}^\\ast$. Thus\n",
    "\n",
    "$$\n",
    "{\\bf x}^{(2)} = {\\bf x}^{\\ast} = \n",
    "\\begin{pmatrix} \\tfrac 12 \\\\ \\tfrac 12 \\\\ -\\tfrac 14 \\\\ -\\tfrac 14 \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77514e93",
   "metadata": {},
   "source": [
    "If you want to convince yourself of this fact, you could write\n",
    "\n",
    "$${\\bf x}^{(2)} = \\begin{pmatrix} \\alpha \\\\ \\alpha \\\\ \\beta \\\\ \\beta \\end{pmatrix}$$\n",
    "\n",
    "using a linear combination of the basisvectors of $\\mathcal{D}_1$ we worked out above. Then\n",
    "$$\n",
    "{\\bf r}^{(2)} = {\\bf b} - \\underline{\\bf A}{\\bf x}^{(2)}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "  1 \\\\ 1 \\\\ 0 \\\\ 0\n",
    "\\end{pmatrix}\n",
    "-\n",
    "\\begin{pmatrix}\n",
    "  2 & 1 & 1 & 1 \\\\\n",
    "  1 & 2 & 1 & 1 \\\\\n",
    "  1 & 1 & 3 & 1 \\\\\n",
    "  1 & 1 & 1 & 3 \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "  \\alpha \\\\ \\alpha \\\\ \\beta \\\\ \\beta\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "  1-3\\alpha - 2\\beta \\\\ 1-3\\alpha - 2\\beta \\\\ -2\\alpha - 4\\beta \\\\ -2\\alpha - 4\\beta\n",
    "\\end{pmatrix}$$\n",
    "with norm-squared\n",
    "$$\n",
    "\\| {\\bf r}^{(2)} \\|^2 = 2 (1-3\\alpha - 2\\beta)^2 + 2(-2\\alpha - 4\\beta)^2\n",
    "$$\n",
    "which we can minimize to get the same answer...\n",
    "\n",
    "Again, we can check this as well by running scipy's gmres algorithm for exactly two iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ba943298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5, -0.25, -0.25]\n",
      "(array([ 0.5 ,  0.5 , -0.25, -0.25]), 0)\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse.linalg as spl\n",
    "pprint([1/2, 1/2, -1/4, -1/4])\n",
    "# performs restart*maxiter iterations\n",
    "pprint(spl.gmres(A, b, maxiter=1, restart=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
