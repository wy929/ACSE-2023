{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inversion and Optimisation Lecture 12 - talk 2\n",
    "\n",
    "## Optional Exercises\n",
    "\n",
    "### Probability Revision\n",
    "\n",
    "1. Consider the \"[triangular probability distribution](https://en.wikipedia.org/wiki/Triangular_distribution)\", the three parameter distribution defined by\n",
    "$$ p(x | a,b,c) =\\begin{cases}\n",
    "\\frac{2(x-a)}{(b-a)(c-a)}& a<x<=c\\\\\n",
    "\\frac{2(x-c)}{(b-a)(b-c)}& c<x<=b\\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{cases} $$\n",
    "  - Plot this function and show that it is indeed a PDF (i.e. its integral is monotonically increasing and bounded by 0 & 1.\n",
    "  - Given a sample $x$ and fixed values for $a$ and $b$, what is the maximum likelihood estimator for $c$?\n",
    "  - Given a sample $x$ and a fixed value for $c$, what are the maximum likelihood estimators for $a$ and $b$\n",
    "     - if $x<c$? \n",
    "     - if $x>c$?\n",
    "\n",
    "2. Show that given $n$ independent trials ${X_1, \\ldots X_n}$ taken from a random variable $X$, with mean $\\mu$ and variance $\\sigma^2$, we have the result\n",
    "$$ E\\left(\\left[\\sum_i X_i^2\\right]-\\frac{1}{n}\\left[\\sum_i X_i\\right]^2\\right) = (n-1)\\sigma^2.$$\n",
    "You may use the identities that $\\left(\\sum_{i=1}^n a_i\\right)^2=\\sum_{i=1}^n a_i^2+\\sum_{i=1}^n \\sum_{j\\neq i}  a_i a_j$, that $E(X+Y) = E(X)+E(Y)$ and that _for independent variables_ $E(XY) = E(X)E(Y)$. Remember the definitions $E(X_i)=\\mu$ and $E(X_i^2)-\\mu^2=\\sigma^2$ and that for a constant, $\\sum_{c=1}^nc= nc$.  \n",
    "\n",
    "3.  Prove the identity for covariances that \n",
    "$$ E \\left( [X-\\mu_X][Y-\\mu_Y]\\right) = E(XY)-\\mu_X\\mu_Y.$$\n",
    "You may use the facts that $\\mu_X:=E(X)$ and $\\mu_Y:=E(Y)$ are constants, that $E(aX) = aE(X)$ and that $E(a) = a$ for constant variables, as well as the identities given in question 2.\n",
    "\n",
    "4. The [polar method](https://apps.dtic.mil/dtic/tr/fulltext/u2/288931.pdf) to generate Gaussian random variables can be stated as follows:\n",
    "   1. Take a pair $(u,v)$ of uniform variables over $[-1,1]$.\n",
    "   2. Let $s=u^2+v^2$. If $s=0$ or $s>1$ goto A and try again.\n",
    "   3. Otherwise, calculate \n",
    "      $$x=u\\sqrt{\\frac{-2\\ln s}{s}}, \\quad y=v\\sqrt{\\frac{-2\\ln s}{s}}.$$\n",
    "   \n",
    "      These will be normal random variables of zero mean and unit variance.\n",
    "   \n",
    "   Implement this method in Python. Plot a histogram of your results to confirm you have the correct distribution.\n",
    " Can you extend the method to work for arbitrary means and variances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The BLUE\n",
    "\n",
    " 1. Try to extend the BLUE analysis to the case of three independent unbiased observations, then  optionally generalize it for $n$ observations. You can do this either using substitution (as in the lecture notes), or by using a Lagrange multiplier for the constraint $E(e_\\textrm{guess})=0$.\n",
    " 2. The BLUE is the best (in the sense of minimum mean square error) linear unbiased estimator. Treating $T_t$ as a known constant deterministic variable, can you find a better biased linear estimator, specified in terms of the $\\sigma_i^2$ and $T_t$? Why would this be no use in practice?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal interpolation\n",
    "\n",
    "Revise the conditions in which the OI method *can* be used to assimilate data, and when it's a *good* method to assimilate data.\n",
    "\n",
    "Run the following experiments with the example OI code from the lecture notes:\n",
    "  1. Try modifying the $\\sigma$ values, including adding a \"real\" $\\sigma_R$ used to add the observation noise and a \"fake\" one used in the OI calculation.\n",
    "  1. Try adding some systematic biases into the observations. How large must these be before the mean square error of the analysis regularly exceeds that of the background?\n",
    "  2. Try adding in the same observation twice, while keeping $R$ diagonal. How should $R$ really change in this circumstance, and what difficulties does that introduce?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D - Var\n",
    "\n",
    "Run the following experiments with the example 3D-Var code from the lecture notes:\n",
    "\n",
    " 1. Try adding more observations, or some observations of the wind direction as well as speed. Which makes more difference to the analysis accuracy?\n",
    " 2. Rewrite the cost function in the example to use a direct solver to calculate $\\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b)$ and $\\mathbf{R}^{-1}(\\mathbf{y}-\\mathbf{h}(\\mathbf{x}_b))$? Compare the time and memory requirements with an iterative inversion method for a few problem sizes.\n",
    " 3. Experiment with other minimization methods; Can you provide a Hessian function to speed things up further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4. Consider the matrix problem\n",
    "\n",
    "$$\\left(\n",
    "\\begin{array}{cc}\n",
    "\\mathbf{R}&\\mathbf{H}\\\\\n",
    "\\mathbf{H}^T &-\\mathbf{B}^{-1}\n",
    "\\end{array}\n",
    "\\right)\\left(\n",
    "\\begin{array}{c}\n",
    "\\mathbf{a}\\\\\n",
    "\\mathbf{b}\n",
    "\\end{array}\n",
    "\\right)=\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\mathbf{c}\\\\\n",
    "\\mathbf{0}\n",
    "\\end{array}\n",
    "\\right).$$\n",
    " Write down the pair of vector equations this gives in non-matrix form. We want to generate an equation in the form $\\mathbf{a}=\\mathbf{K}\\mathbf{c}$, in two different ways:\n",
    "  - first, solving simultaneously, eliminate $\\mathbf{b}$ to get an equation for $\\mathbf{a}$, then substitute into the seond equation to get the equation for $\\mathbf{b}$\n",
    "  - second, by eliminate $\\mathbf{a}$ between the two equations directly.\n",
    "By comparing the form of the results, can you see how to use this to show the equivalence between OI and linear 3D VAR?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman Filter\n",
    "\n",
    "  1. A hindcast is a \"forecast done backwards\". That is to say, the model is run backwards so that $\\mathbf{x}_{k-1}=\\mathbf{M}^{-1}\\mathbf{x}_k$. How do the Kalman Filter equations change if the system is run in reverse? In particular, how does the forecast covariance equation change? Modify the Kalman filter example to run backwards.\n",
    "  3. Modify the example code:\n",
    "    - Since the $\\mathbf{M}$ matrix is sparse here, create functions to calculate $\\mathbf{M}\\mathbf{x}$ and $\\mathbf{M}^T\\mathbf{x}$ directly (a.k.a \"matrix-free\"), rather than through matrix multiplication or the numpy.dot function. How big does nx need to be for this function to be faster? Can you make your function work on matrices?\n",
    "    - Modify the code to allow for arbitrary observations at moving locations. Can you find a good strategy to place the"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4D-Var\n",
    "\n",
    "1. Consider the situation of running a 4D-Var assimilation for a discrete linear model, $\\mathbf{M}$, and linear observation operator $\\mathbf{H}$ taken over a time window of one timestep with observations only on at the end of the window, so that\n",
    "$$ \\mathbf{x}_1 = \\mathbf{M}\\mathbf{x}_0,$$\n",
    "$$\\mathcal{J} = \\frac{1}{2}(\\mathbf{x}_0-\\mathbf{x}_b)^T\\mathbf{B}^{-1}(\\mathbf{x}_0-\\mathbf{x}_b) + \\frac{1}{2}(\\mathbf{y}_1-\\mathbf{H}\\mathbf{x}_1)^T\\mathbf{R}^{-1}(\\mathbf{y}_1-\\mathbf{H}\\mathbf{x}_1).$$\n",
    "By direct substitution or otherwise, give the relevant optimal value of $\\mathbf{x}_1$, in the form $\\mathbf{x}_1 = \\mathbf{M}\\mathbf{x}_b +\\mathbf{W}(\\mathbf{y}-\\mathbf{H}\\mathbf{M}\\mathbf{x}_b).$. By comparing the optimal interpolation equation or linear 3D-Var solution, can you state the equivalent forecast error covariance matrix being used to assimilate $y_1$? How does this compare to the forward and hindcasting Kalman filter equations?\n",
    "\n",
    "2. Modify the example code:\n",
    "  - vary the diffusion coefficient and advection speed in the forward and adjoint models. Which has more influence on the error? \n",
    "  - try setting inconsistent values in the adjoint model versus the forward model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
